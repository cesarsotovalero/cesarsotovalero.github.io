<!doctype html><html xmlns=http://www.w3.org/1999/xhtml lang=en-us xml:lang=en-US itemscope itemtype=http://schema.org/WebSite><head><script>(function(){let a=localStorage.getItem('theme');a==='dark'&&document.documentElement.setAttribute('data-theme','dark')})()</script><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><title>Evaluation Metrics for Real-Time Financial Fraud Detection ML Models</title><meta name=keywords content="financial fraudfraud detectionmachine learningdeep learningsurvey"><link rel=alternate type=application/rss+xml title="César Soto Valero  César - Computer Scientist" href=https://www.cesarsotovalero.net/feed.xml><script src=https://cdn.jsdelivr.net/npm/typed.js@2.0.12></script>
<link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Rouge+Script&display=swap" rel=stylesheet><link rel=apple-touch-icon sizes=180x180 href=../img/favicon/redketchup/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../img/favicon/redketchup/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../img/favicon/redketchup/favicon-16x16.png><link rel=manifest href=../img/favicon/redketchup/site.webmanifest><link id=code rel=stylesheet href=../css/pygment_highlights.css><script src=https://d3js.org/d3.v7.min.js></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-107061705-1"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','UA-107061705-1')</script><script type=text/javascript>(function(a,e,b,f,g,c,d){a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},c=e.createElement(f),c.async=1,c.src="https://www.clarity.ms/tag/"+g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d)})(window,document,"clarity","script","bs3gcidnol")</script><script src=../js/anchor.min.js></script>
<link rel=stylesheet href=//use.fontawesome.com/releases/v5.12.0/css/all.css><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/bootstrap-social.css><link rel=stylesheet href=/css/main.css><link rel=stylesheet href=/css/academicons.css><link rel=stylesheet href="//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic"><link rel=stylesheet href="//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800"><meta name=generator content="Jekyll v4.3.2"><meta property="og:title" content="Evaluation Metrics for Real-Time Financial Fraud Detection ML Models"><meta name=author content="César Soto Valero"><meta property="og:locale" content="en_US"><meta name=description content="After training a real-time financial fraud detection model, the next step is to evaluate its performance. This post provides an overview of the most common evaluation metrics and considerations for fraud detection models, including confusion matrix, precision, recall, F1-score, AUC-ROC, AUC-PR, and more."><meta property="og:description" content="After training a real-time financial fraud detection model, the next step is to evaluate its performance. This post provides an overview of the most common evaluation metrics and considerations for fraud detection models, including confusion matrix, precision, recall, F1-score, AUC-ROC, AUC-PR, and more."><link rel=canonical href=https://www.cesarsotovalero.net/blog/evaluation-metrics-for-real-time-financial-fraud-detection-ml-models.html><meta property="og:url" content="https://www.cesarsotovalero.net/blog/evaluation-metrics-for-real-time-financial-fraud-detection-ml-models.html"><meta property="og:site_name" content="César Soto Valero"><meta property="og:image" content="https://www.cesarsotovalero.net/img/posts/2025/2025-05-08/kungstradgarden_cover.jpg"><meta property="og:type" content="article"><meta property="article:published_time" content="2025-05-08T00:00:00-07:00"><meta name=twitter:card content="summary"><meta property="twitter:image" content="https://www.cesarsotovalero.net/img/posts/2025/2025-05-08/kungstradgarden_cover.jpg"><meta property="twitter:title" content="Evaluation Metrics for Real-Time Financial Fraud Detection ML Models"><meta name=twitter:site content="@cesarsotovalero"><meta name=twitter:creator content="@César Soto Valero"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"César Soto Valero","url":"https://www.cesarsotovalero.net/about-me"},"dateModified":"2025-07-29T11:58:39-07:00","datePublished":"2025-05-08T00:00:00-07:00","description":"After training a real-time financial fraud detection model, the next step is to evaluate its performance. This post provides an overview of the most common evaluation metrics and considerations for fraud detection models, including confusion matrix, precision, recall, F1-score, AUC-ROC, AUC-PR, and more.","headline":"Evaluation Metrics for Real-Time Financial Fraud Detection ML Models","image":"https://www.cesarsotovalero.net/img/posts/2025/2025-05-08/kungstradgarden_cover.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.cesarsotovalero.net/blog/evaluation-metrics-for-real-time-financial-fraud-detection-ml-models.html"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://www.cesarsotovalero.net/img/pages/cesar/avatar-icon-2024.jpg"},"name":"César Soto Valero"},"url":"https://www.cesarsotovalero.net/blog/evaluation-metrics-for-real-time-financial-fraud-detection-ml-models.html"}</script><script type=text/x-mathjax-config> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } });
   </script><script type=text/x-mathjax-config>
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          processEscapes: true
        }
      });
   </script><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type=text/javascript></script></head><body><nav class="navbar-fixed-top navbar-custom navbar navbar-expand-lg navbar-light bg-light"><button class=navbar-toggle type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="collapse navbar-collapse container-fluid" id=navbarSupportedContent><ul class="navigation list-inline text-center footer-links" id=black-icons><li class="nav-item navbar-custom" title=Home><a id=navbar-brand href=https://www.cesarsotovalero.net>Home</a></li><li class="nav-item navbar-custom"><a href=/blog>Blog</a></li><li class="nav-item navbar-custom"><a href=/linkedin>LinkedIn</a></li><li class="nav-item navbar-custom"><a href=/youtube>YouTube</a></li><li class="nav-item navbar-custom"><a href=/podcasts>Podcasts</a></li><li class="nav-item navbar-custom"><a href=/talks>Talks</a></li><li class="nav-item navbar-custom"><a href=/about-me>About</a></li><li class="nav-item navbar-custom" title="Toggle Night Mode"><a href=# id=theme-toggle onclick=modeSwitcher() style=cursor:pointer></a></li></ul></div></nav><header class=header-section><div class="intro-header no-img"><div class=container><div class=row><div class=header-image style=padding-left:15px><div class="row justify-content-center" style=margin-right:0;margin-left:0></div></div><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=post-heading><h1 class=no-anchor>Evaluation Metrics for Real-Time Financial Fraud Detection ML Models</h1><h2 class=post-subheading>An overview</h2><div class="container flex-container"><a href=https://www.cesarsotovalero.net/about-me><img class=avatar-img-small src=/img/pages/cesar/avatar-icon-2024.jpg alt="César Soto Valero" id=meta-img></a><div class=flex-item><span class=post-meta>Posted on May 8, 2025</span><div><span class=post-meta title="Estimated read time"><svg id="i-clock" viewBox="0 0 32 32" width="18" height="18" style="vertical-align:middle" fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="3"><circle cx="16" cy="16" r="14"/><path d="M16 8v8l4 4"/></svg>&nbsp;12 mins read</span>
<span class="blog-tags post-meta"><svg id="i-tag" aria-hidden="true" focusable="false" data-prefix="far" data-icon="tag" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="svg-inline--fa fa-tag fa-w-16" width="16.5" height="16.5"><path fill="none" d="M497.941 225.941 286.059 14.059A48 48 0 00252.118.0H48C21.49.0.0 21.49.0 48v204.118a47.998 47.998.0 0014.059 33.941l211.882 211.882c18.745 18.745 49.137 18.746 67.882.0l204.118-204.118c18.745-18.745 18.745-49.137.0-67.882zM259.886 463.996 48 252.118V48h204.118L464 259.882 259.886 463.996zM192 144c0 26.51-21.49 48-48 48s-48-21.49-48-48 21.49-48 48-48 48 21.49 48 48z"/></svg>ai</span></div></div></div><div class=flex-item><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1" style=margin-left:0><section id=share-section><script type=text/javascript src=https://storage.ko-fi.com/cdn/widget/Widget_2.js></script><script type=text/javascript>kofiwidget2.init('Support this blog','#7aa4d1','G2G3CRPPI'),kofiwidget2.draw()</script></section></div></div></div><div class=cesarcarbon id=cesarcarbonads-post><script async type=text/javascript src="//cdn.carbonads.com/carbon.js?serve=CESI52JM&placement=wwwcesarsotovaleronet" id=_carbonads_js></script></div></div></div><div class="col-lg-4 col-lg-pull-2 col-md-2 col-md-pull-2"><ol id=toc class=section-nav><li class="toc-entry toc-h1"><a href=#confusion-matrix>Confusion Matrix</a></li><li class="toc-entry toc-h1"><a href=#precision>Precision</a></li><li class="toc-entry toc-h1"><a href=#recall>Recall</a></li><li class="toc-entry toc-h1"><a href=#f1-score>F1-Score</a></li><li class="toc-entry toc-h1"><a href=#fpr>FPR</a></li><li class="toc-entry toc-h1"><a href=#fnr>FNR</a></li><li class="toc-entry toc-h1"><a href=#tnr>TNR</a></li><li class="toc-entry toc-h1"><a href=#auc-roc>AUC-ROC</a></li><li class="toc-entry toc-h1"><a href=#auc-pr>AUC-PR</a></li><li class="toc-entry toc-h1"><a href=#latency>Latency</a></li><li class="toc-entry toc-h1"><a href=#summary>Summary</a></li><li class="toc-entry toc-h1"><a href=#footnotes>Footnotes</a></li></ol></div></div></div></div></header><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-AMS-MML_HTMLorMML,https://idcrook.github.io/assets/js/MathJaxLocal.js"></script><div class=container><div class=row><div class="col-lg-9 col-lg-offset-2 col-md-10 col-md-offset-1"><article role=main class=blog-post><p>After training a machine learning model for <a href=../blog/real-time-financial-fraud-detection.html>real-time financial fraud detection</a>, the next step is to evaluate its performance.
Fraud detection models face unique challenges during evaluation.
Two examples are (1) class imbalance and (2) the different costs of false positives vs. false negatives.
Class imbalance means that the model doesn’t have enough fraudulent transactions to learn from, as they are rare compared to legitimate ones.
False positives, which flag legitimate transactions as fraudulent, can lead to customer dissatisfaction, while false negatives, where fraudulent transactions go undetected, can result in significant financial losses.
In this post, I cover the most common metrics and considerations for evaluating fraud detection models while keeping these unique challenges in mind.</p><h1 id=confusion-matrix>Confusion Matrix</h1><p>Most financial fraud detection systems generate a binary output: a prediction indicating whether a transaction (txt) is fraudulent (1) or genuine (0).</p><p>By leveraging this universal approach to binary classification, we can establish standard evaluation methodologies to assess the performance of this type of models.</p><p>The confusion matrix is a widely used tool for summarizing and visualizing the performance of a classification model in a tabular format.
It provides a clear breakdown of predictions vs. actual outcomes.</p><p>In a confusion matrix:</p><ul><li>The <em>x</em>-axis represents the ground-truth labels (actual outcomes).</li><li>The <em>y</em>-axis represents the predictions made by the classification model.</li></ul><p>Both axes are divided into two categories: positive (fraudulent txt) and negative (genuine txt).
The positive class corresponds to the minority class (fraud), while the negative class corresponds to the majority class (genuine).</p><p><img src=/img/posts/2025/2025-05-08/confusion-matrix.svg alt="Confusion Matrix"></p><p>This representation allows us to visualize the model’s performance in terms of true positives, true negatives, false positives, and false negatives.</p><ul><li><strong>True Positives (TP):</strong> The number of fraudulent transactions correctly identified as fraud.</li><li><strong>True Negatives (TN):</strong> The number of genuine transactions correctly identified as genuine.</li><li><strong>False Positives (FP):</strong> The number of genuine transactions incorrectly flagged as fraudulent.</li><li><strong>False Negatives (FN):</strong> The number of fraudulent transactions missed by the system.</li></ul><p>By analyzing these metrics, we gain a comprehensive view of the model’s strengths and weaknesses, enabling informed decisions for further optimization.</p><h1 id=precision>Precision</h1><p><a href=https://en.wikipedia.org/wiki/Precision_and_recall>Precision</a> is the fraction of transactions that were actually fraud among all transactions the model flagged as fraud (predicted positive).</p><p>For example, if a model flagged 100 transactions as fraud, and 80 of those were indeed fraudulent, then:</p>\[\text{Precision} = \frac{TP}{TP + FP} = \frac{80}{80 + 20} = 0.8\]<p>High Precision means few false positives, so it is crucial for operational efficiency.</p><p>Low Precision means investigators waste time on many false alarms, and customers suffer unnecessary transaction declines.</p><p>Many top systems aim for very high Precision (e.g., 0.9+) at low fraud rates, but there’s a trade-off with Recall.
For example, if we lower the threshold to catch more fraud, Precision may drop.
Therefore, Precision is often reported at a certain operating point or as an average if multiple thresholds are considered.</p><p>An example interpretation: “Of the transactions our system blocked, 95% were indeed fraudulent”, that’s a Precision of 95%.</p><h1 id=recall>Recall</h1><p><a href=https://en.wikipedia.org/wiki/Precision_and_recall>Recall</a> is the fraction of actual fraud cases that the model correctly predicts as fraud (true positives) out of all actual fraud cases.</p><p>For example, if there were 100 actual fraud cases and the model caught 80 of them, then:</p>\[\text{Recall} = \frac{TP}{TP + FN} = \frac{80}{80 + 20} = 0.8\]<p>A Recall of 0.80 means 80% of fraud instances were detected (20% missed).</p><p>High Recall means few false negatives, which is critical in fraud detection because missing a fraudulent transaction can lead to financial losses.</p><p>Low Recall means many frauds slip through and cause losses.</p><p>We can usually increase Recall by lowering the detection threshold at the cost of Precision.
In practice, businesses may set a Recall target like “catch at least 70% of fraud” and then maximize Precision under that constraint.</p><h1 id=f1-score>F1-Score</h1><p><a href=https://en.wikipedia.org/wiki/F-score>F1-Score</a>, or F1 for short, is the harmonic mean of Precision and Recall.
It gives a single-figure balance of both metrics, which is useful when we want a combined score for model selection or when class distribution is skewed.</p><p>For example, if Precision is 0.8 and Recall is 0.6, then:</p>\[F1 = \frac{2 \times Precision \times Recall}{Precision + Recall} \approx 0.686\]<p>High F1 means both Precision and Recall are reasonably high.
Low F1 means either Precision or Recall is low, which is undesirable in fraud detection.</p><p>Overall, F1 is a good metric to assess a fraud detection model.
It is also a popular metric in Kaggle competitions and papers to compare models, ensuring they are not just optimizing one at the expense of the other.</p><h1 id=fpr>FPR</h1><p>False Positive Rate (FPR) is is the share of legitimate transactions that get incorrectly flagged as fraud.</p><p>For example, if there were 100 legitimate transactions and the model flagged 5 of them as fraud, then:</p>\[\text{FPR} = \frac{FP}{TN + FP} = \frac{5}{95 + 5} = 0.05\]<p>FPR is important because it directly impacts customer experience.
High FPR means many legitimate transactions are blocked, leading to customer dissatisfaction and potential loss of business trust.</p><p>Sometimes businesses set FPR requirements to control false alarms.
For example: “We can only tolerate reviewing 0.5% of transactions, so FPR must be ≤ 0.005.”</p><h1 id=fnr>FNR</h1><p>False Negative Rate (FNR) is the share of fraudulent transactions the model misses.</p><p>For example, if there were 100 actual fraud cases and the model missed 2 of them, then:</p>\[\text{FNR} = \frac{FN}{TP + FN} = \frac{2}{98 + 2} = 0.025\]<p>Some businesses set FNR requirements to control missed fraud.
For example: “We cannot tolerate missing more than 10% of fraud, so FNR ≤ 0.1” which implies Recall ≥ 0.9.</p><h1 id=tnr>TNR</h1><p>True Negative Rate (TNR) or Specificity measures how well the system correctly identifies legitimate transactions as non-fraud.</p><p>For example, if there were 1000 legitimate transactions and the model flagged 50 of them incorrectly as fraud (FP), the calculation would be:</p>\[\text{TNR} = \frac{TN}{TN + FP} = \frac{950}{950 + 50} = 0.95\]<p>TNR is often overlooked in fraud detection because it’s essentially the complement of the False Positive Rate (FPR):</p>\[\text{TNR} = 1 - \text{FPR}\]<p>TNR is typically very high in fraud detection systems because the number of legitimate transactions (TN) is much larger than the number of frauds or false positives.</p><p>Since Precision already focuses on avoiding false positives, and we typically assume we want to approve as many legitimate transactions as possible, TNR doesn’t usually take center stage.</p><p>However, in some contexts, like regulatory requirements or customer experience, it’s important to keep FPR below a certain threshold, such as “FPR ≤ 0.1%,” which directly relates to maintaining high TNR.</p><h1 id=auc-roc>AUC-ROC</h1><p>The Area Under the ROC Curve<sup id=fnref:1><a href=#fn:1 class=footnote rel=footnote role=doc-noteref>1</a></sup> (AUC-ROC) measures a model’s ability to distinguish fraud from non-fraud across all possible thresholds.
In essence, it plots Recall against FPR.</p><p>The AUC is the area under this curve:</p><ul><li>AUC = 0.5 means random guessing.</li><li>AUC = 1.0 means perfect discrimination.</li></ul><p>This area is computed as follows:</p>\[\text{AUC} = \int_0^1 \text{Recall}(FPR) \, dFPR\]<p>AUC is threshold-independent: it summarizes performance across all thresholds, and it’s less sensitive to class imbalance than accuracy.</p><p>An intuitive interpretation: “If I randomly pick a fraud and a legitimate transaction, AUC is the chance the fraud gets a higher risk score.”</p><h1 id=auc-pr>AUC-PR</h1><p>The Area Under Precision-Recall Curve (AUC-PR) plots Precision vs. Recall and focuses squarely on the minority class (fraud), so it tells us how well the model catches fraud while keeping false positives low.</p><p>In highly imbalanced data such as fraud detection, AUC-PR is more informative than AUC-ROC because it answers how well the model balances Precision and Recall where it matters.</p><p>For instance, a model could have AUC-ROC = 0.98 and still have AUC-PR = 0.10, which means that the model detects fraud more often than non-fraud, but when it comes to real-world detection, Precision at high Recall isn’t stellar.</p><p>AUC-PR is the go-to metric when fraud cases are rare, and we care about catching as many as possible without overwhelming the system with false alarms.</p><h2 id=threshold-for-auc-pr>Threshold for AUC-PR</h2><p>Once we have chosen the best model as per AUC-PR, we need to decide a threshold, denoted as \(\tau\), to convert this model’s fraud probability output into a concrete binary decision (fraud or not fraud).</p><p>If we look at the Precision-Recall (PR) curve in the figure below, different values of \(\tau\) correspond to different trade-offs between Precision and Recall.</p><figure class=jb_picture><img width=100% style="border:1px solid gray" src=/assets/resized/AUPRC-640x477.png alt="Sample Precision-Recall curves for two models A and B." data-srcset="/assets/resized/AUPRC-640x477.png 640w,/assets/resized/AUPRC-768x573.png 768w,/assets/resized/AUPRC-1024x763.png 1024w,/assets/resized/AUPRC-1366x1018.png 1366w," class="blur-up lazyautosizes lazyload"><figcaption class=stroke>Sample Precision-Recall curves for two models A and B. Model B is superior to model A as is reflected in the AUC-PR values of the two models. Different points on the PR curve represent different threshold values and different trade-offs between Precision and Recall metrics.</figcaption></figure><p>We ultimately need to choose the right trade-off that suits our use case.
The threshold \(\tau\) determines the decision boundary for classifying transactions as fraudulent or genuine. Mathematically, this can be expressed as:</p>\[\text{Decision: Fraud if } P(x) > \tau\]<p>Where:</p><ul><li>\(P(x)\) is the predicted probability of fraud for transaction \(x\).</li><li>\(\tau\) is the threshold value.</li></ul><p>With the above framework in mind, we can decide the threshold value \(\tau\) based on the value of \(k\), where \(k\) represents the minimum Precision we want to maintain.</p><p>For example, if we want to maintain a minimum Precision of 90%, then \(k = 90\). Using the Precision-Recall curve, we can derive the threshold value \(\tau\) as well as the equivalent Recall value at 90% Precision.</p><p>This strategy allows us to calculate the optimal threshold \(\tau\) while evaluating our trained model on the test set. Once the threshold is determined, it can be used to classify transactions during deployment:</p>\[\text{Fraud if } P(x) > \tau, \text{ otherwise Genuine.}\]<p>By adjusting \(\tau\), we can balance Precision and Recall to meet specific business objectives and constraints.</p><h1 id=latency>Latency</h1><p>Latency is the time it takes for the system to process a transaction and make an inference.
Keeping latency low is crucial for real-time systems.
Fraud models not only need to have good statistical performance but also operate quickly enough to be used in practice.</p><p>Latency and complexity matter in payment systems.
In the example below, the <code class="language-plaintext highlighter-rouge">Payment Server</code> dispatches parallel calls to <code class="language-plaintext highlighter-rouge">KYC Service</code>, <code class="language-plaintext highlighter-rouge">Fraud Check</code>, and <code class="language-plaintext highlighter-rouge">Payment Rail</code>, but the transaction can only complete once the slowest of these services responds (the <code class="language-plaintext highlighter-rouge">KYC Service</code> in this example).
Even though <code class="language-plaintext highlighter-rouge">Fraud Check</code> takes just 25 ms, any fluctuation (like a network hiccup or a slow third-party response) can bottleneck the entire flow.
That’s why latency is a system-wide risk amplifier.</p><pre><code class=language-mermaid>flowchart LR
  %% define the Payment App node
  PaymentApp[Payment App]

  %% container for backend services
  subgraph container [" "]
    direction LR
    Server[Payment Server]
    KYC[KYC Service]
    Fraud[Fraud Check]
    Rail[Payment Rail]
  end

  %% draw the edges with labels
  PaymentApp --&gt;|50ms| Server
  Server     --&gt;|50ms| KYC
  Server     --&gt;|25ms| Fraud
  Server     --&gt;|25ms| Rail

  %% style all four links purple
  linkStyle 0,1,2,3 stroke:#800080,stroke-width:2px

  %% highlight Fraud Check in red
  style Fraud fill:#ffe6e6,stroke:#ff0000,stroke-width:2px
</code></pre><p>Real-time fraud detection latency is commonly measured in two ways:</p><ol><li><strong>Online decision latency (ODL):</strong> How long it takes to score a single transaction and respond (which affects user experience and fraud blocking effectiveness).</li><li><strong>Time-to-detection for fraud patterns (TTD):</strong> If an attack starts at a certain time, how long before the system detects and flags it.</li></ol><p>ODL is usually measured in milliseconds.
For example, a payment system might have an end-to-end latency budget of 200ms for authorization, out of which fraud checks get 20–30ms.
Modern systems often aim for fraud model inference under ~50ms.
In practice, we can look at 99th percentile latency (e.g., 99% of transactions scored in &lt;500ms), to ensure worst-case delays are bounded.</p><p>TTD is more about monitoring and measuring the resilience of the system to detect an emerging fraud <em>modus operandi</em>.
For example: “Did we catch the new fraud ring the first day it appeared, or did it go undetected for weeks?”
This is harder to quantify but important in evaluating adaptive systems.</p><h1 id=summary>Summary</h1><p>In practice, evaluating a fraud detection model involves:</p><ol><li>Analyzing the confusion matrix at the operating point.</li><li>Reviewing Recall, F1, and AUC-PR.</li><li>Choosing a threshold that satisfies business constraints (e.g., maximum number of tolerable false positives).</li></ol><p>But evaluation doesn’t stop at metrics.
Weighting fraud by transaction amount matters: catching a 10,000 USD fraud is more impactful than catching five 1,000 USD cases.
Moreover, metrics on static test sets aren’t enough.
We also need to perform <a href=https://en.wikipedia.org/wiki/Backtesting>backtesting</a> (simulate past performance) and <a href=https://en.wikipedia.org/wiki/Sandbox_(computer_security)>sandbox testing</a> (simulate deployment), and monitor the model in production.</p><p>Observe how fraud patterns change: Do attackers evolve? Do false positives creep up?</p><p>Or even better: run <a href=https://en.wikipedia.org/wiki/A/B_testing>A/B tests</a>.</p><p>Put the new model in production in <a href=https://en.wikipedia.org/wiki/Shadowing_(computing)>shadow mode</a> and compare it to the previous version.</p><p>But that’s content for another post.</p><h1 id=footnotes>Footnotes</h1><div class=footnotes role=doc-endnotes><ol><li id=fn:1><p><a href=https://en.wikipedia.org/wiki/Receiver_operating_characteristic>ROC Curve</a> stands for “Receiver Operating Characteristic”, a very weird name, indeed. <a href=#fnref:1 class=reversefootnote role=doc-backlink>&#8617;</a></p></li></ol></div></article><ul class="pager blog-pager"><li class=previous><a href=/blog/from-classical-ml-to-dnns-and-gnns-for-real-time-financial-fraud-detection.html data-toggle=tooltip data-placement=top title="From Classical ML to DNNs and GNNs for Real-Time Financial Fraud Detection">&larr; Previous Post</a></li><li class=next><a href=/blog/i-am-switching-to-python-and-actually-liking-it.html data-toggle=tooltip data-placement=top title="I'm Switching to Python and Actually Liking It">Next Post &rarr;</a></li></ul><br></div></div></div><script>anchors.options={position:'left'},anchors.add('h1:not(.no-anchor)','h2','h3')</script><div class=container><div class=row><div class="col-lg-10 col-lg-offset-2 col-md-10 col-md-offset-1" style=padding-right:15px><div class=flex-container-footer-badges><section class=share-section><a href="https://twitter.com/intent/tweet?text=After+training+a+real-time+financial+fraud+detection+model%2C+the+next+step+is+to+evaluate+its+performance.+This+post+provides+an+overview+of+the+most+common+evaluation+metrics+and+considerations+for+fraud+detection+models%2C+including+confusion+matrix%2C+precision%2C+recall%2C+F1-score%2C+AUC-ROC%2C+AUC-PR%2C+and+more.%0A+https://www.cesarsotovalero.net/blog/evaluation-metrics-for-real-time-financial-fraud-detection-ml-models.html" class="btn btn-my-icon btn-twitter" title="Share on Twitter"><span id=share-twitter class="fab fa-twitter" aria-hidden=true></span>
<span class=sr-only>Twitter</span></a>
<a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.cesarsotovalero.net/blog/evaluation-metrics-for-real-time-financial-fraud-detection-ml-models.html" class="btn btn-my-icon btn-linkedin" title="Share on LinkedIn"><span id=share-linkedin class="fab fa-linkedin-in" aria-hidden=true></span>
<span class=sr-only>LinkedIn</span></a>
<a href="https://www.facebook.com/sharer/sharer.php?u=https://www.cesarsotovalero.net/blog/evaluation-metrics-for-real-time-financial-fraud-detection-ml-models.html" class="btn btn-my-icon btn-facebook" title="Share on Facebook"><span class="fab fa-facebook-f" aria-hidden=true></span>
<span class=sr-only>Facebook</span></a>
<a href="https://www.reddit.com/submit?url=https%3A%2F%2Fwww.cesarsotovalero.net%2Fblog%2Fevaluation-metrics-for-real-time-financial-fraud-detection-ml-models.html" class="btn btn-my-icon btn-reddit" title="Share on Reddit"><span class="fab fa-fw fa-reddit" aria-hidden=true></span>
<span class=sr-only>Reddit</span></a>
<a href="https://t.me/share/url?url=https%3A%2F%2Fwww.cesarsotovalero.net%2Fblog%2Fevaluation-metrics-for-real-time-financial-fraud-detection-ml-models.html&text=After+training+a+real-time+financial+fraud+detection+model%2C+the+next+step+is+to+evaluate+its+performance.+This+post+provides+an+overview+of+the+most+common+evaluation+metrics+and+considerations+for+fraud+detection+models%2C+including+confusion+matrix%2C+precision%2C+recall%2C+F1-score%2C+AUC-ROC%2C+AUC-PR%2C+and+more.%0A" class="btn btn-my-icon btn-telegram" title="Share on Telegram"><i class="fab fa-telegram"></i>
<span class=sr-only>Telegram</span></a>
<a href="http://news.ycombinator.com/submitlink?u=https%3A%2F%2Fwww.cesarsotovalero.net%2Fblog%2Fevaluation-metrics-for-real-time-financial-fraud-detection-ml-models.html&t=Evaluation+Metrics+for+Real-Time+Financial+Fraud+Detection+ML+Models" class="btn btn-my-icon btn-hn" title="Share on Hacker News"><i class="fab fa-hacker-news-square"></i>
<span class=sr-only>Hacker News</span></a></section></div><div id=related-posts><hr><h2>Related Posts</h2><div class="panel-deck row"><div class=col-sm-4><div class=panel><a href=/blog/why-genai-will-not-replace-software-engineers-just-yet.html><div class=panel-body><div class=text-left><h4 class=title data-toc-skip>Why GenAI Will NOT Replace Software Engineers</h4><div class=date style=margin-bottom:0>Posted on August 19, 2024</div></div><img src=../img/posts/2024/2024-08-19/armillary-sphere_cover.jpg alt="Why GenAI Will NOT Replace Software Engineers"></div></a></div></div><div class=col-sm-4><div class=panel><a href=/blog/building-genai-applications-today.html><div class=panel-body><div class=text-left><h4 class=title data-toc-skip>Building GenAI Applications Today</h4><div class=date style=margin-bottom:0>Posted on November 17, 2024</div></div><img src=../img/posts/2024/2024-11-17/twisted-revolver_cover.jpg alt="Building GenAI Applications Today"></div></a></div></div><div class=col-sm-4><div class=panel><a href=/blog/from-classical-ml-to-dnns-and-gnns-for-real-time-financial-fraud-detection.html><div class=panel-body><div class=text-left><h4 class=title data-toc-skip>From Classical ML to DNNs and GNNs for Real-Time Financial Fraud Detection</h4><div class=date style=margin-bottom:0>Posted on April 3, 2025</div></div><img src=../img/posts/2025/2025-04-03/justitiabrunnen_cover.jpg alt="From Classical ML to DNNs and GNNs for Real-Time Financial Fraud Detection"></div></a></div></div></div></div><div><div id=giscus-container></div><script src=/js/load-giscus.js></script>
<script>window.addEventListener('load',loadGiscus)</script></div></div></div></div><script>window.addEventListener('DOMContentLoaded',()=>{const a=new IntersectionObserver(a=>{a.forEach(a=>{const b=a.target.getAttribute('id');a.intersectionRatio>0?document.querySelector(`#toc li a[href="#${b}"]`).parentElement.classList.add('active'):document.querySelector(`#toc li a[href="#${b}"]`).parentElement.classList.remove('active')})});document.querySelectorAll('h1[id]').forEach(b=>{a.observe(b)}),document.querySelectorAll('h2[id]').forEach(b=>{a.observe(b)})})</script><footer><div class="container beautiful-jekyll-footer"><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div style=text-align:center><div class=flex-container-footer-badges><a href=https://www.linkedin.com/in/cesarsotovalero><img alt="Connect on LinkedIn" src="https://img.shields.io/badge/LinkedIn-Connect-blue?style=social&logo=linkedin"></a>
<a href=https://www.youtube.com/channel/UCR4rI98w6-MqYoCS6jR9LGg><img alt="Subscribe to my YouTube channel" src="https://img.shields.io/youtube/channel/subscribers/UCR4rI98w6-MqYoCS6jR9LGg?logoColor=black&label=Subscribe"></a>
<a href=https://github.com/cesarsotovalero><img alt="Follow me on GitHub" src="https://img.shields.io/github/followers/cesarsotovalero?label=Follow%20me"></a></div></div><div class=flex-container-footer-copyright><p class="copyright text-muted">&copy; César Soto Valero&nbsp;&nbsp;&bull;&nbsp;&nbsp;2018&ndash;2025</p></div></div></div></div></footer><script>typeof jQuery=='undefined'&&document.write('<script src="/js/jquery-1.11.2.min.js"><\/script>')</script><script src=/js/bootstrap.min.js></script>
<script src=/js/main.js></script>
<script src=/js/flowtype.js></script>
<script src=/js/scroll-to-top.js></script>
<script src=/js/lazysizes.min.js></script>
<script src=/js/popup-footnotes.js></script>
<script src=/js/mermaid.min.js></script>
<script>$(document).ready(function(){mermaid.initialize({startOnLoad:!0,theme:"default"}),window.mermaid.init(void 0,document.querySelectorAll('.language-mermaid'))})</script><script>(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)})(window,document,'script','//www.google-analytics.com/analytics.js','ga'),ga('create','UA-107061705-1','auto'),ga('send','pageview')</script><script src=/js/mode-switcher.js></script></body></html>