<!doctype html><html xmlns=http://www.w3.org/1999/xhtml lang=en-us xml:lang=en-US itemscope itemtype=http://schema.org/WebSite><head><script>(function(){let a=localStorage.getItem('theme');a==='dark'&&document.documentElement.setAttribute('data-theme','dark')})()</script><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><title>From Classical ML to DNNs and GNNs for Real-Time Financial Fraud Detection</title><meta name=keywords content="financial fraudfraud detectionmachine learningdeep learningsurveytransaction monitoring"><link rel=alternate type=application/rss+xml title="César Soto Valero  César - Computer Scientist" href=https://www.cesarsotovalero.net/feed.xml><script src=https://cdn.jsdelivr.net/npm/typed.js@2.0.12></script>
<link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Rouge+Script&display=swap" rel=stylesheet><link rel=apple-touch-icon sizes=180x180 href=../img/favicon/redketchup/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../img/favicon/redketchup/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../img/favicon/redketchup/favicon-16x16.png><link rel=manifest href=../img/favicon/redketchup/site.webmanifest><link id=code rel=stylesheet href=../css/pygment_highlights.css><script src=https://d3js.org/d3.v7.min.js></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-107061705-1"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','UA-107061705-1')</script><script type=text/javascript>(function(a,e,b,f,g,c,d){a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},c=e.createElement(f),c.async=1,c.src="https://www.clarity.ms/tag/"+g,d=e.getElementsByTagName(f)[0],d.parentNode.insertBefore(c,d)})(window,document,"clarity","script","bs3gcidnol")</script><script src=../js/anchor.min.js></script>
<link rel=stylesheet href=//use.fontawesome.com/releases/v5.12.0/css/all.css><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/bootstrap-social.css><link rel=stylesheet href=/css/main.css><link rel=stylesheet href=/css/academicons.css><link rel=stylesheet href="//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic"><link rel=stylesheet href="//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800"><meta name=generator content="Jekyll v4.3.2"><meta property="og:title" content="From Classical ML to DNNs and GNNs for Real-Time Financial Fraud Detection"><meta name=author content="César Soto Valero"><meta property="og:locale" content="en_US"><meta name=description content="Financial transaction fraud is a pervasive problem costing institutions and customers billions annually. This survey reviews the current state-of-the-art in real-time transaction fraud detection, spanning both academic research and industry adopted solutions."><meta property="og:description" content="Financial transaction fraud is a pervasive problem costing institutions and customers billions annually. This survey reviews the current state-of-the-art in real-time transaction fraud detection, spanning both academic research and industry adopted solutions."><link rel=canonical href=https://www.cesarsotovalero.net/blog/from-classical-ml-to-dnns-and-gnns-for-real-time-financial-fraud-detection.html><meta property="og:url" content="https://www.cesarsotovalero.net/blog/from-classical-ml-to-dnns-and-gnns-for-real-time-financial-fraud-detection.html"><meta property="og:site_name" content="César Soto Valero"><meta property="og:image" content="https://www.cesarsotovalero.net/img/posts/2025/2025-04-03/justitiabrunnen_cover.jpg"><meta property="og:type" content="article"><meta property="article:published_time" content="2025-04-03T00:00:00-07:00"><meta name=twitter:card content="summary"><meta property="twitter:image" content="https://www.cesarsotovalero.net/img/posts/2025/2025-04-03/justitiabrunnen_cover.jpg"><meta property="twitter:title" content="From Classical ML to DNNs and GNNs for Real-Time Financial Fraud Detection"><meta name=twitter:site content="@cesarsotovalero"><meta name=twitter:creator content="@César Soto Valero"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"César Soto Valero","url":"https://www.cesarsotovalero.net/about-me"},"dateModified":"2025-07-29T11:52:25-07:00","datePublished":"2025-04-03T00:00:00-07:00","description":"Financial transaction fraud is a pervasive problem costing institutions and customers billions annually. This survey reviews the current state-of-the-art in real-time transaction fraud detection, spanning both academic research and industry adopted solutions.","headline":"From Classical ML to DNNs and GNNs for Real-Time Financial Fraud Detection","image":"https://www.cesarsotovalero.net/img/posts/2025/2025-04-03/justitiabrunnen_cover.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.cesarsotovalero.net/blog/from-classical-ml-to-dnns-and-gnns-for-real-time-financial-fraud-detection.html"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://www.cesarsotovalero.net/img/pages/cesar/avatar-icon-2024.jpg"},"name":"César Soto Valero"},"url":"https://www.cesarsotovalero.net/blog/from-classical-ml-to-dnns-and-gnns-for-real-time-financial-fraud-detection.html"}</script><script type=text/x-mathjax-config> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } });
   </script><script type=text/x-mathjax-config>
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          processEscapes: true
        }
      });
   </script><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type=text/javascript></script></head><body><nav class="navbar-fixed-top navbar-custom navbar navbar-expand-lg navbar-light bg-light"><button class=navbar-toggle type=button data-toggle=collapse data-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="collapse navbar-collapse container-fluid" id=navbarSupportedContent><ul class="navigation list-inline text-center footer-links" id=black-icons><li class="nav-item navbar-custom" title=Home><a id=navbar-brand href=https://www.cesarsotovalero.net>Home</a></li><li class="nav-item navbar-custom"><a href=/blog>Blog</a></li><li class="nav-item navbar-custom"><a href=/linkedin>LinkedIn</a></li><li class="nav-item navbar-custom"><a href=/youtube>YouTube</a></li><li class="nav-item navbar-custom"><a href=/podcasts>Podcasts</a></li><li class="nav-item navbar-custom"><a href=/talks>Talks</a></li><li class="nav-item navbar-custom"><a href=/about-me>About</a></li><li class="nav-item navbar-custom" title="Toggle Night Mode"><a href=# id=theme-toggle onclick=modeSwitcher() style=cursor:pointer></a></li></ul></div></nav><header class=header-section><div class="intro-header no-img"><div class=container><div class=row><div class=header-image style=padding-left:15px><div class="row justify-content-center" style=margin-right:0;margin-left:0></div></div><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=post-heading><h1 class=no-anchor>From Classical ML to DNNs and GNNs for Real-Time Financial Fraud Detection</h1><h2 class=post-subheading>A survey of state-of-the-art techniques</h2><div class="container flex-container"><a href=https://www.cesarsotovalero.net/about-me><img class=avatar-img-small src=/img/pages/cesar/avatar-icon-2024.jpg alt="César Soto Valero" id=meta-img></a><div class=flex-item><span class=post-meta>Posted on April 3, 2025</span><div><span class=post-meta title="Estimated read time"><svg id="i-clock" viewbox="0 0 32 32" width="18" height="18" style="vertical-align:middle" fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="3"><circle cx="16" cy="16" r="14"/><path d="M16 8v8l4 4"/></svg> 43 mins read</span>
<span class="blog-tags post-meta"><svg id="i-tag" aria-hidden="true" focusable="false" data-prefix="far" data-icon="tag" role="img" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 512 512" class="svg-inline--fa fa-tag fa-w-16" width="16.5" height="16.5"><path fill="none" d="M497.941 225.941 286.059 14.059A48 48 0 00252.118.0H48C21.49.0.0 21.49.0 48v204.118a47.998 47.998.0 0014.059 33.941l211.882 211.882c18.745 18.745 49.137 18.746 67.882.0l204.118-204.118c18.745-18.745 18.745-49.137.0-67.882zM259.886 463.996 48 252.118V48h204.118L464 259.882 259.886 463.996zM192 144c0 26.51-21.49 48-48 48s-48-21.49-48-48 21.49-48 48-48 48 21.49 48 48z"/></svg>ai</span></div></div></div><div class=flex-item><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1" style=margin-left:0><section id=share-section><script type=text/javascript src=https://storage.ko-fi.com/cdn/widget/Widget_2.js></script><script type=text/javascript>kofiwidget2.init('Support this blog','#7aa4d1','G2G3CRPPI'),kofiwidget2.draw()</script></section></div></div></div><div class=cesarcarbon id=cesarcarbonads-post><script async type=text/javascript src="//cdn.carbonads.com/carbon.js?serve=CESI52JM&placement=wwwcesarsotovaleronet" id=_carbonads_js></script></div></div></div><div class="col-lg-4 col-lg-pull-2 col-md-2 col-md-pull-2"><ol id=toc class=section-nav><li class="toc-entry toc-h1"><a href=#classical-ml-models>Classical ML Models</a></li><li class="toc-entry toc-h1"><a href=#deep-learning-models>Deep Learning Models</a></li><li class="toc-entry toc-h1"><a href=#graph-based-models>Graph-Based Models</a></li><li class="toc-entry toc-h1"><a href=#transformer-models>Transformer Models</a></li><li class="toc-entry toc-h1"><a href=#appendix>Appendix</a></li><li class="toc-entry toc-h1"><a href=#external-resources>External Resources</a></li><li class="toc-entry toc-h1"><a href=#footnotes>Footnotes</a></li></ol></div></div></div></div></header><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-AMS-MML_HTMLorMML,https://idcrook.github.io/assets/js/MathJaxLocal.js"></script><div class=container><div class=row><div class="col-lg-9 col-lg-offset-2 col-md-10 col-md-offset-1"><article role=main class=blog-post><p>Financial fraud is a pervasive problem costing institutions and customers billions annually.<sup id=fnref:1><a href=#fn:1 class=footnote rel=footnote role=doc-noteref>1</a></sup>
Most known examples include credit card fraud, fraudulent online payments, and money laundering.
Banks worldwide faced an estimated \(\$442\) billion in fraud-related losses in 2023 alone.
In particular, credit card transactional fraud is projected to reach \(\$43\) billion in annual losses by 2026.
Beyond direct losses, fraud undermines customer trust and damages banks’ reputation.
For example, it leads to false positives where legitimate transactions are wrongly blocked.
Consequently, financial fraud detection systems (a.k.a fraud scoring) must not only catch as much fraud as possible but also minimize false positives.</p><p>Fraudsters’ tactics evolve rapidly.
Traditional rule-based systems (or simple statistical methods) have proven inadequate against today’s adaptive fraud models.
On one hand, fraudsters form complex schemes and exploit networks of accounts.
On the other hand, legitimate transaction volumes continue to grow due to the rise of e-commerce and digital payments.</p><p>This situation has driven a shift toward Machine Learning (ML) and AI-based approaches that can learn subtle patterns and adapt over time.
Critically, financial fraud detection must happen in real-time (or near-real time) to intervene before fraudsters can complete illicit transactions.
Catching fraud “closer to the time of fraud occurrence is key” so that suspicious transactions can be blocked or flagged immediately.<sup id=fnref:2><a href=#fn:2 class=footnote rel=footnote role=doc-noteref>2</a></sup></p><p>This article deep dives into the current state-of-the-art of real-time transactional fraud detection, spanning both academic research and current industry practices.</p><p>I cover the major model families used today:</p><ol><li><strong>Classical ML models:</strong> Logistic regression, decision trees, random forests, and SVMs.</li><li><strong>Deep Learning models:</strong> ANNs, CNNs, RNNs/LSTMs, autoencoders, and GANs.</li><li><strong>Graph-based models:</strong> GNNs and graph algorithms that leverage transaction relationships.</li><li><strong>Transformer-based and foundation models:</strong> Large pre-trained models like Stripe’s payments foundation model.</li></ol><p>For each category, I discuss representative use cases or studies, highlight strengths and weaknesses, and comment on their suitability for real-time fraud detection.</p><h1 id=classical-ml-models>Classical ML Models</h1><p>Classical ML algorithms have long been used in fraud detection and remain strong baselines in both research and production systems.
These include linear models like <a href=https://en.wikipedia.org/wiki/Logistic_regression>logistic regression</a>, distance-based classifiers like <a href=https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm>k-Nearest Neighbors</a>, and tree-based models such as <a href=https://en.wikipedia.org/wiki/Random_forest>random forest</a> and <a href=https://en.wikipedia.org/wiki/Gradient_boosting>gradient boosted trees</a> (e.g., <a href=https://xgboost.readthedocs.io/>XGBoost</a>).
These approaches operate on hand-crafted features derived from transaction data (e.g., <code class="language-plaintext highlighter-rouge">transaction_amount</code>, <code class="language-plaintext highlighter-rouge">location</code>, <code class="language-plaintext highlighter-rouge">device_id</code>, <code class="language-plaintext highlighter-rouge">time_of_day</code>, etc.), often requiring substantial feature engineering by domain experts.</p><p><strong>Logistic regression</strong> is a foundational model in fraud detection.
Banks and financial institutions have historically relied on it due to its simplicity and interpretability (each coefficient \(w_i\) has a direct and intuitive meaning). A positive coefficient means the feature increases the log-odds of fraud, a negative coefficient means it decreases the risk.</p>\[P(y = 1 \mid \mathbf{x}) = \frac{1}{1 + e^{-(\mathbf{w}^\top \mathbf{x} + b)}}\]<ul><li>\(\mathbf{x}\): Feature vector (e.g., transaction amount, time of day, merchant category)</li><li>\(\mathbf{w}\): Coefficients (risk factors)</li><li>\(b\): Bias or intercept</li></ul><p>Even today, logistic models serve as interpretable baseline detectors and are sometimes combined with a <a href=https://en.wikipedia.org/wiki/Business_rule_management_system>Business Rule Management Systems</a>.
However, linear models struggle to capture complex non-linear patterns in large transaction datasets.</p><aside class=quote><em>“XGBoost builds trees sequentially, where each tree learns from the mistakes of the previous ones.”</em></aside><p><strong>Decision trees</strong> and ensemble forests address this by automatically learning non-linear splits and interactions.
In fact, boosted decision tree ensembles (like XGBoost) became popular in fraud detection competitions and industry solutions due to their high accuracy on tabular data.<sup id=fnref:34><a href=#fn:34 class=footnote rel=footnote role=doc-noteref>3</a></sup>
These models can capture anomalous combinations of features in individual transactions effectively, learning complex, non-linear interactions between features.
For example, <a href=https://github.com/VedangW/ieee-cis-fraud-detection>the winning solutions</a> of the <a href=https://www.kaggle.com/c/ieee-fraud-detection/overview>IEEE-CIS fraud detection Kaggle challenge</a> (2019) heavily used engineered features fed into gradient boosting models, achieving strong performance (AUC ≈ 0.91).</p><p><strong>Support Vector Machines</strong> (<a href=https://en.wikipedia.org/wiki/Support_vector_machine>SVMs</a>) have also been explored in academic studies.
However, while they can model non-linear boundaries (with kernels), they tend to be computationally heavy for large datasets and offer no interpretable output.
Therefore, the industry has gravitated more to tree ensembles for complex models.</p><h2 id=strengths>Strengths</h2><p>Classical ML models are typically fast to train and infer, and many (especially logistic regression and decision trees) are relatively easy to interpret.
For instance, a logistic regression might directly quantify how much a mismatched billing address raises fraud probability, and a decision tree might provide a rule-like structure (e.g., “if IP country ≠ card country and amount > $1000 ⇒ flag fraud”).
More complex models like XGBoost still allow some interpretability through <a href=https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/>feature importance scores</a>, <a href=https://medium.com/@lfoster49203/interpretable-machine-learning-models-using-shap-and-lime-for-complex-data-6f65e1224209>CHAP values</a>, or partial dependence plots.</p><p>Classical ML models can be deployed in real-time with minimal latency.
A logistic regression is essentially a dot-product of features, and even a large XGBoost ensemble can score a transaction in tens of milliseconds or less on modern hardware.</p><p>They also perform well with limited data.
With careful feature engineering, a simple model can already catch a large fraction of fraud patterns.
Consequently, industry adoption is widespread, many banks initially deploy logistic or tree-based models in production, and even today XGBoost is a common choice in fraud ML pipelines.</p><h2 id=weaknesses>Weaknesses</h2><p>A key limitation of classical ML models is the reliance on manual feature engineering.
In other words, they cannot automatically invent new abstractions beyond the input features given.
These models may miss complex patterns such as sequential spending behavior or R-ring collusion<sup id=fnref:28><a href=#fn:28 class=footnote rel=footnote role=doc-noteref>4</a></sup> between groups of accounts unless analysts explicitly code such features (e.g., number of purchases in the last hour, or count of accounts sharing an email domain).</p><p>They may also struggle with high-dimensional data like raw event logs or image data (this is where deep learning excels).
However, this is less an issue for structured transaction records.</p><p>Another challenge is class imbalance.
The occurrence of fraud is typically rare (often $ &lt;1\% $ of transactions), which can bias models to predict the majority “non-fraud” class.
Techniques like <a href=https://medium.com/@ravi.abhinav4/improving-class-imbalance-with-class-weights-in-machine-learning-af072fdd4aa4>balanced class weighting</a>, <a href=https://www.kaggle.com/code/residentmario/undersampling-and-oversampling-imbalanced-data>undersampling</a>, or <a href=https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/>SMOTE</a> are often needed to train classical models effectively on imbalanced fraud data.<sup id=fnref:16><a href=#fn:16 class=footnote rel=footnote role=doc-noteref>5</a></sup></p><p>Finally, while faster than deep neural networks, complex ensembles (hundreds of trees) can become memory-intensive and may require optimization for ultra-low latency at high transaction volumes.</p><h2 id=real-time-suitability>Real-Time Suitability</h2><p>Classical models are generally well-suited to real-time fraud scoring.
They have low latency inference and modest resource requirements.</p><p>For example, a bank’s fraud engine might run a logistic regression and a few decision tree rules in under 10ms per transaction on a CPU.
Even a sophisticated random forest or gradient boosting model can be served via highly optimized C++ libraries or cloud ML endpoints to meet sub-hundred-millisecond SLAs.<sup id=fnref:29><a href=#fn:29 class=footnote rel=footnote role=doc-noteref>6</a></sup></p><p>The straightforward nature of these models also simplifies transaction monitoring and model updates.
New data can be used to frequently retrain or update coefficients (even via <a href=https://en.wikipedia.org/wiki/Online_machine_learning>online learning</a> for logistic regression).
The main caution is that if fraud patterns shift significantly (<a href=https://en.wikipedia.org/wiki/Concept_drift>concept drift</a>), purely static classical models will need frequent retraining to keep up.</p><p>In practice, many organizations retrain or fine-tune their fraud models on recent data weekly or even daily to adapt to new fraud tactics.
So, while classical models are fast to deploy and iterate on, they do require ongoing maintenance to remain effective.</p><h2 id=examples>Examples</h2><p>Representative research and use-cases for classical methods include:</p><ul><li><strong>Logistic regression and decision trees as baseline models:</strong> Many banks have deployed logistic regression for real-time credit card fraud scoring due to its interpretability.</li><li><strong>Ensemble methods in academic studies:</strong> Research has focused on evaluating logistic vs. decision tree vs. random forest on a credit card dataset (often finding tree ensembles outperform linear models in Recall).<sup id=fnref:17><a href=#fn:17 class=footnote rel=footnote role=doc-noteref>7</a></sup></li><li><strong>Kaggle competitions:</strong> XGBoost was heavily used in the <a href=https://www.kaggle.com/c/ieee-fraud-detection>Kaggle IEEE-CIS 2019 competition</a>, leveraging high accuracy on tabular features.</li><li><strong>Hybrid systems:</strong> Many production systems combine manual business rules for known high-risk patterns with an ML model for subtler patterns, using the rules for immediate high-precision flags and the ML model for broad coverage.</li></ul><h1 id=deep-learning-models>Deep Learning Models</h1><p>In recent years, <a href=https://en.wikipedia.org/wiki/Deep_learning#Deep_neural_networks>Deep Neural Networks</a> (DNNs) have been <a href=https://opencv.org/blog/online-transaction-fraud-detection-using-deep-learning>applied to transaction fraud detection</a> with promising results.
DNNs can automatically learn complex feature representations from raw data, potentially capturing patterns that are hard to manually engineer or find with classical ML models.</p><h2 id=deep-learning-architectures>Deep Learning Architectures</h2><p>Several deep architectures have been explored for fraud detection.
Below, I summarize the most common types.</p><h3 id=feed-forward-neural-networks-anns>Feed-Forward Neural Networks (ANNs)</h3><p><a href=https://en.wikipedia.org/wiki/Feedforward_neural_network>ANNs</a> are multi-layer perceptron treating each transaction’s features as input neurons.
These can model non-linear combinations of features beyond what logistic regression can capture.
In practice, simple feed-forward networks have been used as a baseline deep model for fraud (e.g., a 3-layer network on credit card data).
They often perform similarly to tree ensembles if ample data is available but are harder to interpret.
They also don’t inherently handle sequential or time-based information beyond what the input features provide.</p><h3 id=convolutional-neural-networks-cnns>Convolutional Neural Networks (CNNs)</h3><p><a href=https://en.wikipedia.org/wiki/Convolutional_neural_network>CNNs</a> are most famous for image-related tasks.
However, they have also being been applied to fraud by treating transaction data as temporal or spatial sequences.
For example, a CNN can slide over a sequence of past transactions for a user to detect local patterns or use 1D convolution on time-series of transaction amounts.</p><p>CNNs excel at automatic feature extraction of localized patterns.
Some research reformats transaction histories into a 2D “image” (e.g., time vs. feature dimension) so that CNNs can detect anomalous shapes.</p><p>CNNs for detecting fraud have seen limited but growing use.
One recent study reported ~99% detection accuracy with a CNN on a credit card dataset.<sup id=fnref:19><a href=#fn:19 class=footnote rel=footnote role=doc-noteref>8</a></sup>
However, such high accuracy is likely due to the highly imbalanced nature of the dataset (using AUC or F1 is more meaningful).</p><h3 id=recurrent-neural-networks-rnns>Recurrent Neural Networks (RNNs)</h3><p><a href=https://en.wikipedia.org/wiki/Recurrent_neural_network>RNNs</a>, including <a href=https://en.wikipedia.org/wiki/Long_short-term_memory>LSTM</a> and <a href=https://en.wikipedia.org/wiki/Gated_recurrent_unit>GRU</a> networks, are well-suited for sequential transactional data.
They maintain a memory of past events, making them ideal for modeling an account’s behavior over time.</p><p>For example, an LSTM can consume a customer’s sequence of transactions (with timestamps) and detect if the latest transaction is anomalous given the recent pattern.
This temporal modeling is very powerful for fraud because many fraud patterns only make sense in context (e.g., a sudden spending spike, or a purchase in a new country right after another far-away purchase).</p><p>Research has shown LSTM-based models can effectively distinguish fraudulent vs. legitimate sequences.
In one case, an LSTM achieved significantly higher Recall than static models by catching subtle temporal shifts in user behavior.<sup id=fnref:13><a href=#fn:13 class=footnote rel=footnote role=doc-noteref>9</a></sup>
RNNs do require sequential data, so for one-off transactions without history they are less applicable (unless modeling at the merchant or account aggregate level).</p><h3 id=autoencoders>Autoencoders</h3><p><a href=https://en.wikipedia.org/wiki/Autoencoder>Autoencoders</a> are unsupervised anomaly detection models that learn to compress and reconstruct data.
When trained on predominantly legitimate transactions, an autoencoder captures the underlying structure of normal behavior (a.k.a. the “normal manifold”).
As a result, it can reconstruct typical transactions with very low error, but struggles with atypical or anomalous ones.
A transaction that doesn’t conform to the learned normal pattern will produce a higher reconstruction error.
By setting a threshold, we can flag transactions with unusually high reconstruction error as potential fraud.</p><p>Autoencoders shine in fraud detection, particularly when labeled fraud data is scarce or nonexistent.<sup id=fnref:33><a href=#fn:33 class=footnote rel=footnote role=doc-noteref>10</a></sup>
Their strength lies in identifying transactions that deviate from the learned “normal” without requiring explicit fraud labels during training.
For example, an autoencoder trained on millions of legitimate transactions will likely assign high reconstruction error to fraudulent ones it’s never seen before.
<a href=https://en.wikipedia.org/wiki/Variational_autoencoder>Variational Autoencoder</a>s (VAEs), which introduce probabilistic modeling and latent-space regularization—have also been explored for fraud detection, offering potentially richer representations of normal transaction behavior.<sup id=fnref:21><a href=#fn:21 class=footnote rel=footnote role=doc-noteref>11</a></sup></p><h3 id=generative-adversarial-networks-gans>Generative Adversarial Networks (GANs)</h3><p><a href=https://en.wikipedia.org/wiki/Generative_adversarial_network>GANs</a> consist of a generator and discriminator.
The generator creates synthetic data, while the discriminator tries to distinguish real from fake data.</p><p>There are two main applications of GANs in fraud detection:</p><ol><li><p><strong>Generate realistic synthetic fraud examples:</strong> GANs can augment training data to address class imbalance. The generator is trained to produce fake transactions that the discriminator (trained to distinguish real vs. fake) finds plausible. By adding these synthetic frauds to the training set, models (including non-deep models) can learn a broader decision boundary.</p></li><li><p><strong>Serve as anomaly detectors:</strong> The generator tries to model the distribution of legitimate transactions, and the discriminator’s output can highlight outliers.</p></li></ol><p>Some financial institutions have experimented with GANs.
For example, <a href=https://developer.nvidia.com/blog/detecting-financial-fraud-using-gans-at-swedbank-with-hopsworks-and-gpus/>Swedbank reportedly used GANs</a> to generate additional fraudulent examples for training their models.
However, GAN training can be complex and less common in production.
Still, in research, GAN-based methods have shown improved Recall by expanding the fraud training sample space.<sup id=fnref:22><a href=#fn:22 class=footnote rel=footnote role=doc-noteref>12</a></sup></p><h3 id=hybrid-deep-learning-models>Hybrid Deep Learning Models</h3><p>There are also custom DNNs architectures combining elements of the above, or combining deep models with classical ones.</p><p>For example, a “wide and deep model” might have a linear (wide) component for memorizing known risk patterns and a neural network (deep) component for generalization.
Another example is combining an LSTM for sequence modeling with a feed-forward network for static features (“dual-stream” models).</p><p>Ensembles of deep and non-deep models have also been used (e.g., using an autoencoder’s anomaly score as an input feature to a random forest).
Recent research explores stacking deep models with tree models to improve robustness and interpretability.</p><h2 id=strengths-1>Strengths</h2><p>DNNs biggest advantage is automated feature learning.
These types of models can uncover intricate, non-linear relationships and subtle correlations within massive datasets that older methods miss.
They can digest raw inputs (inc. unstructured data) and find patterns without explicit human-designed features.
For instance, an RNN can learn the notion of “rapid spending spree” or “geographical inconsistency” from raw sequences, which would be hard to capture with handcrafted features.</p><p>In fraud detection, large payment companies have millions of transactions which deep models can leverage to potentially exceed the accuracy of simpler models.
DNNs also tend to improve with more data, whereas classical models may saturate in performance.</p><p>Another strength is handling complex data types.
For example, if one incorporates additional signals like device fingerprints, text (e.g., product names), or network information, deep networks can combine these modalities more seamlessly (e.g., an embedding layer for device ID, an LSTM for text description, etc.).</p><p>In practice, DNNs have shown higher Recall at a given false-positive rate compared to classical models, in several cases.<sup id=fnref:13:1><a href=#fn:13 class=footnote rel=footnote role=doc-noteref>9</a></sup>
They are also adaptive architectures like RNNs or online learning frameworks can update as new data comes in, enabling continuous learning, which is important as fraud scenarios evolve.</p><h2 id=weaknesses-1>Weaknesses</h2><p>The primary downsides of DNN are complexity and interpretability.</p><p>Deep networks are considered “black boxes”, meaning that it’s non-trivial to explain why a certain transaction was flagged as fraudulent.
This is problematic for financial institutions that need to justify decisions to customers or regulators.
Techniques like <a href=https://shap.readthedocs.io/>SHapley Additive exPlanations</a> (SHAP) or <a href=https://github.com/marcotcr/lime>Local Interpretable Model-Agnostic Explanations</a> (LIME) can help interpret feature importance for deep models, <a href=https://www.milliman.com/en/insight/Explainable-AI-in-fraud-detection>but it’s still harder</a> compared to a linear model or decision tree.</p><aside class=quote><em>“DNNs can really shine only when there are huge datasets or additional unlabeled data to pre-train on.”</em></aside><p>Another issue is the data and compute requirement.
Training large DNNs may require GPUs and extensive hyperparameter tuning, which can be overkill for some fraud datasets, especially if data is limited or highly imbalanced.<sup id=fnref:32><a href=#fn:32 class=footnote rel=footnote role=doc-noteref>13</a></sup>
In fact, many academic studies on the popular <a href=https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud>Kaggle credit card dataset</a> (284,807 transactions) found that simpler models can match DNNs performance, likely because the dataset is small and mostly numeric features.</p><p>Overfitting is a risk too, fraud datasets are skewed and sometimes composed of static snapshots in time.
A DNN might memorize past fraud patterns that fraudsters no longer use, if not carefully regularized.</p><p>Finally, latency can be a concern.
A large CNN or LSTM might take longer to evaluate than a logistic regression.
However, many deep models used for fraud are not excessively large (e.g., an LSTM with a few hundred units), and with optimized inference (batching, quantization, etc.) they can often still meet real-time requirements.
I discuss latency more later, but suffice it to say that deploying deep models at scale might necessitate GPU acceleration or model optimizations in high-throughput environments.</p><h2 id=real-time-suitability-1>Real-Time Suitability</h2><p>DNNs models can be deployed for real-time fraud scoring, but it requires more care than classical models.
Simpler networks (small MLPs) are no issue in real-time.
However, RNNs or CNNs might introduce slight latency (tens of milliseconds).
Nevertheless, modern inference servers and even FPGAs/TPUs can handle thousands of inferences per second.
For instance, Visa reportedly targets fraud model evaluations in under ~25ms as part of their payment authorization pipeline.
It’s feasible to achieve this with a moderately sized neural network and good infrastructure.</p><p>Scaling to high transaction volumes is another aspect.
Deep models may consume more CPU/GPU resources, so a cloud deployment might need to autoscale instances or use GPU inference for peak loads.</p><p>A potential strategy for real-time use is a two-stage system: a fast classical model first filters obvious cases (either definitely legitimate or obviously fraudulent), and a slower deep model only analyzes the ambiguous middle chunk of transactions.
This way, the heavy model is used on a fraction of traffic to keep overall throughput high.</p><p>Additionally, organizations often maintain a feedback loop.
Flagged predictions are first reviewed by analysts or via outcomes like chargebacks, and then a DNN model is retrained frequently to incorporate the latest data.</p><p>Some deep models can be updated via online learning.
For example, an RNN that continuously updates its hidden state or a streaming NN that periodically retrains on a rolling window of data, which helps keep them current with
concept drift.</p><h2 id=examples-1>Examples</h2><p>Notable examples of deep learning in fraud detection:</p><ul><li><strong>Feedforward DNNs:</strong> PayPal in the mid-2010s <a href="https://www.paypal.com/us/brc/article/payment-fraud-detection-machine-learning?utm_source=chatgpt.com">applied neural networks to fraud</a>, fintech companies like Feedzai have further advanced this methodology by combining DNNs with tree-based models.<sup id=fnref:24><a href=#fn:24 class=footnote rel=footnote role=doc-noteref>14</a></sup></li><li><strong>RNNs and LSTMs:</strong> Multiple studies have shown that LSTM networks can detect sequential fraud behavior that static models miss, improving Recall by capturing temporal patterns. Large merchants have employed LSTM-based models to analyze user event streams, enabling the detection of account takeovers and session-based fraud in real-time.</li><li><strong>Autoencoder-based anomaly detection:</strong> Unsupervised autoencoders have been used by banks to flag new types of fraud. For instance, an autoencoder trained on normal mobile transactions flagged anomalies that turned out to be new fraud rings exploiting a loophole (detected via high reconstruction error).</li><li><strong>Hybrid models:</strong> Recent trends include using DNNs to generate features for a gradient boosted tree. One effective approach is to use deep learning models, such as autoencoders or embedding networks, to learn rich feature representations from transaction data. These learned embeddings are then fed into XGBoost, combining the deep models’ ability to capture complex patterns with the interpretability and efficiency of tree-based methods</li></ul><h1 id=graph-based-models>Graph-Based Models</h1><p>Groups of fraudsters might share information (e.g., using the same stolen cards or devices), or a single fraudster might operate many accounts that transact with each other.
A powerful class of methods treats the financial system as a graph, linking entities like users, accounts, devices, IP addresses, merchants, etc.
<a href=https://github.com/safe-graph/graph-fraud-detection-papers>Graph-based fraud detection models</a> aim to exploit these relational structures to detect fraud patterns that single-transaction models might miss.
Classical graph algorithms can then be applied, such as community detection<sup id=fnref:25><a href=#fn:25 class=footnote rel=footnote role=doc-noteref>15</a></sup> and link analysis (e.g., <a href=https://en.wikipedia.org/wiki/PageRank>PageRank</a> on the fraud graph).</p><figure class=jb_picture><img width=100% style="border:1px solid gray" src=/assets/resized/suspicious-subgraphs-640x378.png alt="Illustration of entity linkages in transaction fraud" data-srcset="/assets/resized/suspicious-subgraphs-640x378.png 640w,/assets/resized/suspicious-subgraphs-768x454.png 768w," class="blur-up lazyautosizes lazyload"><figcaption class=stroke>Illustration of entity linkages in transaction fraud: Shared devices, phone numbers, and locations connect different users. Fraudsters (devil icons) may create many accounts that all link through common data points (phone, IP, geo), forming <b>suspicious</b> subgraphs that graph-based methods can detect.</figcaption></figure><p>For example, in a bipartite graph of credit card transactions, one set of nodes represent cardholders, another set are merchants, and there is an edge connecting a cardholder to a merchant for each transaction.
Fraudulent cards might cluster via merchant edges (e.g., a fraud ring testing many stolen cards at one merchant), or vice versa.<sup id=fnref:35><a href=#fn:35 class=footnote rel=footnote role=doc-noteref>16</a></sup>
Similarly, for online payments we can create nodes for user accounts, email addresses, IP addresses, device IDs, etc., and connect nodes that are observed together in a transaction or account registration.
This yields a rich heterogeneous graph of entities.</p><p><a href=https://en.wikipedia.org/wiki/Graph_neural_network>Graph Neural Networks</a> (GNNs) in recent years has led to many applications of this technology in fraud detection.<sup id=fnref:23><a href=#fn:23 class=footnote rel=footnote role=doc-noteref>17</a></sup> <sup id=fnref:30><a href=#fn:30 class=footnote rel=footnote role=doc-noteref>18</a></sup> <sup id=fnref:31><a href=#fn:31 class=footnote rel=footnote role=doc-noteref>19</a></sup>
GNNs are deep learning models designed for graph-structured data.
They propagate information along edges, allowing each node to aggregate features from its neighbors.
In fraud terms, a GNN can learn to identify suspicious nodes (e.g., users or transactions) by looking at their connected partners.
For instance, if a particular device ID node connects to many user accounts that were later flagged as fraud, a GNN can learn to embed that device node as high-risk, which in turn raises the risk of any new account connected to it.</p><aside class=quote><em>“Fraud is rarely a problem of isolated events… fraudsters operate within complex networks.”</em></aside><p>GNNs consider connections between accounts and transactions to reveal patterns of suspicious activity across the network.
By incorporating relational context, GNNs have demonstrated higher fraud detection accuracy and fewer false positives than models that ignore graph structure.
For example, combining GNNs features with an XGBoost classifier led to catching fraud that would otherwise go undetected and reducing false alarms due to the added network context.
A GNN approach might catch a seemingly normal transaction if the card, device, or IP involved has connections to known frauds that a non-graph model wouldn’t see.</p><p>Several types of GNNs architectures have been used.
Notably, <a href=https://paperswithcode.com/method/gcn>Graph Convolutional Networks</a> (GCN), <a href=https://neo4j.com/docs/graph-data-science/current/machine-learning/node-embeddings/graph-sage/>GraphSAGE</a>, heterogeneous GNNs for multi-type node graphs, and even <a href=https://paperswithcode.com/method/graph-transformer>Graph Transformers</a>.</p><p>A popular benchmark for GNNs is the <a href=https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.EllipticBitcoinDataset.html>Elliptic dataset</a>, a Bitcoin transaction graph where GNNs have been applied to identify illicit transactions by classifying nodes in a large transaction graph.
GNNs have also been applied to credit card networks: e.g., researchers have built graphs linking credit card numbers, merchants, and phone numbers, and used a heterogeneous GNN to detect fraud cases involving synthetic identities and collusive merchants.<sup id=fnref:3><a href=#fn:3 class=footnote rel=footnote role=doc-noteref>20</a></sup></p><h2 id=strengths-2>Strengths</h2><p>Graph-based methods can detect patterns of collusion and linkage that purely feature-based models miss.
They effectively augment each transaction with context.
Rather than evaluating an event in isolation, the model considers the broader network (device usage graph, money flow graph, etc.).
This is crucial for catching fraud rings.
For example, multiple accounts controlled by one entity or chains of transactions moving funds, which might appear normal individually but are anomalous in aggregate.
GNNs in particular combine the best of both worlds: they leverage graph structure + attribute features together, learning meaningful representations of nodes/edges.<sup id=fnref:3:1><a href=#fn:3 class=footnote rel=footnote role=doc-noteref>20</a></sup>
This is important when fraudsters deliberately make individual transactions look innocuous but cannot hide the relationships (e.g., reusing the same phone or IP address across many accounts).</p><p>Another advantage is in reducing false positives by providing context.
For example, a transaction with a new device might normally seem risky, but if that device has a long history with the same user and no links to bad accounts, a graph model can recognize it as low risk, avoiding a false alarm.
Industry reports indicate that adding graph features or GNNs outputs has improved Precision of fraud systems by filtering out cases that looked suspicious in isolation but were safe in context.<sup id=fnref:4><a href=#fn:4 class=footnote rel=footnote role=doc-noteref>21</a></sup></p><h2 id=weaknesses-2>Weaknesses</h2><aside class=quote><em>“Current GNNs solutions mainly rely on offline batch training and inference, predicting fraudsters in real-time is crucial but challenging.”</em></aside><p>The biggest challenge is complexity in implementation and deployment.
Building and maintaining the graph data (a.k.a. the “graph pipeline”) is non-trivial.
Transactions arrive in a stream and must update the graph in real-time (e.g., adding new nodes, new edges).
Querying the graph for each new transaction’s neighborhood can be slow if not engineered well.
The inference itself can be heavy.
Running a GNNs means loading a subgraph and doing matrix operations that are costlier than a simple ML model.
Consequently, many current GNNs solutions operate in batch mode (offline).
There are limited reference architectures for real-time GNNs serving, though this is an active development area.</p><p>Another issue is scalability.
Graphs of financial transactions or users can be enormous (millions of nodes, tens of millions of edges).
Training a full GNNs on such a graph might not fit in memory or might be extremely slow without sampling techniques.
Some approaches use graph sampling or partitioning to handle this, or only use GNNs to generate features offline.</p><p>GNNs can be hard to interpret (even more so than regular deep nets) since the features are aggregate of neighbors.
It can be challenging to explain to an analyst why a certain account was flagged: the reason might be “it’s connected to three other accounts that had chargebacks,” which is somewhat understandable, but the GNN’s learned weights on those connections are not human-interpretable beyond that concept.</p><h2 id=real-time-suitability-2>Real-Time Suitability</h2><p>Real-time deployment of graph-based models is at the cutting edge.
It is being done in industry but often with approximations.
One pragmatic solution is to use graph analytics to create additional features for a traditional model.
For example, compute features like “number of accounts sharing this card’s IP address that were fraud” or “average fraud score of neighbors” and update these in real-time, then let a gradient boosting model or neural network consume them.
This doesn’t require full GNNs online inference, but captures some graph insights.
However, truly deploying a GNNs in production for each event requires a fast graph database or in-memory graph store.</p><p>AWS demonstrated a prototype using Amazon Neptune (graph DB) + DGL (Deep Graph Library) to serve GNNs predictions in real-time by querying a subgraph around the target node for each inference.<sup id=fnref:3:2><a href=#fn:3 class=footnote rel=footnote role=doc-noteref>20</a></sup>
This kind of pipeline can risk score a transaction within seconds, which may be acceptable for certain use cases (e.g., online account opening fraud).
However, for high-frequency card transactions that need sub-second decisions, a full GNNs might still be too slow today unless heavily optimized.</p><p>An alternative is what Nvidia suggests: use GNNs offline to produce node embeddings or risk scores, then feed those into a superfast inference system (like an XGBoost model or a rules engine) for the real-time decision.<sup id=fnref:4:1><a href=#fn:4 class=footnote rel=footnote role=doc-noteref>21</a></sup>
This hybrid approach was shown to work at large scale, where GNN-based features improved detection by even a small percent (say 1% AUC gain), which for big banks translates to millions saved.</p><p>Lastly, maintaining graph models demands continuous updates as the graph evolves.
This is still manageable, as new data can be incrementally added, but one must watch for concept drift in graph structure.
For example, fraud rings forming new connectivity patterns.</p><h2 id=examples-2>Examples</h2><p>Representative examples of graph-based fraud detection:</p><ul><li><strong>Blockchain networks:</strong> The <a href=https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.EllipticBitcoinDataset.html>Elliptic Bitcoin Dataset</a> is a graph of 203,769 transactions (nodes) with known illicit vs. licit labels. GNNs models on this dataset achieved strong results, showing that analyzing the transaction network is effective for detecting illicit cryptocurrency flows.</li><li><strong>Credit card networks:</strong> Researchers built a graph of credit card transaction and applied a GNNs which outperformed a baseline MLP by leveraging connections (e.g., card linked to a fraudulent merchant gives card a higher fraud probability).</li><li><strong>E-commerce networks:</strong> Companies like Alibaba and PayPal have internal systems modeling user networks. For example, accounts connected via a shared device or IP can indicate <a href=https://en.wikipedia.org/wiki/Sybil_attack>sybil attacks</a> or mule accounts. Graph algorithms identified clusters of accounts that share many attributes (forming fraud communities) which were then taken down as a whole.</li><li><strong>Telecom identity fraud:</strong> Graphs connecting phone numbers, IDs, and addresses have been used to catch identity fraud rings. A famous case is detecting “bust-out fraud” in which a group of credit card accounts all max out and default: the accounts often share phone or address; linking them in a graph helps catch the ring before the bust-out completes.</li><li><strong>Social networks:</strong> In social finance platforms or peer-to-peer payments, graph methods are used to detect money laundering or collusion by analyzing the network of transactions among users (e.g., unusually interconnected payment groups).</li></ul><p>Overall, graph-based methods, especially GNNs, represent a cutting-edge approach that can significantly enhance fraud detection by considering relational data.
As tooling and infrastructure improve (graph databases, streaming graph processing), I expect to see more real-time GNNs deployments for fraud in the coming years.</p><h1 id=transformer-models>Transformer Models</h1><h2 id=transformers>Transformers</h2><p><a href=https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)>Transformers</a> (originally developed for language processing) have revolutionized many domains, and they are now making inroads in fraud detection.
The key innovation of transformers is the <a href=https://www.ibm.com/think/topics/self-attention>self-attention mechanism</a>, which allows modeling long-range dependencies in sequences.
In the context of transaction data, transformers can analyze transaction sequences or sets of features in flexible ways.</p><p>Large pre-trained foundation models (akin to GPT or BERT, but for payments) are emerging.
In this case, a model is pre-trained on massive amounts of transaction data to learn general patterns, then fine-tuned for specific fraud tasks.
So that these models can “speak” transactional data.</p><blockquote><p>“One of the most notable recent developments comes from Stripe’s <a href=https://www.linkedin.com/posts/gautam-kedia-8a275730_tldr-we-built-a-transformer-based-payments-activity-7325973745292980224-vCPR/>transformer-based payments foundation model.</a>
This is a large-scale self-supervised model trained on tens of billions of transactions to create embeddings of each transaction.
The idea is analogous to how LLMs work: to learn a high-dimensional embedding for a transaction that captures its essential characteristics and context.
Transactions with similar patterns end up with similar embeddings, e.g., transactions from the same bank or the same email domain cluster together in embedding space.
These embeddings serve as a universal representation that can be used for various tasks: fraud detection, risk scoring, identifying businesses in trouble, etc.
For the fraud use-case, Stripe reports a dramatic improvement: by feeding sequences of these transaction embeddings into a downstream classifier, they achieved an increase in detection rate for certain fraud attacks from 59% to 97% overnight.
In particular, they targeted “card testing fraud” (i.e., fraudsters testing stolen card credentials with small purchases), something that often hides in high-volume data.
The transformer foundation model was able to spot subtle sequential patterns of card testing that previous feature-engineered models missed, blocking attacks in real-time before they could do damage.”</p></blockquote><p>Researchers have applied Transformer encoders to tabular data as well.<sup id=fnref:18><a href=#fn:18 class=footnote rel=footnote role=doc-noteref>22</a></sup>
For example, using models like <a href=https://github.com/lucidrains/tab-transformer-pytorch>TabTransformer</a> or integration of transformers with structured data.
They reported improved accuracy over MLPs and even over tree models in some cases.<sup id=fnref:26><a href=#fn:26 class=footnote rel=footnote role=doc-noteref>23</a></sup></p><p>The ability of transformers to focus attention on important features or interactions could be beneficial for high-dimensional transaction data.
For example, a transformer might learn to put high attention on the <code class="language-plaintext highlighter-rouge">device_id</code> feature when the <code class="language-plaintext highlighter-rouge">ip_address_country</code> is different from the <code class="language-plaintext highlighter-rouge">billing country</code>, effectively learning a rule-like interaction that would be hard for a linear model.</p><p>Transformers can also model cross-item sequences: one can feed a sequence of past transactions as a “sentence” into a transformer, where each transaction is like a token embedding (comprising attributes like amount, merchant category, etc.).
The transformer can then output a representation of the sequence or of the next transaction’s risk.
This is similar to an RNN’s use but with the advantage of attention capturing long-range dependencies (e.g., a pattern that repeats after 20 transactions).
There have been experiments where a transformer outperformed LSTM on fraud sequence classification, due to its parallel processing and ability to consider all transactions’ relations at once.</p><p>Another angle is using transformer models for entity resolution and representation in fraud. For instance, a transformer can be trained on the corpus of all descriptions or merchant names that a user has transacted with, thereby learning a “profile” of the user’s spending habits and detecting an out-of-profile transaction (similar to how language models detect an odd word in a sentence).
Additionally, <a href=https://en.wikipedia.org/wiki/BERT_(language_model)>BERT</a>-like models can be used on event logs or customer support chats to detect social engineering fraud attempts, though that’s adjacent to transaction fraud.</p><h2 id=foundation-models>Foundation models</h2><p><a href=https://en.wikipedia.org/wiki/Foundation_model>Foundation models</a> in fraud detection refer to large models trained on broad data that can then be adapted.
Besides Stripe’s payments’ model, other financial institutions are likely developing similar pre-trained embeddings.
For example, a consortium of banks could train a model on pooled transaction data (in a privacy-preserving way, or via <a href=https://en.wikipedia.org/wiki/Federated_learning>federated learning</a>) to get universal fraud features.</p><p>These large models may use transformers or other architectures, but the common theme is self-supervised learning: e.g., predicting a masked field of a transaction (<code class="language-plaintext highlighter-rouge">merchant_category</code>, or <code class="language-plaintext highlighter-rouge">amount</code>) from other fields, or predicting the next transaction given past ones.
Through such tasks, the model gains a deep understanding of normal transactional patterns.
When fine-tuned on a specific fraud dataset, it starts with a rich feature space and thus performs better with less training data than a model from scratch.
This is analogous to how image models pre-trained on <a href=https://www.image-net.org/>ImageNet</a> are fine-tuned for medical images with small datasets.</p><h2 id=strengths-3>Strengths</h2><p>Transformers and foundation models bring state-of-the-art pattern recognition to fraud.
They particularly shine in capturing complex interactions and sequential/temporal patterns.
The attention mechanism allows the model to focus on the most relevant parts of the input for each decision.
For fraud, this could mean focusing on certain past transactions or specific features that are indicative of risk in context.
This yields high detection performance, especially for “hard fraud” that evades simpler models.</p><p>Another strength is multitasking capabilities.
A large foundation model can be trained once and then used for various related tasks such as fraud, credit risk, or marketing predictions simply by fine-tuning or prompting, rather than maintaining separate models for each.
This “one model, many tasks” approach can simplify the system and leverage cross-task learning (e.g., learning what a risky transaction looks like might also help predict chargebacks or customer churn).</p><p>Moreover, transformers can handle heterogeneous data relatively easily.
One can concatenate different feature types and the self-attention will figure out which parts to emphasize.
For example, Stripe’s model encodes each transaction as a dense vector capturing numeric fields, categorical fields, etc., all in one embedding.</p><p>Finally, foundation models can enable few-shot or zero-shot fraud detection.
Imagine detecting a new fraud pattern that wasn’t in the training data.
A pre-trained model that has generally learned “how transactions usually look” might pick up the anomaly better than a model trained only on past known frauds.</p><h2 id=weaknesses-3>Weaknesses</h2><p>The obvious downsides are resource intensity and complexity.
Training a transformer on billions of transactions is a monumental effort, requiring distributed training, specialized hardware (TPUs/GPUs), and careful tuning.
This is typically only within reach of large organizations or collaborations.
In production, serving a large transformer in real-time can be challenging due to model size and latency.
Transformers can have millions of parameters, and even if each inference is 50-100ms on a GPU, at very high transaction volumes (thousands per second) this could be costly or slow without scaling out.
Techniques like <a href=https://huggingface.co/docs/optimum/en/concept_guides/quantization>model quantization</a>, <a href=https://www.ibm.com/think/topics/knowledge-distillation>knowledge distillation</a>, or efficient transformer variants (e.g., <a href=https://huggingface.co/papers/2403.20041>Transformer Lite</a>) might be needed.</p><p>Another concern is explainability.
Even more so than a standard deep network, a giant foundation model is a black box.
Understanding its decisions requires advanced explainable AI methods, like interpreting attention weights or using SHAP on the embedding features, which is an active research area.
For regulated industries, one might still use a simpler surrogate model to justify decisions externally, while the transformer works under the hood.</p><p>Overfitting and concept drift are also concerns.
A foundation model might capture a lot of patterns, including some that are spurious or not causally related to fraud.
If fraudsters adapt, the model might need periodic re-training or fine-tuning with fresh data to unlearn outdated correlations.
For example, the Stripe model is self-supervised (no fraud labels in pre-training) which helps it generalize, but any discriminative fine-tuning on fraud labels will still need updating as fraud evolves.</p><h2 id=real-time-suitability-3>Real-Time Suitability</h2><p>Surprisingly, with the right engineering, even large transformers can be used in or near real-time.
For example, optimizing the embedding generation via GPU inference or caching mechanisms.
One strategy is to pre-compute embeddings for entities (like a card or user) so that only incremental computation is needed per new transaction.
Another strategy is two-stage scoring: use a smaller model to thin out events, then apply the heavy model to the most suspicious subset.
If real-time means sub-second (say &lt;500ms), a moderately sized transformer model on modern inference servers can fit that window, especially if batch processing a few transactions together to amortize overhead.
Cloud providers also offer accelerated inference endpoints (like AWS Inferentia chips or Azure’s ONNX runtime with GPU) to deploy large models with low latency.</p><p>That said, not every company will want to deploy a 100M+ parameter model for each transaction if a simpler model would do.
There is a trade-off between maximum accuracy and infrastructure cost/complexity.
In many cases, a foundation model could be used to periodically score accounts offline (to detect emerging fraud rings) and a simpler online model handle immediate decisions, combining their outputs.</p><h2 id=examples-3>Examples</h2><p>Use cases and research for transformers in fraud:</p><ul><li><strong>Stripe’s Payments Foundation Model:</strong> A transformer-based model trained on billions of transactions, now used to embed transactions and feed into Stripe’s real-time fraud systems. It improved certain fraud detection rates from 59% to 97% and enabled detection of subtle sequential fraud patterns that were previously missed.</li><li><strong>Tabular transformers:</strong> Studies like Chang et al.<sup id=fnref:18:1><a href=#fn:18 class=footnote rel=footnote role=doc-noteref>22</a></sup> applied a transformer to the Kaggle credit card dataset and compared it to SVM, Random Forest, XGBoost, etc. The transformer achieved comparable or superior Precision/Recall, demonstrating that even on tabular data a transformer can learn effectively.</li><li><strong>Sequence anomaly detection:</strong> Some works use transformers to model time series of transactions per account. A transformer may be trained to predict the next transaction features; if the actual next transaction diverges significantly, it could flag an anomaly. This is analogous to language model use (predict next word).</li><li><strong>Cross-entity sequence modeling:</strong> Transformers can also encode sequences of transactions across entities, e.g., tracing a chain of transactions through intermediary accounts (useful in money laundering detection). The recent FraudGT model<sup id=fnref:27><a href=#fn:27 class=footnote rel=footnote role=doc-noteref>24</a></sup> combines ideas of GNNs and transformer to handle transaction graphs with sequential relations.</li><li><strong>Foundation models for documents and text in fraud:</strong> While not the focus here, note that transformers (BERT, GPT) are heavily used to detect fraud in textual data (e.g., scam emails, fraudulent insurance claims text, etc). In a holistic fraud system, a foundation model might take into account not just the structured transaction info but also any unstructured data, like customer input or messages, to make a decision.</li></ul><p>Transformer-based models and foundation models represent the frontier of fraud detection modeling.
They offer unparalleled modeling capacity and flexibility, at the cost of high complexity.
Early results, especially from industry leaders, indicate they can substantially raise the bar on fraud detection performance when deployed thoughtfully.
As these models become more accessible (with open-source frameworks and possibly smaller specialized versions), more fraud teams will likely adopt them, particularly for large-scale, multi-faceted fraud problems where simpler models hit a ceiling.</p><h1 id=appendix>Appendix</h1><h2 id=public-datasets>Public Datasets</h2><p>Research in fraud detection often relies on a few key <strong>public datasets</strong> to evaluate models, given that real financial data is usually proprietary.</p><p>Below I summarize some of the most commonly used datasets, along with their characteristics:</p><ul><li><p><a href=https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud><img class=emoji title=:globe_with_meridians: alt=:globe_with_meridians: src=https://github.githubassets.com/images/icons/emoji/unicode/1f310.png height=20 width=20> Credit Card Fraud Detection (Kaggle, 2013)</a>: A classic dataset containing real European credit card transactions over two days. Its key characteristics are its extreme class imbalance (0.172% fraud) and anonymized features (28 PCA components), making it a standard benchmark for testing algorithms on imbalanced data.</p></li><li><p><a href=https://www.kaggle.com/c/ieee-fraud-detection><img class=emoji title=:globe_with_meridians: alt=:globe_with_meridians: src=https://github.githubassets.com/images/icons/emoji/unicode/1f310.png height=20 width=20> IEEE-CIS Fraud Detection (Kaggle, 2019)</a>: A large, rich dataset from an e-commerce provider, released for a Kaggle competition. It features ~300 raw features (device info, card details, etc.), missing values, and a moderate imbalance (3.5% fraud). It is ideal for evaluating complex feature engineering and ensemble models for card-not-present fraud.</p></li><li><p><a href=https://www.kaggle.com/datasets/ealaxi/paysim1><img class=emoji title=:globe_with_meridians: alt=:globe_with_meridians: src=https://github.githubassets.com/images/icons/emoji/unicode/1f310.png height=20 width=20> PaySim (Kaggle, 2016)</a>: A large-scale synthetic dataset that simulates mobile money transactions. It contains over 6 million transactions and is useful for testing model scalability in a controlled environment. Because it is synthetic, models may achieve unrealistically high performance.</p></li><li><p><a href=https://www.kaggle.com/datasets/ellipticco/elliptic-data-set><img class=emoji title=:globe_with_meridians: alt=:globe_with_meridians: src=https://github.githubassets.com/images/icons/emoji/unicode/1f310.png height=20 width=20> Elliptic Bitcoin Dataset (Kaggle, 2019)</a>: A temporal graph of over 200,000 Bitcoin transactions, where nodes are transactions and edges represent fund flows. It is a key benchmark for evaluating graph-based fraud detection methods like GNNs. Only a small fraction of nodes are labeled as illicit, presenting a challenge.</p></li></ul><p>⚠️ Due to high imbalance, accuracy is not informative (e.g., the credit card dataset has 99.8% non-fraud, so a trivial model gets 99.8% accuracy by predicting all non-fraud!). Hence, papers report metrics like AUC-ROC, Precision/Recall, or F1-score. For instance, on the Kaggle credit card data, an AUC-ROC around 0.95+ is achievable by top models, and PR AUC is much lower (since base fraud rate is 0.172%). In IEEE-CIS data, top models achieved about 0.92–0.94 AUC-ROC in the competition. PaySim being synthetic often yields extremely high AUC (sometimes >0.99 for simple models) since patterns might be easier to learn. When evaluating on these sets, it’s crucial to use proper cross-validation or the given train/test splits to avoid overfitting (particularly an issue with the small Kaggle credit card data).</p><p>Overall, these datasets have driven a lot of research.
However, one should be cautious when extrapolating results from them to real-world performance.
Real production data can be more complex (concept drift, additional features, feedback loops).
Nonetheless, the above datasets provide valuable benchmarks to compare algorithms under controlled conditions.</p><h1 id=external-resources>External Resources</h1><ul><li><a href=https://github.com/safe-graph/graph-fraud-detection-papers/><i class="fab fa-github"></i></a> <a href=https://github.com/safe-graph/graph-fraud-detection-papers/>Awesome Graph Fraud Detection Papers</a></li><li><a href=https://github.com/safe-graph/DGFraud><i class="fab fa-github"></i></a> <a href=https://github.com/safe-graph/DGFraud>DGFraud: A Deep Graph-based Toolbox for Fraud Detection</a></li><li><a href=https://github.com/junhongmit/FraudGT><i class="fab fa-github"></i></a> <a href=https://github.com/junhongmit/FraudGT>FraudGT: A Simple, Effective, and Efficient Graph Transformer for Financial Fraud Detection</a></li></ul><h1 id=footnotes>Footnotes</h1><div class=footnotes role=doc-endnotes><ol><li id=fn:1><p>Oztas, Berkan, et al. “<em><a href=https://www.sciencedirect.com/science/article/pii/S0167739X24002607>Transaction monitoring in anti-money laundering: A qualitative analysis and points of view from industry.</a></em>” Future Generation Computer Systems (2024). <a href=#fnref:1 class=reversefootnote role=doc-backlink>↩</a></p></li><li id=fn:2><p>G. Praspaliauskas, V. Raman (2023). <em>“<a href=https://aws.amazon.com/blogs/machine-learning/real-time-fraud-detection-using-aws-serverless-and-machine-learning-services/>Real-time fraud detection using AWS serverless and machine learning services</a>.</em> AWS Machine Learning Blog – outlines a serverless architecture using Amazon Kinesis, Lambda, and Amazon Fraud Detector for near-real-time fraud prevention.” <a href=#fnref:2 class=reversefootnote role=doc-backlink>↩</a></p></li><li id=fn:34><p>Desai, Ajit, Anneke Kosse, and Jacob Sharples. “<em><a href=https://www.sciencedirect.com/science/article/pii/S2405918825000157>Finding a needle in a haystack: a machine learning framework for anomaly detection in payment systems.</a>.</em>” The Journal of Finance and Data Science 11 (2025): 100163. <a href=#fnref:34 class=reversefootnote role=doc-backlink>↩</a></p></li><li id=fn:28><p>R-ring collusion is a form of coordinated behavior where multiple accounts, potentially belonging to different individuals or groups, engage in fraudulent activities that benefit each other. <a href=#fnref:28 class=reversefootnote role=doc-backlink>↩</a></p></li><li id=fn:16><p>For a Python library dedicated to handling imbalanced datasets and techniques, see <em><a href=https://imbalanced-learn.org/stable/>imbalanced-learn</a></em>, which provides tools for oversampling, undersampling, and more. <a href=#fnref:16 class=reversefootnote role=doc-backlink>↩</a></p></li><li id=fn:29><p>Service Level Agreement (SLA) is a commitment between a service provider and a client that outlines the expected level of service, including performance metrics and response times. <a href=#fnref:29 class=reversefootnote role=doc-backlink>↩</a></p></li><li id=fn:17><p>Afriyie, Jonathan Kwaku, et al. <em>“<a href=https://doi.org/10.1016/j.dajour.2023.100163>A supervised machine learning algorithm for detecting and predicting fraud in credit card transactions.</a></em> Decision Analytics Journal 6 (2023): 100163.” <a href=#fnref:17 class=reversefootnote role=doc-backlink>↩</a></p></li><li id=fn:19><p>Onyeoma, Chidinma Faith, et al. “<em><a href=https://ieeexplore.ieee.org/abstract/document/10838456>Credit Card Fraud Detection Using Deep Neural Network with Shapley Additive Explanations</a>.</em>” 2024 International Conference on Frontiers of Information Technology (FIT). IEEE, 2024. <a href=#fnref:19 class=reversefootnote role=doc-backlink>↩</a></p></li><li id=fn:13><p>Kandi, Kianeh, and Antonio García-Dopico. “<em><a href=https://www.mdpi.com/2504-4990/7/1/20>Enhancing Performance of Credit Card Model by Utilizing LSTM Networks and XGBoost Algorithms.</a></em>” Machine Learning and Knowledge Extraction 7.1 (2025): 20. <a href=#fnref:13 class=reversefootnote role=doc-backlink>↩</a> <a href=#fnref:13:1 class=reversefootnote role=doc-backlink>↩<sup>2</sup></a></p></li><li id=fn:33><p>Cherif, Asma, et al. “<em><a href=https://www.sciencedirect.com/science/article/pii/S1319157824000922>Encoder–decoder graph neural network for credit card fraud detection.</a></em>” Journal of King Saud University-Computer and Information Sciences 36.3 (2024). <a href=#fnref:33 class=reversefootnote role=doc-backlink>↩</a></p></li><li id=fn:21><p>Alshameri, Faleh, and Ran Xia. “<em><a href=https://www.sciopen.com/article/10.26599/BDMA.2023.9020035>An Evaluation of Variational Autoencoder in Credit Card Anomaly Detection</a>.</em>” Big Data Mining and Analytics (2024). <a href=#fnref:21 class=reversefootnote role=doc-backlink>↩</a></p></li><li id=fn:22><p>Charitou, Charitos, Artur d’Avila Garcez, and Simo Dragicevic. “<em><a href=https://ieeexplore.ieee.org/document/9206844>Semi-supervised GANs for fraud detection</a>.</em>” 2020 International Joint Conference on Neural Networks (IJCNN). IEEE, 2020. <a href=#fnref:22 class=reversefootnote role=doc-backlink>↩</a></p></li><li id=fn:32><p>Huang, Huajie, et al. “<em><a href=https://www.sciencedirect.com/science/article/abs/pii/S156849462400142X>Imbalanced credit card fraud detection data: A solution based on hybrid neural network and clustering-based undersampling technique.</a></em>” Applied Soft Computing 154 (2024). <a href=#fnref:32 class=reversefootnote role=doc-backlink>↩</a></p></li><li id=fn:24><p>Branco, Bernardo, et al. “<em><a href=https://doi.org/10.1145/3394486.3403361>Interleaved sequence RNNs for fraud detection</a>.</em>” Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining. 2020. <a href=#fnref:24 class=reversefootnote role=doc-backlink>↩</a></p></li><li id=fn:25><p>Masihullah, Shaik, et al. “<em><a href=https://link.springer.com/chapter/10.1007/978-3-031-14463-9_10>Identifying fraud rings using domain aware weighted community detection</a>.</em>” International Cross-Domain Conference for Machine Learning and Knowledge Extraction. Cham: Springer International Publishing, 2022. <a href=#fnref:25 class=reversefootnote role=doc-backlink>↩</a></p></li><li id=fn:35><p>Boyapati, Mallika, and Ramazan Aygun. “<em><a href=https://www.sciencedirect.com/science/article/abs/pii/S0893608024008554>BalancerGNN: Balancer Graph Neural Networks for imbalanced datasets: A case study on fraud detection.</a>.</em>” Neural Networks 182 (2025): 106926. <a href=#fnref:35 class=reversefootnote role=doc-backlink>↩</a></p></li><li id=fn:23><p>Motie, Soroor, and Bijan Raahemi. “<em><a href=https://doi.org/10.1016/j.eswa.2023.122156>Financial fraud detection using graph neural networks: A systematic review</a>.</em>” Expert Systems with Applications (2024). <a href=#fnref:23 class=reversefootnote role=doc-backlink>↩</a></p></li><li id=fn:30><p>Shih, Yi-Cheng, et al. “<em><a href=https://www.sciencedirect.com/science/article/abs/pii/S0957417424020785>Fund transfer fraud detection: Analyzing irregular transactions and customer relationships with self-attention and graph neural networks.</a></em>” Expert Systems with Applications. 2025. <a href=#fnref:30 class=reversefootnote role=doc-backlink>↩</a></p></li><li id=fn:31><p>Tong, Guoxiang, and Jieyu Shen. “<em><a href=https://www.sciencedirect.com/science/article/abs/pii/S1568494623010025>Financial transaction fraud detector based on imbalance learning and graph neural network.</a></em>” Applied Soft Computing 149 (2023): 110984. <a href=#fnref:31 class=reversefootnote role=doc-backlink>↩</a></p></li><li id=fn:3><p>Jian Zhang et al. (2022). <em>“<a href=https://aws.amazon.com/blogs/machine-learning/build-a-gnn-based-real-time-fraud-detection-solution-using-amazon-sagemaker-amazon-neptune-and-the-deep-graph-library/>Build a GNN-based real-time fraud detection solution using Amazon SageMaker, Amazon Neptune, and DGL</a>.</em> AWS ML Blog – explains how graph neural networks can be served in real-time for fraud detection, noting challenges in moving from batch to real-time GNN inference.” <a href=#fnref:3 class=reversefootnote role=doc-backlink>↩</a> <a href=#fnref:3:1 class=reversefootnote role=doc-backlink>↩<sup>2</sup></a> <a href=#fnref:3:2 class=reversefootnote role=doc-backlink>↩<sup>3</sup></a></p></li><li id=fn:4><p>Summer Liu et al. (2024). <em>“<a href=https://developer.nvidia.com/blog/supercharging-fraud-detection-in-financial-services-with-graph-neural-networks/>Supercharging Fraud Detection in Financial Services with GNNs</a>.</em> NVIDIA Technical Blog.” <a href=#fnref:4 class=reversefootnote role=doc-backlink>↩</a> <a href=#fnref:4:1 class=reversefootnote role=doc-backlink>↩<sup>2</sup></a></p></li><li id=fn:18><p>Yu, Chang, et al. “<em><a href=https://arxiv.org/pdf/2406.03733v2>Credit Card Fraud Detection Using Advanced Transformer Model</a>.</em>” 2024 IEEE International Conference on Metaverse Computing, Networking, and Applications (MetaCom). IEEE, 2024. <a href=#fnref:18 class=reversefootnote role=doc-backlink>↩</a> <a href=#fnref:18:1 class=reversefootnote role=doc-backlink>↩<sup>2</sup></a></p></li><li id=fn:26><p>Krutikov, Sergei, et al. “<em><a href=https://arxiv.org/html/2405.13692v1>Challenging Gradient Boosted Decision Trees with Tabular Transformers for Fraud Detection at Booking.com</a>.</em>” arXiv preprint arXiv:2405.13692 (2024). <a href=#fnref:26 class=reversefootnote role=doc-backlink>↩</a></p></li><li id=fn:27><p>Lin, Junhong, et al. “<em><a href=https://dl.acm.org/doi/abs/10.1145/3677052.3698648>FraudGT: A Simple, Effective, and Efficient Graph Transformer for Financial Fraud Detection</a>.</em>” Proceedings of the 5th ACM International Conference on AI in Finance. 2024. <a href=#fnref:27 class=reversefootnote role=doc-backlink>↩</a></p></li></ol></div></article><ul class="pager blog-pager"><li class=previous><a href=/blog/building-genai-applications-today.html data-toggle=tooltip data-placement=top title="Building GenAI Applications Today">← Previous Post</a></li><li class=next><a href=/blog/evaluation-metrics-for-real-time-financial-fraud-detection-ml-models.html data-toggle=tooltip data-placement=top title="Evaluation Metrics for Real-Time Financial Fraud Detection ML Models">Next Post →</a></li></ul><br></div></div></div><script>anchors.options={position:'left'},anchors.add('h1:not(.no-anchor)','h2','h3')</script><div class=container><div class=row><div class="col-lg-10 col-lg-offset-2 col-md-10 col-md-offset-1" style=padding-right:15px><div class=flex-container-footer-badges><section class=share-section><a href="https://twitter.com/intent/tweet?text=Financial+transaction+fraud+is+a+pervasive+problem+costing+institutions+and+customers+billions+annually.+This+survey+reviews+the+current+state-of-the-art+in+real-time+transaction+fraud+detection%2C+spanning+both+academic+research+and+industry+adopted+solutions.%0A+https://www.cesarsotovalero.net/blog/from-classical-ml-to-dnns-and-gnns-for-real-time-financial-fraud-detection.html" class="btn btn-my-icon btn-twitter" title="Share on Twitter"><span id=share-twitter class="fab fa-twitter" aria-hidden=true></span>
<span class=sr-only>Twitter</span></a>
<a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.cesarsotovalero.net/blog/from-classical-ml-to-dnns-and-gnns-for-real-time-financial-fraud-detection.html" class="btn btn-my-icon btn-linkedin" title="Share on LinkedIn"><span id=share-linkedin class="fab fa-linkedin-in" aria-hidden=true></span>
<span class=sr-only>LinkedIn</span></a>
<a href="https://www.facebook.com/sharer/sharer.php?u=https://www.cesarsotovalero.net/blog/from-classical-ml-to-dnns-and-gnns-for-real-time-financial-fraud-detection.html" class="btn btn-my-icon btn-facebook" title="Share on Facebook"><span class="fab fa-facebook-f" aria-hidden=true></span>
<span class=sr-only>Facebook</span></a>
<a href="https://www.reddit.com/submit?url=https%3A%2F%2Fwww.cesarsotovalero.net%2Fblog%2Ffrom-classical-ml-to-dnns-and-gnns-for-real-time-financial-fraud-detection.html" class="btn btn-my-icon btn-reddit" title="Share on Reddit"><span class="fab fa-fw fa-reddit" aria-hidden=true></span>
<span class=sr-only>Reddit</span></a>
<a href="https://t.me/share/url?url=https%3A%2F%2Fwww.cesarsotovalero.net%2Fblog%2Ffrom-classical-ml-to-dnns-and-gnns-for-real-time-financial-fraud-detection.html&text=Financial+transaction+fraud+is+a+pervasive+problem+costing+institutions+and+customers+billions+annually.+This+survey+reviews+the+current+state-of-the-art+in+real-time+transaction+fraud+detection%2C+spanning+both+academic+research+and+industry+adopted+solutions.%0A" class="btn btn-my-icon btn-telegram" title="Share on Telegram"><i class="fab fa-telegram"></i>
<span class=sr-only>Telegram</span></a>
<a href="http://news.ycombinator.com/submitlink?u=https%3A%2F%2Fwww.cesarsotovalero.net%2Fblog%2Ffrom-classical-ml-to-dnns-and-gnns-for-real-time-financial-fraud-detection.html&t=From+Classical+ML+to+DNNs+and+GNNs+for+Real-Time+Financial+Fraud+Detection" class="btn btn-my-icon btn-hn" title="Share on Hacker News"><i class="fab fa-hacker-news-square"></i>
<span class=sr-only>Hacker News</span></a></section></div><div id=related-posts><hr><h2>Related Posts</h2><div class="panel-deck row"><div class=col-sm-4><div class=panel><a href=/blog/why-genai-will-not-replace-software-engineers-just-yet.html><div class=panel-body><div class=text-left><h4 class=title data-toc-skip>Why GenAI Will NOT Replace Software Engineers</h4><div class=date style=margin-bottom:0>Posted on August 19, 2024</div></div><img src=../img/posts/2024/2024-08-19/armillary-sphere_cover.jpg alt="Why GenAI Will NOT Replace Software Engineers"></div></a></div></div><div class=col-sm-4><div class=panel><a href=/blog/building-genai-applications-today.html><div class=panel-body><div class=text-left><h4 class=title data-toc-skip>Building GenAI Applications Today</h4><div class=date style=margin-bottom:0>Posted on November 17, 2024</div></div><img src=../img/posts/2024/2024-11-17/twisted-revolver_cover.jpg alt="Building GenAI Applications Today"></div></a></div></div><div class=col-sm-4><div class=panel><a href=/blog/surviving-the-ai-revolution-as-a-software-engineer.html><div class=panel-body><div class=text-left><h4 class=title data-toc-skip>Surviving the AI Revolution as a Software Engineer</h4><div class=date style=margin-bottom:0>Posted on September 17, 2023</div></div><img src=../img/posts/2023/2023-09-17/head-down-teddy_cover.jpg alt="Surviving the AI Revolution as a Software Engineer"></div></a></div></div></div></div><div><div id=giscus-container></div><script src=/js/load-giscus.js></script>
<script>window.addEventListener('load',loadGiscus)</script></div></div></div></div><script>window.addEventListener('DOMContentLoaded',()=>{const a=new IntersectionObserver(a=>{a.forEach(a=>{const b=a.target.getAttribute('id');a.intersectionRatio>0?document.querySelector(`#toc li a[href="#${b}"]`).parentElement.classList.add('active'):document.querySelector(`#toc li a[href="#${b}"]`).parentElement.classList.remove('active')})});document.querySelectorAll('h1[id]').forEach(b=>{a.observe(b)}),document.querySelectorAll('h2[id]').forEach(b=>{a.observe(b)})})</script><footer><div class="container beautiful-jekyll-footer"><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div style=text-align:center><div class=flex-container-footer-badges><a href=https://www.linkedin.com/in/cesarsotovalero><img alt="Connect on LinkedIn" src="https://img.shields.io/badge/LinkedIn-Connect-blue?style=social&logo=linkedin"></a>
<a href=https://www.youtube.com/channel/UCR4rI98w6-MqYoCS6jR9LGg><img alt="Subscribe to my YouTube channel" src="https://img.shields.io/youtube/channel/subscribers/UCR4rI98w6-MqYoCS6jR9LGg?logoColor=black&label=Subscribe"></a>
<a href=https://github.com/cesarsotovalero><img alt="Follow me on GitHub" src="https://img.shields.io/github/followers/cesarsotovalero?label=Follow%20me"></a></div></div><div class=flex-container-footer-copyright><p class="copyright text-muted">© César Soto Valero  •  2018–2025</p></div></div></div></div></footer><script>typeof jQuery=='undefined'&&document.write('<script src="/js/jquery-1.11.2.min.js"><\/script>')</script><script src=/js/bootstrap.min.js></script>
<script src=/js/main.js></script>
<script src=/js/flowtype.js></script>
<script src=/js/scroll-to-top.js></script>
<script src=/js/lazysizes.min.js></script>
<script src=/js/popup-footnotes.js></script>
<script src=/js/mermaid.min.js></script>
<script>$(document).ready(function(){mermaid.initialize({startOnLoad:!0,theme:"default"}),window.mermaid.init(void 0,document.querySelectorAll('.language-mermaid'))})</script><script>(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)})(window,document,'script','//www.google-analytics.com/analytics.js','ga'),ga('create','UA-107061705-1','auto'),ga('send','pageview')</script><script src=/js/mode-switcher.js></script></body></html>