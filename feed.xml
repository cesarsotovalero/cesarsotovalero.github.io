<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://www.cesarsotovalero.net/feed.xml" rel="self" type="application/atom+xml" /><link href="https://www.cesarsotovalero.net/" rel="alternate" type="text/html" /><updated>2025-07-29T11:54:56-07:00</updated><id>https://www.cesarsotovalero.net/feed.xml</id><title type="html">César Soto Valero</title><subtitle>César - Computer Scientist</subtitle><author><name>César Soto Valero</name><email>NaN</email></author><entry><title type="html">I’m Switching to Python and Actually Liking It</title><link href="https://www.cesarsotovalero.net/blog/i-am-switching-to-python-and-actually-liking-it.html" rel="alternate" type="text/html" title="I’m Switching to Python and Actually Liking It" /><published>2025-07-15T00:00:00-07:00</published><updated>2025-07-29T11:52:25-07:00</updated><id>https://www.cesarsotovalero.net/blog/i-am-switching-to-python-and-actually-liking-it</id><content type="html" xml:base="https://www.cesarsotovalero.net/blog/i-am-switching-to-python-and-actually-liking-it.html"><![CDATA[<p>I started to code more in <a href="https://www.python.org/">Python</a> around 6 months ago.
Why?
Because of AI, obviously.
It’s clear (to me) that big <del>money</del> opportunities are all over AI these days.
And guess what’s the <em>de facto</em> programming language for AI?
Yep, that sneaky one.</p>

<p>I had used Python before, but only for small scripts.
For example, <a href="https://github.com/cesarsotovalero/cesarsotovalero.github.io/blob/1fb2efe0577719a72fdf7d5bdf2a8d4d51ee58c5/scripts/fetch_all_youtube_videos.py">this script</a> scrapes metadata from all videos on <a href="https://www.youtube.com/channel/UCR4rI98w6-MqYoCS6jR9LGg">my YouTube channel</a>.
The metadata is dumped as <a href="https://github.com/cesarsotovalero/cesarsotovalero.github.io/blob/1fb2efe0577719a72fdf7d5bdf2a8d4d51ee58c5/_data/youtube-videos.json">a JSON file</a> that I use to nicely display statistics of the videos <a href="https://www.cesarsotovalero.net/youtube">on this static page</a>.
As you can <a href="https://github.com/cesarsotovalero/cesarsotovalero.github.io/blob/1fb2efe0577719a72fdf7d5bdf2a8d4d51ee58c5/.github/workflows/update-youtube-videos.yml">see here</a>, this little script runs in solo mode every Monday via GitHub Actions.
Doing this kind of thing in Python is just way more convenient than, say, using Batch.
Not only because the syntax is more human-friendly, but also because the Python interpreter is natively integrated in all Unix distros.
Isn’t that cool?</p>

<p>So yeah, Python is powerful, and it couples very well with the now ubiquitous <a href="https://code.visualstudio.com/">VSCode</a> editor.
But I didn’t treat it seriously until recently,<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> it was just after I wanted to build AI applications (RAG, Agents, GenAI tools, etc.) for the “real world” that I realized that whether you like it or not, Python is the language of choice for those matters.</p>

<p>So I decided to give it a serious try, and to my great surprise, I’ve found that Python, and everything around it, has really improved a lot over the last decades.</p>

<p>Here’re just three examples:</p>

<ol>
  <li>Python has created a very complete ecosystem of libraries and tools for processing and analyzing data.<sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">2</a></sup></li>
  <li>Python has gotten faster with optimized static compilers like <a href="https://cython.org/">Cython</a>.</li>
  <li>Python has done a good job of hiding its legacy ugliness (such as <code class="language-plaintext highlighter-rouge">__init__</code>, <code class="language-plaintext highlighter-rouge">__new__</code>, and similar aberrations), sweetening its syntax to accommodate developers <del>with good taste</del>.</li>
</ol>

<p>Thanks to this and many other things, I’m now feeling a particular joy for the language.</p>

<p>However, during this time, I’ve found that there’s still a big gap between using Python for “production-ready”<sup id="fnref:7"><a href="#fn:7" class="footnote" rel="footnote" role="doc-noteref">3</a></sup> apps vs. the usual Jupyter notebook or script-based workflow.</p>

<p>So in this post, I share the tools, libraries, configs, and other integrations that bring me joy, and that I now use for building my Python applications.</p>

<p>⚠️ This post is highly biased toward the tools I personally use today, and if you think I’m missing some gem, please let me/us know (preferably in the comment section below).</p>

<p><strong>NOTE:</strong> Somehow this article got <a href="https://news.ycombinator.com/item?id=44579717">680+ comments</a> on Hacker News (just another proof that you never really know).</p>

<h1 id="project-structure">Project Structure</h1>

<p>I prefer to use a <a href="https://en.wikipedia.org/wiki/Monorepo">monorepo</a> structure (backend and frontend) for my Python projects.<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">4</a></sup></p>

<p>Why?</p>

<ol>
  <li>Because of my bad memory: I don’t like code parts scattered across multiple repositories (it’s definitely not search-friendly).</li>
  <li>Because having multiple repos is mostly unnecessary: I’m just one guy, and I believe that if a project grows to the point that it needs to be split into multiple repositories, then it’s a sign of over-engineering.</li>
  <li>Because I’m lazy: I like to keep things as simple as possible, compile, test, containerize, and deploy from a single location.<sup id="fnref:5"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">5</a></sup></li>
</ol>

<p>I would like to have a tool that generates the project structure for me, but I haven’t found one that fits me yet.
In the past, I used <a href="https://cookiecutter-data-science.drivendata.org/">CCDS</a>, a project initialization tool mostly for Data Science projects.
It’s very good, but it’s not targeting full-stack developers as its core users.</p>

<p>Here’s the typical structure of a project with a frontend-backend architecture (I’ll go through each subpart later in this post):</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">project/
│
├── .github/ <span class="c"># GitHub Actions workflows for CI/CD pipelines</span>
│   ├── workflows/ <span class="c"># Directory containing YAML files for automated workflows</span>
│   └── dependabot.yml <span class="c"># Configuration for Dependabot to manage dependencies</span>
│
├── .vscode/ <span class="c"># VSCode configuration for the project</span>
│   ├── launch.json <span class="c"># Debugging configurations for VSCode</span>
│   └── settings.json <span class="c"># Project-specific settings for VSCode</span>
│
├── docs/ <span class="c"># Website and docs (a static SPA with MkDocs)</span>
│
├── project-api/ <span class="c"># Backend API for handling business logic and heavy processing</span>
│   ├── data/ <span class="c"># Directory for storing datasets or other static files</span>
│   ├── notebooks/ <span class="c"># Jupyter notebooks for quick (and dirty) experimentation and prototyping</span>
│   ├── tools/ <span class="c"># Utility scripts and tools for development or deployment</span>
│   ├── src/ <span class="c"># Source code for the backend application</span>
│   │   ├── app/ <span class="c"># Main application code</span>
│   │   └── tests/ <span class="c"># Unit tests for the backend</span>
│   │
│   ├── .dockerignore <span class="c"># Specifies files to exclude from Docker builds</span>
│   ├── .python-version <span class="c"># Python version specification for pyenv</span>
│   ├── Dockerfile <span class="c"># Docker configuration for containerizing the backend</span>
│   ├── Makefile <span class="c"># Automation tasks for building, testing, and deploying</span>
│   ├── pyproject.toml <span class="c"># Python project configuration file</span>
│   ├── README.md <span class="c"># Documentation for the backend API</span>
│   └── uv.lock <span class="c"># Lock file for dependencies managed by UV</span>
│
├── project-ui/ <span class="c"># Frontend UI for the project (Next.js, React, etc.)</span>
│
├── .gitignore <span class="c"># Global Git ignore file for the repository</span>
├── .pre-commit-config.yaml <span class="c"># Configuration for pre-commit hooks</span>
├── CONTRIBUTING.md <span class="c"># Guidelines for contributing to the project</span>
├── docker-compose.yml <span class="c"># Docker Compose configuration for multi-container setups</span>
├── LICENSE <span class="c"># License information for the project (I always choose MIT)</span>
├── Makefile <span class="c"># Automation tasks for building, testing, and deploying</span>
└── README.md <span class="c"># Main documentation for the project (main features, installation, and usage)</span></code></pre></figure>

<p>My <code class="language-plaintext highlighter-rouge">project</code> is the root directory and the name of my GitHub repo.
I like short names for projects, ideally less than 10 characters long. No <code class="language-plaintext highlighter-rouge">snake_case</code>; separation with hyphens is OK to me.
Note that the project should be self-contained, meaning it includes documentation, build/deployment infrastructure, and any other necessary files to run it standalone.</p>

<p>It’s important not to do any heavy data processing steps in the <code class="language-plaintext highlighter-rouge">project-ui</code>, as I opted to separate frontend logic from backend responsibilities.
Instead, I choose to make HTTP requests to the <code class="language-plaintext highlighter-rouge">project-api</code> server that contains the Python code.
This way, we keep the browser application light while delegating the heavy lifting and business logic to the server.</p>

<p>There’s an <code class="language-plaintext highlighter-rouge">__init__.py</code> file in <code class="language-plaintext highlighter-rouge">project-api/src/app</code> to indicate that <code class="language-plaintext highlighter-rouge">app</code> is a Python module (it can be imported from other modules).</p>

<h1 id="python-toolbox">Python Toolbox</h1>

<h2 id="uv">uv</h2>

<p>I use <a href="https://github.com/astral-sh/uv">uv</a> as my Python package manager and build tool. 
It’s all I need to install and manage dependencies.</p>

<p>Here are the core commands to set it up:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre></td><td class="code"><pre><span class="c"># Install uv globally if not already installed</span>
curl <span class="nt">-sSfL</span> https://astral.sh/install.sh | sh

<span class="c"># Initialize a new project (adds .gitignore, .python-version, pyproject.toml, etc.)</span>
uv init project-api

<span class="c"># Add some dependencies into the project and update pyproject.toml</span>
uv add <span class="nt">--dev</span> pytest ruff pre-commit mkdocs gitleaks fastapi pydantic

<span class="c"># Update the lock file with the latest versions of the dependencies</span>
<span class="c"># (this will also create a .venv in your project directory if it doesn't exist)</span>
uv <span class="nb">sync</span>

<span class="c"># (Opt‑in) explicitly create or recreate a venv in a custom path:</span>
uv venv create .venv

<span class="c"># Activate the .venv yourself</span>
<span class="c"># ──────────────────────────────────────────────</span>
<span class="c"># On macOS/Linux:</span>
<span class="nb">source</span> .venv/bin/activate
<span class="c"># On Windows (PowerShell):</span>
.<span class="se">\.</span>venv<span class="se">\S</span>cripts<span class="se">\A</span>ctivate.ps1
<span class="c"># On Windows (cmd.exe):</span>
.<span class="se">\.</span>venv<span class="se">\S</span>cripts<span class="se">\a</span>ctivate.bat
</pre></td></tr></tbody></table></code></pre></figure>

<p>Note that the most important file for <code class="language-plaintext highlighter-rouge">uv</code> is <code class="language-plaintext highlighter-rouge">pyproject.toml</code>.<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">6</a></sup><br />
This file <a href="https://packaging.python.org/en/latest/guides/writing-pyproject-toml/">contains</a> metadata and the list of dependencies required to build and run the project.</p>

<h2 id="ruff">ruff</h2>

<p>I really like <a href="https://github.com/astral-sh/ruff">ruff</a>.
It’s a super-fast Python linter and code formatter, designed to help lazy developers like me keep our codebases clean and maintainable.
Ruff combines <code class="language-plaintext highlighter-rouge">isort</code>, <code class="language-plaintext highlighter-rouge">flake8</code>, <code class="language-plaintext highlighter-rouge">autoflake</code>, and similar tools into a single command-line interface:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="code"><pre><span class="c"># Lint all files in `/path/to/code` (and any subdirectories)</span>

ruff check path/to/code/

<span class="c"># Format all files in `/path/to/code` (and any subdirectories)</span>

ruff format path/to/code/
</pre></td></tr></tbody></table></code></pre></figure>

<p>Ruff supports the <a href="https://pep8.org/">PEP 8</a> style guide out of the box.</p>

<h2 id="ty">ty</h2>

<p><a href="https://github.com/astral-sh/ty">ty</a> is a type checker for Python.
It is a great combo for <a href="https://docs.python.org/3/library/typing.html">typing</a>, the popular Python module for adding static typing.
I think typing really helps me catch type errors early in the development process. I actually don’t care about having to write more code, in fact, I prefer it if it improves code quality and reduces the likelihood of runtime errors.</p>

<p><strong>NOTE:</strong> At the time of writing, <code class="language-plaintext highlighter-rouge">ty</code> is still in early development by Astral (the same company behind <code class="language-plaintext highlighter-rouge">uv</code> and <code class="language-plaintext highlighter-rouge">ruff</code>), but I’ve been using it and haven’t found any noticeable flaws so far.</p>

<h2 id="pytest">pytest</h2>

<p><a href="https://docs.pytest.org/en/stable/">pytest</a> is <em>THE</em> testing library for Python.
Writing simple and scalable test cases with it is just super easy.
It supports fixtures, parameterized tests, and has a rich ecosystem of plugins.
Just create a file named <code class="language-plaintext highlighter-rouge">test_&lt;unit_or_module&gt;.py</code> in <code class="language-plaintext highlighter-rouge">project-api/src/app/tests/</code>, and run:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">uv run pytest</code></pre></figure>

<p>That’s it!</p>

<h2 id="pydantic">Pydantic</h2>

<p><a href="https://pydantic-docs.helpmanual.io/">Pydantic</a> is a data validation and settings management library for Python.
It helps manage all kinds of configuration settings, such as API keys, database connection details, or model parameters (hardcoding these values is a very bad practice, btw).</p>

<p>In particular, <a href="https://docs.pydantic.dev/latest/concepts/pydantic_settings/">Pydantic Settings</a> allows you to define application configurations using Pydantic models.
It can automatically load settings from environment variables or special <code class="language-plaintext highlighter-rouge">.env</code> files, validate their types, and make them easily accessible in your code.</p>

<p>Here’s an illustrative example:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="code"><pre><span class="kn">from</span> <span class="n">pydantic</span> <span class="kn">import</span> <span class="n">BaseSettings</span>

<span class="k">class</span> <span class="nc">Settings</span><span class="p">(</span><span class="n">BaseSettings</span><span class="p">):</span>
    <span class="n">api_key</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">db_url</span><span class="p">:</span> <span class="nb">str</span>

    <span class="k">class</span> <span class="nc">Config</span><span class="p">:</span>
        <span class="n">env_file</span> <span class="o">=</span> <span class="sh">"</span><span class="s">.env</span><span class="sh">"</span>

<span class="n">settings</span> <span class="o">=</span> <span class="nc">Settings</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>Now, when you run this code, Pydantic will automatically load the values of <code class="language-plaintext highlighter-rouge">api_key</code> and <code class="language-plaintext highlighter-rouge">db_url</code> from the <code class="language-plaintext highlighter-rouge">.env</code> file or environment variables.
These values will be accessible and validated according to the types defined in the <code class="language-plaintext highlighter-rouge">Settings</code> model.
Just great!</p>

<h2 id="mkdocs">MkDocs</h2>

<p>I use <a href="https://www.mkdocs.org/">MkDocs</a> for documentation and static generation of the website for the project.<sup id="fnref:6"><a href="#fn:6" class="footnote" rel="footnote" role="doc-noteref">7</a></sup>
I’m not a designer, so I prefer to just copy an aesthetically pleasing design from another similar open-source project and make some simple modifications to the CSS (like changing fonts and colors).</p>

<h2 id="fastapi">FastAPI</h2>

<p>I use <a href="https://fastapi.tiangolo.com/">FastAPI</a> for building APIs.
It has been a game changer for me; it allows for easy creation of RESTful APIs with automatic validation, serialization, and documentation.
FastAPI is built on top of Starlette and Pydantic, which means it provides excellent performance and type safety.
It’s fast, easy to use, and integrates seamlessly with Pydantic for data validation.</p>

<h2 id="dataclasses">Dataclasses</h2>

<p><a href="https://docs.python.org/3/library/dataclasses.html">Dataclasses</a> is not a library but a Python feature that provides a way to define classes that are primarily used to store data.
They offer a simple syntax for creating classes that automatically generate special methods like <code class="language-plaintext highlighter-rouge">__init__()</code>, <code class="language-plaintext highlighter-rouge">__repr__()</code>, and <code class="language-plaintext highlighter-rouge">__eq__()</code>.</p>

<p>This greatly reduces boilerplate when creating data containers.</p>

<p>Here’s an example:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="code"><pre><span class="kn">from</span> <span class="n">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>

<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">Point</span><span class="p">:</span>
    <span class="n">x</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">y</span><span class="p">:</span> <span class="nb">int</span>

<span class="n">p</span> <span class="o">=</span> <span class="nc">Point</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>  <span class="c1"># Output: Point(x=1, y=2)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>So goodbye boilerplate and cryptic code!</p>

<h1 id="version-control">Version Control</h1>

<h2 id="github-actions">GitHub Actions</h2>

<p>I’m a fanboy of <a href="https://github.com/features/actions">GitHub Actions</a>, especially for CI across different OSs.
I recommend using it for both API and UI pipelines.</p>

<p>A typical workflow for <code class="language-plaintext highlighter-rouge">project-api</code> looks like this:</p>

<figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre></td><td class="code"><pre><span class="na">name</span><span class="pi">:</span> <span class="s">CI-API</span>

<span class="na">on</span><span class="pi">:</span>
  <span class="na">push</span><span class="pi">:</span>
    <span class="na">branches</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">main</span>
  <span class="na">pull_request</span><span class="pi">:</span>
    <span class="na">branches</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">main</span>

<span class="na">jobs</span><span class="pi">:</span>
  <span class="na">build-and-test</span><span class="pi">:</span>
    <span class="na">runs-on</span><span class="pi">:</span> <span class="s">ubuntu-latest</span>
    <span class="na">steps</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Checkout code</span>
        <span class="na">uses</span><span class="pi">:</span> <span class="s">actions/checkout@v3</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Build Docker image</span>
        <span class="na">run</span><span class="pi">:</span> <span class="s">docker build -t project-api:ci ./project-api</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Run tests</span>
        <span class="na">run</span><span class="pi">:</span> <span class="s">docker run --rm project-api:ci pytest</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>Note that this workflow uses Docker to run the tests in an isolated environment.<sup id="fnref:8"><a href="#fn:8" class="footnote" rel="footnote" role="doc-noteref">8</a></sup>
You can change the OS by setting the <code class="language-plaintext highlighter-rouge">runs-on</code> parameter to <code class="language-plaintext highlighter-rouge">windows-latest</code> or <code class="language-plaintext highlighter-rouge">macos-latest</code>.</p>

<h2 id="dependabot">Dependabot</h2>

<p>Handling dependencies is a pain, but <a href="https://dependabot.com/">Dependabot</a> makes it easier.
It automatically checks for outdated dependencies and creates pull requests to update them.</p>

<p>Here’s a sample configuration for Dependabot in the <code class="language-plaintext highlighter-rouge">.github/dependabot.yml</code> file:</p>

<figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="code"><pre><span class="na">version</span><span class="pi">:</span> <span class="m">2</span>
<span class="na">updates</span><span class="pi">:</span>

<span class="pi">-</span> <span class="na">package-ecosystem</span><span class="pi">:</span> <span class="s2">"</span><span class="s">uv"</span>
    <span class="na">directory</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/"</span>
    <span class="na">schedule</span><span class="pi">:</span>
      <span class="na">interval</span><span class="pi">:</span> <span class="s2">"</span><span class="s">weekly"</span>
</pre></td></tr></tbody></table></code></pre></figure>

<h2 id="gitleaks">Gitleaks</h2>

<p>If there’s something that could hurt our reputation, it’s committing sensitive information, like API keys or passwords, directly to a repository.
Fortunately, <a href="https://github.com/gitleaks/gitleaks">Gitleaks</a> helps prevent this from happening.
There’s just no reason not to use it.</p>

<h2 id="pre-commit-hooks">Pre-commit Hooks</h2>

<p>I use <a href="https://pre-commit.com/">pre-commit</a> to run checks and format code before committing.
It helps ensure that the code is always in a good state and follows the project’s coding standards.
For example, I use it to run <a href="https://github.com/astral-sh/ruff-pre-commit">ruff-pre-commit</a> and <code class="language-plaintext highlighter-rouge">gitleaks</code> before committing my code.</p>

<p>Here’s a sample <code class="language-plaintext highlighter-rouge">.pre-commit-config.yaml</code> file that I use:</p>

<figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="code"><pre><span class="na">repos</span><span class="pi">:</span>

<span class="pi">-</span> <span class="na">repo</span><span class="pi">:</span> <span class="s">&lt;https://github.com/astral-sh/ruff-pre-commit&gt;</span>
    <span class="s">rev</span><span class="err">:</span> <span class="s">v0.12.3</span>  <span class="c1"># Ruff version.</span>
    <span class="na">hooks</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">id</span><span class="pi">:</span> <span class="s">ruff-check</span> <span class="c1"># Run the linter.</span>
        <span class="na">args</span><span class="pi">:</span> <span class="pi">[</span> <span class="nv">--fix</span> <span class="pi">]</span>
  <span class="pi">-</span> <span class="na">id</span><span class="pi">:</span> <span class="s">ruff-format</span>  <span class="c1"># Run the formatter.</span>
<span class="pi">-</span> <span class="na">repo</span><span class="pi">:</span> <span class="s">&lt;https://github.com/gitleaks/gitleaks&gt;</span>
    <span class="s">rev</span><span class="err">:</span> <span class="s">v8.27.2</span>
    <span class="s">hooks</span><span class="err">:</span>
  <span class="pi">-</span> <span class="na">id</span><span class="pi">:</span> <span class="s">gitleaks</span>
</pre></td></tr></tbody></table></code></pre></figure>

<h1 id="infrastructure-management">Infrastructure Management</h1>

<h2 id="make">Make</h2>

<p><a href="https://www.gnu.org/software/make/">Make</a> is a Swiss Army knife, a classic utility for automating tasks.
I use it to create simple shortcuts for common development commands.
Instead of remembering and typing out long CLI incantations to run tests, build Docker images, or start services, I define these tasks in a <code class="language-plaintext highlighter-rouge">Makefile</code>.
Then I just run commands like <code class="language-plaintext highlighter-rouge">make test</code> or <code class="language-plaintext highlighter-rouge">make infrastructure-up</code>.</p>

<p>As you might have noticed, there is a <code class="language-plaintext highlighter-rouge">Makefile</code> in both the <code class="language-plaintext highlighter-rouge">project-api</code> and the global <code class="language-plaintext highlighter-rouge">project</code> directories:</p>

<ol>
  <li><code class="language-plaintext highlighter-rouge">project/project-api/Makefile</code>: For linting, testing, and running the API.</li>
  <li><code class="language-plaintext highlighter-rouge">project/Makefile</code>: For building and running the infrastructure (via <code class="language-plaintext highlighter-rouge">docker-compose</code>).</li>
</ol>

<p>Here’s an extremely simple example of the <code class="language-plaintext highlighter-rouge">project-api</code> Makefile:</p>

<figure class="highlight"><pre><code class="language-plaintext" data-lang="plaintext"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="code"><pre>DIR := . # project/project-api/Makefile

test:
 uv run pytest

format-fix:
 uv run ruff format $(DIR)
 uv run ruff check --select I --fix

lint-fix:
 uv run ruff check --fix
</pre></td></tr></tbody></table></code></pre></figure>

<p>Now, if I want to run the tests, I just run <code class="language-plaintext highlighter-rouge">make test</code>, and it executes <code class="language-plaintext highlighter-rouge">uv run pytest</code> in the current directory.</p>

<p>For the global project, I use the following <code class="language-plaintext highlighter-rouge">Makefile</code>:</p>

<figure class="highlight"><pre><code class="language-plaintext" data-lang="plaintext"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="code"><pre>infrastructure-build:
 docker compose build

infrastructure-up:
 docker compose up --build -d

infrastructure-stop:
 docker compose stop
</pre></td></tr></tbody></table></code></pre></figure>

<p><code class="language-plaintext highlighter-rouge">make</code> is a powerful tool that can help you automate almost anything in your development workflow.
Although the examples above are very simple, just imagine how you can add more complex tasks as needed.</p>

<h2 id="docker">Docker</h2>

<p><a href="https://www.docker.com/">Docker</a> is a tool that allows you to package your application and its dependencies into a container,including everything needed to run: dependencies, system tools, code, and runtime OS.
When working locally, I use <a href="https://docs.docker.com/compose/">Docker Compose</a> to connect all Docker images into the same network.
Like Docker for dependencies, Docker Compose allows encapsulating the whole application stack and separating it from the rest of your local environment.</p>

<p>To fully grasp this concept, let’s take a look at a simple <code class="language-plaintext highlighter-rouge">docker-compose.yml</code> file:</p>

<figure class="highlight"><pre><code class="language-yaml" data-lang="yaml"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
</pre></td><td class="code"><pre><span class="na">version</span><span class="pi">:</span> <span class="s1">'</span><span class="s">3.8'</span>
<span class="na">services</span><span class="pi">:</span>
  <span class="na">project-api</span><span class="pi">:</span>
    <span class="na">build</span><span class="pi">:</span>
      <span class="na">context</span><span class="pi">:</span> <span class="s">./project-api</span>
      <span class="na">dockerfile</span><span class="pi">:</span> <span class="s">Dockerfile</span>
    <span class="na">ports</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s2">"</span><span class="s">8000:8000"</span>
    <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">./project-api:/app</span>
    <span class="na">environment</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">ENV_VAR=value</span>
    <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">project-network</span>

  <span class="na">project-ui</span><span class="pi">:</span>
    <span class="na">build</span><span class="pi">:</span>
      <span class="na">context</span><span class="pi">:</span> <span class="s">./project-ui</span>
      <span class="na">dockerfile</span><span class="pi">:</span> <span class="s">Dockerfile</span>
    <span class="na">ports</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s2">"</span><span class="s">3000:3000"</span>
    <span class="na">networks</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">project-network</span>

<span class="na">networks</span><span class="pi">:</span>
  <span class="na">project-network</span><span class="pi">:</span>
    <span class="na">driver</span><span class="pi">:</span> <span class="s">bridge</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>In this file, we define two services: <code class="language-plaintext highlighter-rouge">project-api</code> and <code class="language-plaintext highlighter-rouge">project-ui</code>.
Each service has its own build context (<code class="language-plaintext highlighter-rouge">Dockerfile</code>), ports, volumes, and environment variables.</p>

<p>Here’s a sample <code class="language-plaintext highlighter-rouge">Dockerfile</code> for the <code class="language-plaintext highlighter-rouge">project-api</code> service:</p>

<figure class="highlight"><pre><code class="language-dockerfile" data-lang="dockerfile"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre></td><td class="code"><pre><span class="k">FROM</span><span class="s"> python:3.11-slim</span>

<span class="c"># Install system dependencies</span>

<span class="k">COPY</span><span class="s"> --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/</span>

<span class="k">WORKDIR</span><span class="s"> /app</span>

<span class="k">COPY</span><span class="s"> uv.lock pyproject.toml README.md ./</span>
<span class="k">RUN </span>uv <span class="nb">sync</span> <span class="nt">--frozen</span> <span class="nt">--no-cache</span>

<span class="c"># Bring in the actual application code</span>

<span class="k">COPY</span><span class="s"> src/app app/</span>
<span class="k">COPY</span><span class="s"> tools tools/</span>

<span class="c"># Define a command to run the application</span>

<span class="k">CMD</span><span class="s"> ["/app/.venv/bin/fastapi", "run", "project/infrastructure/api.py", "--port", "8000", "--host", "0.0.0.0"]</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>As you can see, the Dockerfile starts from a Python base image, installs dependencies, copies the project files, and defines the command to run the FastAPI application.</p>

<p>This way, you can run the entire application stack with a single command:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">docker compose up <span class="nt">--build</span> <span class="nt">-d</span></code></pre></figure>

<h1 id="footnotes">Footnotes</h1>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:3">
      <p>If you know me, you know I used to be mostly a Java/JavaScript/R kind of guy. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>For example, today <a href="https://jupyter.org/">Jupyter</a> is bundled by almost every major cloud provider as the de facto tool for interactive data science and scientific computing. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7">
      <p>“Production-ready,” for me, means I can deploy the app to the cloud as-is, without needing to make a lot of infrastructure changes. <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:1">
      <p>Don’t get me wrong, I understand there are cases where a multi-repo structure is necessary, like when multiple teams work on different parts of the project or when dependencies needs to be shared across projects. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5">
      <p>I believe that aviding premature decomposition is a good idea. If a codebase is less than, say, 1/2 million LoC, then setting a network layer (like API calls) over it only would make maintenance a pain for <del>non-Amazon</del> rational developers. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>A <code class="language-plaintext highlighter-rouge">pyproject.toml</code> file is similar to <code class="language-plaintext highlighter-rouge">package.json</code> in Node.js or <code class="language-plaintext highlighter-rouge">pom.xml</code> in Java. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6">
      <p>By the way, I think every single project on GitHub should have its own website (it’s extremely easy via <a href="https://pages.github.com/">GitHub Pages</a>), so no excuses, sorry. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:8">
      <p>Using Docker for CI ensures parity with production environments, but it might add some cold-start overhead. You know… compromises, life is full of them. <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>César Soto Valero</name><email>cesarsotovalero@gmail.com</email></author><category term="tools" /><summary type="html"><![CDATA[I've started writing more Python code lately (because of... AI, you know). In this post, I share the tools, libraries, configs, and other integrations I use for building production-grade Python applications following a frontend-backend architecture.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.cesarsotovalero.net/img/posts/2025/2025-07-15/wall-stones_cover.JPG" /><media:content medium="image" url="https://www.cesarsotovalero.net/img/posts/2025/2025-07-15/wall-stones_cover.JPG" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Evaluation Metrics for Real-Time Financial Fraud Detection ML Models</title><link href="https://www.cesarsotovalero.net/blog/evaluation-metrics-for-real-time-financial-fraud-detection-ml-models.html" rel="alternate" type="text/html" title="Evaluation Metrics for Real-Time Financial Fraud Detection ML Models" /><published>2025-05-08T00:00:00-07:00</published><updated>2025-07-29T11:52:25-07:00</updated><id>https://www.cesarsotovalero.net/blog/evaluation-metrics-for-real-time-financial-fraud-detection-ml-models</id><content type="html" xml:base="https://www.cesarsotovalero.net/blog/evaluation-metrics-for-real-time-financial-fraud-detection-ml-models.html"><![CDATA[<p>After training a machine learning model for <a href="../blog/real-time-financial-fraud-detection.html">real-time financial fraud detection</a>, the next step is to evaluate its performance.
Fraud detection models face unique challenges during evaluation.
Two examples are (1) class imbalance and (2) the different costs of false positives vs. false negatives.
Class imbalance means that the model doesn’t have enough fraudulent transactions to learn from, as they are rare compared to legitimate ones.
False positives, which flag legitimate transactions as fraudulent, can lead to customer dissatisfaction, while false negatives, where fraudulent transactions go undetected, can result in significant financial losses.
In this post, I cover the most common metrics and considerations for evaluating fraud detection models while keeping these unique challenges in mind.</p>

<h1 id="confusion-matrix">Confusion Matrix</h1>

<p>Most financial fraud detection systems generate a binary output: a prediction indicating whether a transaction (txt) is fraudulent (1) or genuine (0).</p>

<p>By leveraging this universal approach to binary classification, we can establish standard evaluation methodologies to assess the performance of this type of models.</p>

<p>The confusion matrix is a widely used tool for summarizing and visualizing the performance of a classification model in a tabular format.
It provides a clear breakdown of predictions vs. actual outcomes.</p>

<p>In a confusion matrix:</p>

<ul>
  <li>The <em>x</em>-axis represents the ground-truth labels (actual outcomes).</li>
  <li>The <em>y</em>-axis represents the predictions made by the classification model.</li>
</ul>

<p>Both axes are divided into two categories: positive (fraudulent txt) and negative (genuine txt).
The positive class corresponds to the minority class (fraud), while the negative class corresponds to the majority class (genuine).</p>

<p><img src="/img/posts/2025/2025-05-08/confusion-matrix.svg" alt="Confusion Matrix" /></p>

<p>This representation allows us to visualize the model’s performance in terms of true positives, true negatives, false positives, and false negatives.</p>

<ul>
  <li><strong>True Positives (TP):</strong> The number of fraudulent transactions correctly identified as fraud.</li>
  <li><strong>True Negatives (TN):</strong> The number of genuine transactions correctly identified as genuine.</li>
  <li><strong>False Positives (FP):</strong> The number of genuine transactions incorrectly flagged as fraudulent.</li>
  <li><strong>False Negatives (FN):</strong> The number of fraudulent transactions missed by the system.</li>
</ul>

<p>By analyzing these metrics, we gain a comprehensive view of the model’s strengths and weaknesses, enabling informed decisions for further optimization.</p>

<h1 id="precision">Precision</h1>

<p><a href="https://en.wikipedia.org/wiki/Precision_and_recall">Precision</a> is the fraction of transactions that were actually fraud among all transactions the model flagged as fraud (predicted positive).</p>

<p>For example, if a model flagged 100 transactions as fraud, and 80 of those were indeed fraudulent, then:</p>

\[\text{Precision} = \frac{TP}{TP + FP} = \frac{80}{80 + 20} = 0.8\]

<p>High Precision means few false positives, so it is crucial for operational efficiency.</p>

<p>Low Precision means investigators waste time on many false alarms, and customers suffer unnecessary transaction declines.</p>

<p>Many top systems aim for very high Precision (e.g., 0.9+) at low fraud rates, but there’s a trade-off with Recall.
For example, if we lower the threshold to catch more fraud, Precision may drop.
Therefore, Precision is often reported at a certain operating point or as an average if multiple thresholds are considered.</p>

<p>An example interpretation: “Of the transactions our system blocked, 95% were indeed fraudulent”, that’s a Precision of 95%.</p>

<h1 id="recall">Recall</h1>

<p><a href="https://en.wikipedia.org/wiki/Precision_and_recall">Recall</a> is the fraction of actual fraud cases that the model correctly predicts as fraud (true positives) out of all actual fraud cases.</p>

<p>For example, if there were 100 actual fraud cases and the model caught 80 of them, then:</p>

\[\text{Recall} = \frac{TP}{TP + FN} = \frac{80}{80 + 20} = 0.8\]

<p>A Recall of 0.80 means 80% of fraud instances were detected (20% missed).</p>

<p>High Recall means few false negatives, which is critical in fraud detection because missing a fraudulent transaction can lead to financial losses.</p>

<p>Low Recall means many frauds slip through and cause losses.</p>

<p>We can usually increase Recall by lowering the detection threshold at the cost of Precision.
In practice, businesses may set a Recall target like “catch at least 70% of fraud” and then maximize Precision under that constraint.</p>

<h1 id="f1-score">F1-Score</h1>

<p><a href="https://en.wikipedia.org/wiki/F-score">F1-Score</a>, or F1 for short, is the harmonic mean of Precision and Recall.
It gives a single-figure balance of both metrics, which is useful when we want a combined score for model selection or when class distribution is skewed.</p>

<p>For example, if Precision is 0.8 and Recall is 0.6, then:</p>

\[F1 = \frac{2 \times Precision \times Recall}{Precision + Recall} \approx 0.686\]

<p>High F1 means both Precision and Recall are reasonably high.
Low F1 means either Precision or Recall is low, which is undesirable in fraud detection.</p>

<p>Overall, F1 is a good metric to assess a fraud detection model.
It is also a popular metric in Kaggle competitions and papers to compare models, ensuring they are not just optimizing one at the expense of the other.</p>

<h1 id="fpr">FPR</h1>

<p>False Positive Rate (FPR) is is the share of legitimate transactions that get incorrectly flagged as fraud.</p>

<p>For example, if there were 100 legitimate transactions and the model flagged 5 of them as fraud, then:</p>

\[\text{FPR} = \frac{FP}{TN + FP} = \frac{5}{95 + 5} = 0.05\]

<p>FPR is important because it directly impacts customer experience.
High FPR means many legitimate transactions are blocked, leading to customer dissatisfaction and potential loss of business trust.</p>

<p>Sometimes businesses set FPR requirements to control false alarms.
For example: “We can only tolerate reviewing 0.5% of transactions, so FPR must be ≤ 0.005.”</p>

<h1 id="fnr">FNR</h1>

<p>False Negative Rate (FNR) is the share of fraudulent transactions the model misses.</p>

<p>For example, if there were 100 actual fraud cases and the model missed 2 of them, then:</p>

\[\text{FNR} = \frac{FN}{TP + FN} = \frac{2}{98 + 2} = 0.025\]

<p>Some businesses set FNR requirements to control missed fraud.
For example: “We cannot tolerate missing more than 10% of fraud, so FNR ≤ 0.1” which implies Recall ≥ 0.9.</p>

<h1 id="tnr">TNR</h1>

<p>True Negative Rate (TNR) or Specificity measures how well the system correctly identifies legitimate transactions as non-fraud.</p>

<p>For example, if there were 1000 legitimate transactions and the model flagged 50 of them incorrectly as fraud (FP), the calculation would be:</p>

\[\text{TNR} = \frac{TN}{TN + FP} = \frac{950}{950 + 50} = 0.95\]

<p>TNR is often overlooked in fraud detection because it’s essentially the complement of the False Positive Rate (FPR):</p>

\[\text{TNR} = 1 - \text{FPR}\]

<p>TNR is typically very high in fraud detection systems because the number of legitimate transactions (TN) is much larger than the number of frauds or false positives.</p>

<p>Since Precision already focuses on avoiding false positives, and we typically assume we want to approve as many legitimate transactions as possible, TNR doesn’t usually take center stage.</p>

<p>However, in some contexts, like regulatory requirements or customer experience, it’s important to keep FPR below a certain threshold, such as “FPR ≤ 0.1%,” which directly relates to maintaining high TNR.</p>

<h1 id="auc-roc">AUC-ROC</h1>

<p>The Area Under the ROC Curve<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> (AUC-ROC) measures a model’s ability to distinguish fraud from non-fraud across all possible thresholds.
In essence, it plots Recall against FPR.</p>

<p>The AUC is the area under this curve:</p>

<ul>
  <li>AUC = 0.5 means random guessing.</li>
  <li>AUC = 1.0 means perfect discrimination.</li>
</ul>

<p>This area is computed as follows:</p>

\[\text{AUC} = \int_0^1 \text{Recall}(FPR) \, dFPR\]

<p>AUC is threshold-independent: it summarizes performance across all thresholds, and it’s less sensitive to class imbalance than accuracy.</p>

<p>An intuitive interpretation: “If I randomly pick a fraud and a legitimate transaction, AUC is the chance the fraud gets a higher risk score.”</p>

<h1 id="auc-pr">AUC-PR</h1>

<p>The Area Under Precision-Recall Curve (AUC-PR) plots Precision vs. Recall and focuses squarely on the minority class (fraud), so it tells us how well the model catches fraud while keeping false positives low.</p>

<p>In highly imbalanced data such as fraud detection, AUC-PR is more informative than AUC-ROC because it answers how well the model balances Precision and Recall where it matters.</p>

<p>For instance, a model could have AUC-ROC = 0.98 and still have AUC-PR = 0.10, which means that the model detects fraud more often than non-fraud, but when it comes to real-world detection, Precision at high Recall isn’t stellar.</p>

<p>AUC-PR is the go-to metric when fraud cases are rare, and we care about catching as many as possible without overwhelming the system with false alarms.</p>

<h2 id="threshold-for-auc-pr">Threshold for AUC-PR</h2>

<p>Once we have chosen the best model as per AUC-PR, we need to decide a threshold, denoted as \(\tau\), to convert this model’s fraud probability output into a concrete binary decision (fraud or not fraud).</p>

<p>If we look at the Precision-Recall (PR) curve in the figure below, different values of \(\tau\) correspond to different trade-offs between Precision and Recall.</p>

<figure class="jb_picture">
  



<img width="100%" style="border: 1px solid #808080;" src="/assets/resized/AUPRC-640x477.png" alt="Sample Precision-Recall curves for two models A and B." data-srcset="/assets/resized/AUPRC-640x477.png 640w,/assets/resized/AUPRC-768x573.png 768w,/assets/resized/AUPRC-1024x763.png 1024w,/assets/resized/AUPRC-1366x1018.png 1366w," class="blur-up lazyautosizes lazyload" />
  <figcaption class="stroke">
    Sample Precision-Recall curves for two models A and B. Model B is superior to model A as is reflected in the AUC-PR values of the two models. Different points on the PR curve represent different threshold values and different trade-offs between Precision and Recall metrics.
  </figcaption>
</figure>

<p>We ultimately need to choose the right trade-off that suits our use case.
The threshold \(\tau\) determines the decision boundary for classifying transactions as fraudulent or genuine. Mathematically, this can be expressed as:</p>

\[\text{Decision: Fraud if } P(x) &gt; \tau\]

<p>Where:</p>

<ul>
  <li>\(P(x)\) is the predicted probability of fraud for transaction \(x\).</li>
  <li>\(\tau\) is the threshold value.</li>
</ul>

<p>With the above framework in mind, we can decide the threshold value \(\tau\) based on the value of \(k\), where \(k\) represents the minimum Precision we want to maintain.</p>

<p>For example, if we want to maintain a minimum Precision of 90%, then \(k = 90\). Using the Precision-Recall curve, we can derive the threshold value \(\tau\) as well as the equivalent Recall value at 90% Precision.</p>

<p>This strategy allows us to calculate the optimal threshold \(\tau\) while evaluating our trained model on the test set. Once the threshold is determined, it can be used to classify transactions during deployment:</p>

\[\text{Fraud if } P(x) &gt; \tau, \text{ otherwise Genuine.}\]

<p>By adjusting \(\tau\), we can balance Precision and Recall to meet specific business objectives and constraints.</p>

<h1 id="latency">Latency</h1>

<p>Latency is the time it takes for the system to process a transaction and make an inference.
Keeping latency low is crucial for real-time systems.
Fraud models not only need to have good statistical performance but also operate quickly enough to be used in practice.</p>

<p>Latency and complexity matter in payment systems.
In the example below, the <code class="language-plaintext highlighter-rouge">Payment Server</code> dispatches parallel calls to <code class="language-plaintext highlighter-rouge">KYC Service</code>, <code class="language-plaintext highlighter-rouge">Fraud Check</code>, and <code class="language-plaintext highlighter-rouge">Payment Rail</code>, but the transaction can only complete once the slowest of these services responds (the <code class="language-plaintext highlighter-rouge">KYC Service</code> in this example).
Even though <code class="language-plaintext highlighter-rouge">Fraud Check</code> takes just 25 ms, any fluctuation (like a network hiccup or a slow third-party response) can bottleneck the entire flow.
That’s why latency is a system-wide risk amplifier.</p>

<pre><code class="language-mermaid">flowchart LR
  %% define the Payment App node
  PaymentApp[Payment App]

  %% container for backend services
  subgraph container [" "]
    direction LR
    Server[Payment Server]
    KYC[KYC Service]
    Fraud[Fraud Check]
    Rail[Payment Rail]
  end

  %% draw the edges with labels
  PaymentApp --&gt;|50ms| Server
  Server     --&gt;|50ms| KYC
  Server     --&gt;|25ms| Fraud
  Server     --&gt;|25ms| Rail

  %% style all four links purple
  linkStyle 0,1,2,3 stroke:#800080,stroke-width:2px

  %% highlight Fraud Check in red
  style Fraud fill:#ffe6e6,stroke:#ff0000,stroke-width:2px
</code></pre>

<p>Real-time fraud detection latency is commonly measured in two ways:</p>

<ol>
  <li><strong>Online decision latency (ODL):</strong> How long it takes to score a single transaction and respond (which affects user experience and fraud blocking effectiveness).</li>
  <li><strong>Time-to-detection for fraud patterns (TTD):</strong> If an attack starts at a certain time, how long before the system detects and flags it.</li>
</ol>

<p>ODL is usually measured in milliseconds.
For example, a payment system might have an end-to-end latency budget of 200ms for authorization, out of which fraud checks get 20–30ms.
Modern systems often aim for fraud model inference under ~50ms.
In practice, we can look at 99th percentile latency (e.g., 99% of transactions scored in &lt;500ms), to ensure worst-case delays are bounded.</p>

<p>TTD is more about monitoring and measuring the resilience of the system to detect an emerging fraud <em>modus operandi</em>.
For example: “Did we catch the new fraud ring the first day it appeared, or did it go undetected for weeks?”
This is harder to quantify but important in evaluating adaptive systems.</p>

<h1 id="summary">Summary</h1>

<p>In practice, evaluating a fraud detection model involves:</p>

<ol>
  <li>Analyzing the confusion matrix at the operating point.</li>
  <li>Reviewing Recall, F1, and AUC-PR.</li>
  <li>Choosing a threshold that satisfies business constraints (e.g., maximum number of tolerable false positives).</li>
</ol>

<p>But evaluation doesn’t stop at metrics.
Weighting fraud by transaction amount matters: catching a 10,000 USD fraud is more impactful than catching five 1,000 USD cases.
Moreover, metrics on static test sets aren’t enough.
We also need to perform <a href="https://en.wikipedia.org/wiki/Backtesting">backtesting</a> (simulate past performance) and <a href="https://en.wikipedia.org/wiki/Sandbox_(computer_security)">sandbox testing</a> (simulate deployment), and monitor the model in production.</p>

<p>Observe how fraud patterns change: Do attackers evolve? Do false positives creep up?</p>

<p>Or even better: run <a href="https://en.wikipedia.org/wiki/A/B_testing">A/B tests</a>.</p>

<p>Put the new model in production in <a href="https://en.wikipedia.org/wiki/Shadowing_(computing)">shadow mode</a> and compare it to the previous version.</p>

<p>But that’s content for another post.</p>

<h1 id="footnotes">Footnotes</h1>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p><a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">ROC Curve</a> stands for “Receiver Operating Characteristic”, a very weird name, indeed. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>César Soto Valero</name><email>cesarsotovalero@gmail.com</email></author><category term="ai" /><summary type="html"><![CDATA[After training a real-time financial fraud detection model, the next step is to evaluate its performance. This post provides an overview of the most common evaluation metrics and considerations for fraud detection models, including confusion matrix, precision, recall, F1-score, AUC-ROC, AUC-PR, and more.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.cesarsotovalero.net/img/posts/2025/2025-05-08/kungstradgarden_cover.jpg" /><media:content medium="image" url="https://www.cesarsotovalero.net/img/posts/2025/2025-05-08/kungstradgarden_cover.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">From Classical ML to DNNs and GNNs for Real-Time Financial Fraud Detection</title><link href="https://www.cesarsotovalero.net/blog/from-classical-ml-to-dnns-and-gnns-for-real-time-financial-fraud-detection.html" rel="alternate" type="text/html" title="From Classical ML to DNNs and GNNs for Real-Time Financial Fraud Detection" /><published>2025-04-03T00:00:00-07:00</published><updated>2025-07-29T11:52:25-07:00</updated><id>https://www.cesarsotovalero.net/blog/from-classical-ml-to-dnns-and-gnns-for-real-time-financial-fraud-detection</id><content type="html" xml:base="https://www.cesarsotovalero.net/blog/from-classical-ml-to-dnns-and-gnns-for-real-time-financial-fraud-detection.html"><![CDATA[<p>Financial fraud is a pervasive problem costing institutions and customers billions annually.<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>
Most known examples include credit card fraud, fraudulent online payments, and money laundering.
Banks worldwide faced an estimated \(\$442\) billion in fraud-related losses in 2023 alone.
In particular, credit card transactional fraud is projected to reach \(\$43\) billion in annual losses by 2026.
Beyond direct losses, fraud undermines customer trust and damages banks’ reputation.
For example, it leads to false positives where legitimate transactions are wrongly blocked.
Consequently, financial fraud detection systems (a.k.a fraud scoring) must not only catch as much fraud as possible but also minimize false positives.</p>

<p>Fraudsters’ tactics evolve rapidly.
Traditional rule-based systems (or simple statistical methods) have proven inadequate against today’s adaptive fraud models.
On one hand, fraudsters form complex schemes and exploit networks of accounts.
On the other hand, legitimate transaction volumes continue to grow due to the rise of e-commerce and digital payments.</p>

<p>This situation has driven a shift toward Machine Learning (ML) and AI-based approaches that can learn subtle patterns and adapt over time.
Critically, financial fraud detection must happen in real-time (or near-real time) to intervene before fraudsters can complete illicit transactions.
Catching fraud “closer to the time of fraud occurrence is key” so that suspicious transactions can be blocked or flagged immediately.<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup></p>

<p>This article deep dives into the current state-of-the-art of real-time transactional fraud detection, spanning both academic research and current industry practices.</p>

<p>I cover the major model families used today:</p>

<ol>
  <li><strong>Classical ML models:</strong> Logistic regression, decision trees, random forests, and SVMs.</li>
  <li><strong>Deep Learning models:</strong> ANNs, CNNs, RNNs/LSTMs, autoencoders, and GANs.</li>
  <li><strong>Graph-based models:</strong> GNNs and graph algorithms that leverage transaction relationships.</li>
  <li><strong>Transformer-based and foundation models:</strong> Large pre-trained models like Stripe’s payments foundation model.</li>
</ol>

<p>For each category, I discuss representative use cases or studies, highlight strengths and weaknesses, and comment on their suitability for real-time fraud detection.</p>

<h1 id="classical-ml-models">Classical ML Models</h1>

<p>Classical ML algorithms have long been used in fraud detection and remain strong baselines in both research and production systems.
These include linear models like <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a>, distance-based classifiers like <a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">k-Nearest Neighbors</a>, and tree-based models such as <a href="https://en.wikipedia.org/wiki/Random_forest">random forest</a> and <a href="https://en.wikipedia.org/wiki/Gradient_boosting">gradient boosted trees</a> (e.g., <a href="https://xgboost.readthedocs.io/">XGBoost</a>).
These approaches operate on hand-crafted features derived from transaction data (e.g., <code class="language-plaintext highlighter-rouge">transaction_amount</code>, <code class="language-plaintext highlighter-rouge">location</code>, <code class="language-plaintext highlighter-rouge">device_id</code>, <code class="language-plaintext highlighter-rouge">time_of_day</code>, etc.), often requiring substantial feature engineering by domain experts.</p>

<p><strong>Logistic regression</strong> is a foundational model in fraud detection.
Banks and financial institutions have historically relied on it due to its simplicity and interpretability (each coefficient \(w_i\) has a direct and intuitive meaning). A positive coefficient means the feature increases the log-odds of fraud, a negative coefficient means it decreases the risk.</p>

\[P(y = 1 \mid \mathbf{x}) = \frac{1}{1 + e^{-(\mathbf{w}^\top \mathbf{x} + b)}}\]

<ul>
  <li>\(\mathbf{x}\): Feature vector (e.g., transaction amount, time of day, merchant category)</li>
  <li>\(\mathbf{w}\): Coefficients (risk factors)</li>
  <li>\(b\): Bias or intercept</li>
</ul>

<p>Even today, logistic models serve as interpretable baseline detectors and are sometimes combined with a <a href="https://en.wikipedia.org/wiki/Business_rule_management_system">Business Rule Management Systems</a>.
However, linear models struggle to capture complex non-linear patterns in large transaction datasets.</p>

<aside class="quote">
    <em>“XGBoost builds trees sequentially, where each tree learns from the mistakes of the previous ones.”</em>
</aside>

<p><strong>Decision trees</strong> and ensemble forests address this by automatically learning non-linear splits and interactions.
In fact, boosted decision tree ensembles (like XGBoost) became popular in fraud detection competitions and industry solutions due to their high accuracy on tabular data.<sup id="fnref:34"><a href="#fn:34" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>
These models can capture anomalous combinations of features in individual transactions effectively, learning complex, non-linear interactions between features.
For example, <a href="https://github.com/VedangW/ieee-cis-fraud-detection">the winning solutions</a> of the <a href="https://www.kaggle.com/c/ieee-fraud-detection/overview">IEEE-CIS fraud detection Kaggle challenge</a> (2019) heavily used engineered features fed into gradient boosting models, achieving strong performance (AUC ≈ 0.91).</p>

<p><strong>Support Vector Machines</strong> (<a href="https://en.wikipedia.org/wiki/Support_vector_machine">SVMs</a>) have also been explored in academic studies.
However, while they can model non-linear boundaries (with kernels), they tend to be computationally heavy for large datasets and offer no interpretable output.
Therefore, the industry has gravitated more to tree ensembles for complex models.</p>

<h2 id="strengths">Strengths</h2>

<p>Classical ML models are typically fast to train and infer, and many (especially logistic regression and decision trees) are relatively easy to interpret.
For instance, a logistic regression might directly quantify how much a mismatched billing address raises fraud probability, and a decision tree might provide a rule-like structure (e.g., “if IP country ≠ card country and amount &gt; $1000 ⇒ flag fraud”).
More complex models like XGBoost still allow some interpretability through <a href="https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/">feature importance scores</a>, <a href="https://medium.com/@lfoster49203/interpretable-machine-learning-models-using-shap-and-lime-for-complex-data-6f65e1224209">CHAP values</a>, or partial dependence plots.</p>

<p>Classical ML models can be deployed in real-time with minimal latency.
A logistic regression is essentially a dot-product of features, and even a large XGBoost ensemble can score a transaction in tens of milliseconds or less on modern hardware.</p>

<p>They also perform well with limited data.
With careful feature engineering, a simple model can already catch a large fraction of fraud patterns.
Consequently, industry adoption is widespread, many banks initially deploy logistic or tree-based models in production, and even today XGBoost is a common choice in fraud ML pipelines.</p>

<h2 id="weaknesses">Weaknesses</h2>

<p>A key limitation of classical ML models is the reliance on manual feature engineering.
In other words, they cannot automatically invent new abstractions beyond the input features given.
These models may miss complex patterns such as sequential spending behavior or R-ring collusion<sup id="fnref:28"><a href="#fn:28" class="footnote" rel="footnote" role="doc-noteref">4</a></sup> between groups of accounts unless analysts explicitly code such features (e.g., number of purchases in the last hour, or count of accounts sharing an email domain).</p>

<p>They may also struggle with high-dimensional data like raw event logs or image data (this is where deep learning excels).
However, this is less an issue for structured transaction records.</p>

<p>Another challenge is class imbalance.
The occurrence of fraud is typically rare (often $ &lt;1\% $ of transactions), which can bias models to predict the majority “non-fraud” class.
Techniques like <a href="https://medium.com/@ravi.abhinav4/improving-class-imbalance-with-class-weights-in-machine-learning-af072fdd4aa4">balanced class weighting</a>, <a href="https://www.kaggle.com/code/residentmario/undersampling-and-oversampling-imbalanced-data">undersampling</a>, or <a href="https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/">SMOTE</a> are often needed to train classical models effectively on imbalanced fraud data.<sup id="fnref:16"><a href="#fn:16" class="footnote" rel="footnote" role="doc-noteref">5</a></sup></p>

<p>Finally, while faster than deep neural networks, complex ensembles (hundreds of trees) can become memory-intensive and may require optimization for ultra-low latency at high transaction volumes.</p>

<h2 id="real-time-suitability">Real-Time Suitability</h2>

<p>Classical models are generally well-suited to real-time fraud scoring.
They have low latency inference and modest resource requirements.</p>

<p>For example, a bank’s fraud engine might run a logistic regression and a few decision tree rules in under 10ms per transaction on a CPU.
Even a sophisticated random forest or gradient boosting model can be served via highly optimized C++ libraries or cloud ML endpoints to meet sub-hundred-millisecond SLAs.<sup id="fnref:29"><a href="#fn:29" class="footnote" rel="footnote" role="doc-noteref">6</a></sup></p>

<p>The straightforward nature of these models also simplifies transaction monitoring and model updates.
New data can be used to frequently retrain or update coefficients (even via <a href="https://en.wikipedia.org/wiki/Online_machine_learning">online learning</a> for logistic regression).
The main caution is that if fraud patterns shift significantly (<a href="https://en.wikipedia.org/wiki/Concept_drift">concept drift</a>), purely static classical models will need frequent retraining to keep up.</p>

<p>In practice, many organizations retrain or fine-tune their fraud models on recent data weekly or even daily to adapt to new fraud tactics.
So, while classical models are fast to deploy and iterate on, they do require ongoing maintenance to remain effective.</p>

<h2 id="examples">Examples</h2>

<p>Representative research and use-cases for classical methods include:</p>

<ul>
  <li><strong>Logistic regression and decision trees as baseline models:</strong> Many banks have deployed logistic regression for real-time credit card fraud scoring due to its interpretability.</li>
  <li><strong>Ensemble methods in academic studies:</strong> Research has focused on evaluating logistic vs. decision tree vs. random forest on a credit card dataset (often finding tree ensembles outperform linear models in Recall).<sup id="fnref:17"><a href="#fn:17" class="footnote" rel="footnote" role="doc-noteref">7</a></sup></li>
  <li><strong>Kaggle competitions:</strong> XGBoost was heavily used in the <a href="https://www.kaggle.com/c/ieee-fraud-detection">Kaggle IEEE-CIS 2019 competition</a>, leveraging high accuracy on tabular features.</li>
  <li><strong>Hybrid systems:</strong> Many production systems combine manual business rules for known high-risk patterns with an ML model for subtler patterns, using the rules for immediate high-precision flags and the ML model for broad coverage.</li>
</ul>

<h1 id="deep-learning-models">Deep Learning Models</h1>

<p>In recent years, <a href="https://en.wikipedia.org/wiki/Deep_learning#Deep_neural_networks">Deep Neural Networks</a> (DNNs) have been <a href="https://opencv.org/blog/online-transaction-fraud-detection-using-deep-learning">applied to transaction fraud detection</a> with promising results.
DNNs can automatically learn complex feature representations from raw data, potentially capturing patterns that are hard to manually engineer or find with classical ML models.</p>

<h2 id="deep-learning-architectures">Deep Learning Architectures</h2>

<p>Several deep architectures have been explored for fraud detection.
Below, I summarize the most common types.</p>

<h3 id="feed-forward-neural-networks-anns">Feed-Forward Neural Networks (ANNs)</h3>

<p><a href="https://en.wikipedia.org/wiki/Feedforward_neural_network">ANNs</a> are multi-layer perceptron treating each transaction’s features as input neurons.
These can model non-linear combinations of features beyond what logistic regression can capture.
In practice, simple feed-forward networks have been used as a baseline deep model for fraud (e.g., a 3-layer network on credit card data).
They often perform similarly to tree ensembles if ample data is available but are harder to interpret.
They also don’t inherently handle sequential or time-based information beyond what the input features provide.</p>

<h3 id="convolutional-neural-networks-cnns">Convolutional Neural Networks (CNNs)</h3>

<p><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">CNNs</a> are most famous for image-related tasks.
However, they have also being been applied to fraud by treating transaction data as temporal or spatial sequences.
For example, a CNN can slide over a sequence of past transactions for a user to detect local patterns or use 1D convolution on time-series of transaction amounts.</p>

<p>CNNs excel at automatic feature extraction of localized patterns.
Some research reformats transaction histories into a 2D “image” (e.g., time vs. feature dimension) so that CNNs can detect anomalous shapes.</p>

<p>CNNs for detecting fraud have seen limited but growing use.
One recent study reported ~99% detection accuracy with a CNN on a credit card dataset.<sup id="fnref:19"><a href="#fn:19" class="footnote" rel="footnote" role="doc-noteref">8</a></sup>
However, such high accuracy is likely due to the highly imbalanced nature of the dataset (using AUC or F1 is more meaningful).</p>

<h3 id="recurrent-neural-networks-rnns">Recurrent Neural Networks (RNNs)</h3>

<p><a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">RNNs</a>, including <a href="https://en.wikipedia.org/wiki/Long_short-term_memory">LSTM</a> and <a href="https://en.wikipedia.org/wiki/Gated_recurrent_unit">GRU</a> networks, are well-suited for sequential transactional data.
They maintain a memory of past events, making them ideal for modeling an account’s behavior over time.</p>

<p>For example, an LSTM can consume a customer’s sequence of transactions (with timestamps) and detect if the latest transaction is anomalous given the recent pattern.
This temporal modeling is very powerful for fraud because many fraud patterns only make sense in context (e.g., a sudden spending spike, or a purchase in a new country right after another far-away purchase).</p>

<p>Research has shown LSTM-based models can effectively distinguish fraudulent vs. legitimate sequences.
In one case, an LSTM achieved significantly higher Recall than static models by catching subtle temporal shifts in user behavior.<sup id="fnref:13"><a href="#fn:13" class="footnote" rel="footnote" role="doc-noteref">9</a></sup>
RNNs do require sequential data, so for one-off transactions without history they are less applicable (unless modeling at the merchant or account aggregate level).</p>

<h3 id="autoencoders">Autoencoders</h3>

<p><a href="https://en.wikipedia.org/wiki/Autoencoder">Autoencoders</a> are unsupervised anomaly detection models that learn to compress and reconstruct data.
When trained on predominantly legitimate transactions, an autoencoder captures the underlying structure of normal behavior (a.k.a. the “normal manifold”).
As a result, it can reconstruct typical transactions with very low error, but struggles with atypical or anomalous ones.
A transaction that doesn’t conform to the learned normal pattern will produce a higher reconstruction error.
By setting a threshold, we can flag transactions with unusually high reconstruction error as potential fraud.</p>

<p>Autoencoders shine in fraud detection, particularly when labeled fraud data is scarce or nonexistent.<sup id="fnref:33"><a href="#fn:33" class="footnote" rel="footnote" role="doc-noteref">10</a></sup>
Their strength lies in identifying transactions that deviate from the learned “normal” without requiring explicit fraud labels during training.
For example, an autoencoder trained on millions of legitimate transactions will likely assign high reconstruction error to fraudulent ones it’s never seen before.
<a href="https://en.wikipedia.org/wiki/Variational_autoencoder">Variational Autoencoder</a>s (VAEs), which introduce probabilistic modeling and latent-space regularization—have also been explored for fraud detection, offering potentially richer representations of normal transaction behavior.<sup id="fnref:21"><a href="#fn:21" class="footnote" rel="footnote" role="doc-noteref">11</a></sup></p>

<h3 id="generative-adversarial-networks-gans">Generative Adversarial Networks (GANs)</h3>

<p><a href="https://en.wikipedia.org/wiki/Generative_adversarial_network">GANs</a> consist of a generator and discriminator.
The generator creates synthetic data, while the discriminator tries to distinguish real from fake data.</p>

<p>There are two main applications of GANs in fraud detection:</p>

<ol>
  <li>
    <p><strong>Generate realistic synthetic fraud examples:</strong> GANs can augment training data to address class imbalance. The generator is trained to produce fake transactions that the discriminator (trained to distinguish real vs. fake) finds plausible. By adding these synthetic frauds to the training set, models (including non-deep models) can learn a broader decision boundary.</p>
  </li>
  <li>
    <p><strong>Serve as anomaly detectors:</strong> The generator tries to model the distribution of legitimate transactions, and the discriminator’s output can highlight outliers.</p>
  </li>
</ol>

<p>Some financial institutions have experimented with GANs.
For example, <a href="https://developer.nvidia.com/blog/detecting-financial-fraud-using-gans-at-swedbank-with-hopsworks-and-gpus/">Swedbank reportedly used GANs</a> to generate additional fraudulent examples for training their models.
However, GAN training can be complex and less common in production.
Still, in research, GAN-based methods have shown improved Recall by expanding the fraud training sample space.<sup id="fnref:22"><a href="#fn:22" class="footnote" rel="footnote" role="doc-noteref">12</a></sup></p>

<h3 id="hybrid-deep-learning-models">Hybrid Deep Learning Models</h3>

<p>There are also custom DNNs architectures combining elements of the above, or combining deep models with classical ones.</p>

<p>For example, a “wide and deep model” might have a linear (wide) component for memorizing known risk patterns and a neural network (deep) component for generalization.
Another example is combining an LSTM for sequence modeling with a feed-forward network for static features (“dual-stream” models).</p>

<p>Ensembles of deep and non-deep models have also been used (e.g., using an autoencoder’s anomaly score as an input feature to a random forest).
Recent research explores stacking deep models with tree models to improve robustness and interpretability.</p>

<h2 id="strengths-1">Strengths</h2>

<p>DNNs biggest advantage is automated feature learning.
These types of models can uncover intricate, non-linear relationships and subtle correlations within massive datasets that older methods miss.
They can digest raw inputs (inc. unstructured data) and find patterns without explicit human-designed features.
For instance, an RNN can learn the notion of “rapid spending spree” or “geographical inconsistency” from raw sequences, which would be hard to capture with handcrafted features.</p>

<p>In fraud detection, large payment companies have millions of transactions which deep models can leverage to potentially exceed the accuracy of simpler models.
DNNs also tend to improve with more data, whereas classical models may saturate in performance.</p>

<p>Another strength is handling complex data types.
For example, if one incorporates additional signals like device fingerprints, text (e.g., product names), or network information, deep networks can combine these modalities more seamlessly (e.g., an embedding layer for device ID, an LSTM for text description, etc.).</p>

<p>In practice, DNNs have shown higher Recall at a given false-positive rate compared to classical models, in several cases.<sup id="fnref:13:1"><a href="#fn:13" class="footnote" rel="footnote" role="doc-noteref">9</a></sup>
They are also adaptive architectures like RNNs or online learning frameworks can update as new data comes in, enabling continuous learning, which is important as fraud scenarios evolve.</p>

<h2 id="weaknesses-1">Weaknesses</h2>

<p>The primary downsides of DNN are complexity and interpretability.</p>

<p>Deep networks are considered “black boxes”, meaning that it’s non-trivial to explain why a certain transaction was flagged as fraudulent.
This is problematic for financial institutions that need to justify decisions to customers or regulators.
Techniques like <a href="https://shap.readthedocs.io/">SHapley Additive exPlanations</a> (SHAP) or <a href="https://github.com/marcotcr/lime">Local Interpretable Model-Agnostic Explanations</a> (LIME) can help interpret feature importance for deep models, <a href="https://www.milliman.com/en/insight/Explainable-AI-in-fraud-detection">but it’s still harder</a> compared to a linear model or decision tree.</p>

<aside class="quote">
    <em>“DNNs can really shine only when there are huge datasets or additional unlabeled data to pre-train on.”</em>
</aside>

<p>Another issue is the data and compute requirement.
Training large DNNs may require GPUs and extensive hyperparameter tuning, which can be overkill for some fraud datasets, especially if data is limited or highly imbalanced.<sup id="fnref:32"><a href="#fn:32" class="footnote" rel="footnote" role="doc-noteref">13</a></sup>
In fact, many academic studies on the popular <a href="https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud">Kaggle credit card dataset</a> (284,807 transactions) found that simpler models can match DNNs performance, likely because the dataset is small and mostly numeric features.</p>

<p>Overfitting is a risk too, fraud datasets are skewed and sometimes composed of static snapshots in time.
A DNN might memorize past fraud patterns that fraudsters no longer use, if not carefully regularized.</p>

<p>Finally, latency can be a concern.
A large CNN or LSTM might take longer to evaluate than a logistic regression.
However, many deep models used for fraud are not excessively large (e.g., an LSTM with a few hundred units), and with optimized inference (batching, quantization, etc.) they can often still meet real-time requirements.
I discuss latency more later, but suffice it to say that deploying deep models at scale might necessitate GPU acceleration or model optimizations in high-throughput environments.</p>

<h2 id="real-time-suitability-1">Real-Time Suitability</h2>

<p>DNNs models can be deployed for real-time fraud scoring, but it requires more care than classical models.
Simpler networks (small MLPs) are no issue in real-time.
However, RNNs or CNNs might introduce slight latency (tens of milliseconds).
Nevertheless, modern inference servers and even FPGAs/TPUs can handle thousands of inferences per second.
For instance, Visa reportedly targets fraud model evaluations in under ~25ms as part of their payment authorization pipeline.
It’s feasible to achieve this with a moderately sized neural network and good infrastructure.</p>

<p>Scaling to high transaction volumes is another aspect.
Deep models may consume more CPU/GPU resources, so a cloud deployment might need to autoscale instances or use GPU inference for peak loads.</p>

<p>A potential strategy for real-time use is a two-stage system: a fast classical model first filters obvious cases (either definitely legitimate or obviously fraudulent), and a slower deep model only analyzes the ambiguous middle chunk of transactions.
This way, the heavy model is used on a fraction of traffic to keep overall throughput high.</p>

<p>Additionally, organizations often maintain a feedback loop.
Flagged predictions are first reviewed by analysts or via outcomes like chargebacks, and then a DNN model is retrained frequently to incorporate the latest data.</p>

<p>Some deep models can be updated via online learning.
For example, an RNN that continuously updates its hidden state or a streaming NN that periodically retrains on a rolling window of data, which helps keep them current with
concept drift.</p>

<h2 id="examples-1">Examples</h2>

<p>Notable examples of deep learning in fraud detection:</p>

<ul>
  <li><strong>Feedforward DNNs:</strong> PayPal in the mid-2010s <a href="https://www.paypal.com/us/brc/article/payment-fraud-detection-machine-learning?utm_source=chatgpt.com">applied neural networks to fraud</a>, fintech companies like Feedzai have further advanced this methodology by combining DNNs with tree-based models.<sup id="fnref:24"><a href="#fn:24" class="footnote" rel="footnote" role="doc-noteref">14</a></sup></li>
  <li><strong>RNNs and LSTMs:</strong> Multiple studies have shown that LSTM networks can detect sequential fraud behavior that static models miss, improving Recall by capturing temporal patterns. Large merchants have employed LSTM-based models to analyze user event streams, enabling the detection of account takeovers and session-based fraud in real-time.</li>
  <li><strong>Autoencoder-based anomaly detection:</strong> Unsupervised autoencoders have been used by banks to flag new types of fraud. For instance, an autoencoder trained on normal mobile transactions flagged anomalies that turned out to be new fraud rings exploiting a loophole (detected via high reconstruction error).</li>
  <li><strong>Hybrid models:</strong> Recent trends include using DNNs to generate features for a gradient boosted tree. One effective approach is to use deep learning models, such as autoencoders or embedding networks, to learn rich feature representations from transaction data. These learned embeddings are then fed into XGBoost, combining the deep models’ ability to capture complex patterns with the interpretability and efficiency of tree-based methods</li>
</ul>

<h1 id="graph-based-models">Graph-Based Models</h1>

<p>Groups of fraudsters might share information (e.g., using the same stolen cards or devices), or a single fraudster might operate many accounts that transact with each other.
A powerful class of methods treats the financial system as a graph, linking entities like users, accounts, devices, IP addresses, merchants, etc.
<a href="https://github.com/safe-graph/graph-fraud-detection-papers">Graph-based fraud detection models</a> aim to exploit these relational structures to detect fraud patterns that single-transaction models might miss.
Classical graph algorithms can then be applied, such as community detection<sup id="fnref:25"><a href="#fn:25" class="footnote" rel="footnote" role="doc-noteref">15</a></sup> and link analysis (e.g., <a href="https://en.wikipedia.org/wiki/PageRank">PageRank</a> on the fraud graph).</p>

<figure class="jb_picture">
  



<img width="100%" style="border: 1px solid #808080;" src="/assets/resized/suspicious-subgraphs-640x378.png" alt="Illustration of entity linkages in transaction fraud" data-srcset="/assets/resized/suspicious-subgraphs-640x378.png 640w,/assets/resized/suspicious-subgraphs-768x454.png 768w," class="blur-up lazyautosizes lazyload" />
  <figcaption class="stroke">
    Illustration of entity linkages in transaction fraud: Shared devices, phone numbers, and locations connect different users. Fraudsters (devil icons) may create many accounts that all link through common data points (phone, IP, geo), forming <b>suspicious</b> subgraphs that graph-based methods can detect.
  </figcaption>
</figure>

<p>For example, in a bipartite graph of credit card transactions, one set of nodes represent cardholders, another set are merchants, and there is an edge connecting a cardholder to a merchant for each transaction.
Fraudulent cards might cluster via merchant edges (e.g., a fraud ring testing many stolen cards at one merchant), or vice versa.<sup id="fnref:35"><a href="#fn:35" class="footnote" rel="footnote" role="doc-noteref">16</a></sup>
Similarly, for online payments we can create nodes for user accounts, email addresses, IP addresses, device IDs, etc., and connect nodes that are observed together in a transaction or account registration.
This yields a rich heterogeneous graph of entities.</p>

<p><a href="https://en.wikipedia.org/wiki/Graph_neural_network">Graph Neural Networks</a> (GNNs) in recent years has led to many applications of this technology in fraud detection.<sup id="fnref:23"><a href="#fn:23" class="footnote" rel="footnote" role="doc-noteref">17</a></sup> <sup id="fnref:30"><a href="#fn:30" class="footnote" rel="footnote" role="doc-noteref">18</a></sup> <sup id="fnref:31"><a href="#fn:31" class="footnote" rel="footnote" role="doc-noteref">19</a></sup>
GNNs are deep learning models designed for graph-structured data.
They propagate information along edges, allowing each node to aggregate features from its neighbors.
In fraud terms, a GNN can learn to identify suspicious nodes (e.g., users or transactions) by looking at their connected partners.
For instance, if a particular device ID node connects to many user accounts that were later flagged as fraud, a GNN can learn to embed that device node as high-risk, which in turn raises the risk of any new account connected to it.</p>

<aside class="quote">
    <em>“Fraud is rarely a problem of isolated events… fraudsters operate within complex networks.”</em>
</aside>

<p>GNNs consider connections between accounts and transactions to reveal patterns of suspicious activity across the network.
By incorporating relational context, GNNs have demonstrated higher fraud detection accuracy and fewer false positives than models that ignore graph structure.
For example, combining GNNs features with an XGBoost classifier led to catching fraud that would otherwise go undetected and reducing false alarms due to the added network context.
A GNN approach might catch a seemingly normal transaction if the card, device, or IP involved has connections to known frauds that a non-graph model wouldn’t see.</p>

<p>Several types of GNNs architectures have been used.
Notably, <a href="https://paperswithcode.com/method/gcn">Graph Convolutional Networks</a> (GCN), <a href="https://neo4j.com/docs/graph-data-science/current/machine-learning/node-embeddings/graph-sage/">GraphSAGE</a>, heterogeneous GNNs for multi-type node graphs, and even <a href="https://paperswithcode.com/method/graph-transformer">Graph Transformers</a>.</p>

<p>A popular benchmark for GNNs is the <a href="https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.EllipticBitcoinDataset.html">Elliptic dataset</a>, a Bitcoin transaction graph where GNNs have been applied to identify illicit transactions by classifying nodes in a large transaction graph.
GNNs have also been applied to credit card networks: e.g., researchers have built graphs linking credit card numbers, merchants, and phone numbers, and used a heterogeneous GNN to detect fraud cases involving synthetic identities and collusive merchants.<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">20</a></sup></p>

<h2 id="strengths-2">Strengths</h2>

<p>Graph-based methods can detect patterns of collusion and linkage that purely feature-based models miss.
They effectively augment each transaction with context.
Rather than evaluating an event in isolation, the model considers the broader network (device usage graph, money flow graph, etc.).
This is crucial for catching fraud rings.
For example, multiple accounts controlled by one entity or chains of transactions moving funds, which might appear normal individually but are anomalous in aggregate.
GNNs in particular combine the best of both worlds: they leverage graph structure + attribute features together, learning meaningful representations of nodes/edges.<sup id="fnref:3:1"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">20</a></sup>
This is important when fraudsters deliberately make individual transactions look innocuous but cannot hide the relationships (e.g., reusing the same phone or IP address across many accounts).</p>

<p>Another advantage is in reducing false positives by providing context.
For example, a transaction with a new device might normally seem risky, but if that device has a long history with the same user and no links to bad accounts, a graph model can recognize it as low risk, avoiding a false alarm.
Industry reports indicate that adding graph features or GNNs outputs has improved Precision of fraud systems by filtering out cases that looked suspicious in isolation but were safe in context.<sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">21</a></sup></p>

<h2 id="weaknesses-2">Weaknesses</h2>

<aside class="quote">
    <em>“Current GNNs solutions mainly rely on offline batch training and inference, predicting fraudsters in real-time is crucial but challenging.”</em>
</aside>

<p>The biggest challenge is complexity in implementation and deployment.
Building and maintaining the graph data (a.k.a. the “graph pipeline”) is non-trivial.
Transactions arrive in a stream and must update the graph in real-time (e.g., adding new nodes, new edges).
Querying the graph for each new transaction’s neighborhood can be slow if not engineered well.
The inference itself can be heavy.
Running a GNNs means loading a subgraph and doing matrix operations that are costlier than a simple ML model.
Consequently, many current GNNs solutions operate in batch mode (offline).
There are limited reference architectures for real-time GNNs serving, though this is an active development area.</p>

<p>Another issue is scalability.
Graphs of financial transactions or users can be enormous (millions of nodes, tens of millions of edges).
Training a full GNNs on such a graph might not fit in memory or might be extremely slow without sampling techniques.
Some approaches use graph sampling or partitioning to handle this, or only use GNNs to generate features offline.</p>

<p>GNNs can be hard to interpret (even more so than regular deep nets) since the features are aggregate of neighbors.
It can be challenging to explain to an analyst why a certain account was flagged: the reason might be “it’s connected to three other accounts that had chargebacks,” which is somewhat understandable, but the GNN’s learned weights on those connections are not human-interpretable beyond that concept.</p>

<h2 id="real-time-suitability-2">Real-Time Suitability</h2>

<p>Real-time deployment of graph-based models is at the cutting edge.
It is being done in industry but often with approximations.
One pragmatic solution is to use graph analytics to create additional features for a traditional model.
For example, compute features like “number of accounts sharing this card’s IP address that were fraud” or “average fraud score of neighbors” and update these in real-time, then let a gradient boosting model or neural network consume them.
This doesn’t require full GNNs online inference, but captures some graph insights.
However, truly deploying a GNNs in production for each event requires a fast graph database or in-memory graph store.</p>

<p>AWS demonstrated a prototype using Amazon Neptune (graph DB) + DGL (Deep Graph Library) to serve GNNs predictions in real-time by querying a subgraph around the target node for each inference.<sup id="fnref:3:2"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">20</a></sup>
This kind of pipeline can risk score a transaction within seconds, which may be acceptable for certain use cases (e.g., online account opening fraud).
However, for high-frequency card transactions that need sub-second decisions, a full GNNs might still be too slow today unless heavily optimized.</p>

<p>An alternative is what Nvidia suggests: use GNNs offline to produce node embeddings or risk scores, then feed those into a superfast inference system (like an XGBoost model or a rules engine) for the real-time decision.<sup id="fnref:4:1"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">21</a></sup>
This hybrid approach was shown to work at large scale, where GNN-based features improved detection by even a small percent (say 1% AUC gain), which for big banks translates to millions saved.</p>

<p>Lastly, maintaining graph models demands continuous updates as the graph evolves.
This is still manageable, as new data can be incrementally added, but one must watch for concept drift in graph structure.
For example, fraud rings forming new connectivity patterns.</p>

<h2 id="examples-2">Examples</h2>

<p>Representative examples of graph-based fraud detection:</p>

<ul>
  <li><strong>Blockchain networks:</strong> The <a href="https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.EllipticBitcoinDataset.html">Elliptic Bitcoin Dataset</a> is a graph of 203,769 transactions (nodes) with known illicit vs. licit labels. GNNs models on this dataset achieved strong results, showing that analyzing the transaction network is effective for detecting illicit cryptocurrency flows.</li>
  <li><strong>Credit card networks:</strong> Researchers built a graph of credit card transaction and applied a GNNs which outperformed a baseline MLP by leveraging connections (e.g., card linked to a fraudulent merchant gives card a higher fraud probability).</li>
  <li><strong>E-commerce networks:</strong> Companies like Alibaba and PayPal have internal systems modeling user networks. For example, accounts connected via a shared device or IP can indicate <a href="https://en.wikipedia.org/wiki/Sybil_attack">sybil attacks</a> or mule accounts. Graph algorithms identified clusters of accounts that share many attributes (forming fraud communities) which were then taken down as a whole.</li>
  <li><strong>Telecom identity fraud:</strong> Graphs connecting phone numbers, IDs, and addresses have been used to catch identity fraud rings. A famous case is detecting “bust-out fraud” in which a group of credit card accounts all max out and default: the accounts often share phone or address; linking them in a graph helps catch the ring before the bust-out completes.</li>
  <li><strong>Social networks:</strong> In social finance platforms or peer-to-peer payments, graph methods are used to detect money laundering or collusion by analyzing the network of transactions among users (e.g., unusually interconnected payment groups).</li>
</ul>

<p>Overall, graph-based methods, especially GNNs, represent a cutting-edge approach that can significantly enhance fraud detection by considering relational data.
As tooling and infrastructure improve (graph databases, streaming graph processing), I expect to see more real-time GNNs deployments for fraud in the coming years.</p>

<h1 id="transformer-models">Transformer Models</h1>

<h2 id="transformers">Transformers</h2>

<p><a href="https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)">Transformers</a> (originally developed for language processing) have revolutionized many domains, and they are now making inroads in fraud detection.
The key innovation of transformers is the <a href="https://www.ibm.com/think/topics/self-attention">self-attention mechanism</a>, which allows modeling long-range dependencies in sequences.
In the context of transaction data, transformers can analyze transaction sequences or sets of features in flexible ways.</p>

<p>Large pre-trained foundation models (akin to GPT or BERT, but for payments) are emerging.
In this case, a model is pre-trained on massive amounts of transaction data to learn general patterns, then fine-tuned for specific fraud tasks.
So that these models can “speak” transactional data.</p>

<blockquote>
  <p>“One of the most notable recent developments comes from Stripe’s <a href="https://www.linkedin.com/posts/gautam-kedia-8a275730_tldr-we-built-a-transformer-based-payments-activity-7325973745292980224-vCPR/">transformer-based payments foundation model.</a> <!-- markdownlint-disable-line MD033 -->
This is a large-scale self-supervised model trained on tens of billions of transactions to create embeddings of each transaction.
The idea is analogous to how LLMs work: to learn a high-dimensional embedding for a transaction that captures its essential characteristics and context.
Transactions with similar patterns end up with similar embeddings, e.g., transactions from the same bank or the same email domain cluster together in embedding space.
These embeddings serve as a universal representation that can be used for various tasks: fraud detection, risk scoring, identifying businesses in trouble, etc.
For the fraud use-case, Stripe reports a dramatic improvement: by feeding sequences of these transaction embeddings into a downstream classifier, they achieved an increase in detection rate for certain fraud attacks from 59% to 97% overnight.
In particular, they targeted “card testing fraud” (i.e., fraudsters testing stolen card credentials with small purchases), something that often hides in high-volume data.
The transformer foundation model was able to spot subtle sequential patterns of card testing that previous feature-engineered models missed, blocking attacks in real-time before they could do damage.”</p>
</blockquote>

<p>Researchers have applied Transformer encoders to tabular data as well.<sup id="fnref:18"><a href="#fn:18" class="footnote" rel="footnote" role="doc-noteref">22</a></sup>
For example, using models like <a href="https://github.com/lucidrains/tab-transformer-pytorch">TabTransformer</a> or integration of transformers with structured data.
They reported improved accuracy over MLPs and even over tree models in some cases.<sup id="fnref:26"><a href="#fn:26" class="footnote" rel="footnote" role="doc-noteref">23</a></sup></p>

<p>The ability of transformers to focus attention on important features or interactions could be beneficial for high-dimensional transaction data.
For example, a transformer might learn to put high attention on the <code class="language-plaintext highlighter-rouge">device_id</code> feature when the <code class="language-plaintext highlighter-rouge">ip_address_country</code> is different from the <code class="language-plaintext highlighter-rouge">billing country</code>, effectively learning a rule-like interaction that would be hard for a linear model.</p>

<p>Transformers can also model cross-item sequences: one can feed a sequence of past transactions as a “sentence” into a transformer, where each transaction is like a token embedding (comprising attributes like amount, merchant category, etc.).
The transformer can then output a representation of the sequence or of the next transaction’s risk.
This is similar to an RNN’s use but with the advantage of attention capturing long-range dependencies (e.g., a pattern that repeats after 20 transactions).
There have been experiments where a transformer outperformed LSTM on fraud sequence classification, due to its parallel processing and ability to consider all transactions’ relations at once.</p>

<p>Another angle is using transformer models for entity resolution and representation in fraud. For instance, a transformer can be trained on the corpus of all descriptions or merchant names that a user has transacted with, thereby learning a “profile” of the user’s spending habits and detecting an out-of-profile transaction (similar to how language models detect an odd word in a sentence).
Additionally, <a href="https://en.wikipedia.org/wiki/BERT_(language_model)">BERT</a>-like models can be used on event logs or customer support chats to detect social engineering fraud attempts, though that’s adjacent to transaction fraud.</p>

<h2 id="foundation-models">Foundation models</h2>

<p><a href="https://en.wikipedia.org/wiki/Foundation_model">Foundation models</a> in fraud detection refer to large models trained on broad data that can then be adapted.
Besides Stripe’s payments’ model, other financial institutions are likely developing similar pre-trained embeddings.
For example, a consortium of banks could train a model on pooled transaction data (in a privacy-preserving way, or via <a href="https://en.wikipedia.org/wiki/Federated_learning">federated learning</a>) to get universal fraud features.</p>

<p>These large models may use transformers or other architectures, but the common theme is self-supervised learning: e.g., predicting a masked field of a transaction (<code class="language-plaintext highlighter-rouge">merchant_category</code>, or <code class="language-plaintext highlighter-rouge">amount</code>) from other fields, or predicting the next transaction given past ones.
Through such tasks, the model gains a deep understanding of normal transactional patterns.
When fine-tuned on a specific fraud dataset, it starts with a rich feature space and thus performs better with less training data than a model from scratch.
This is analogous to how image models pre-trained on <a href="https://www.image-net.org/">ImageNet</a> are fine-tuned for medical images with small datasets.</p>

<h2 id="strengths-3">Strengths</h2>

<p>Transformers and foundation models bring state-of-the-art pattern recognition to fraud.
They particularly shine in capturing complex interactions and sequential/temporal patterns.
The attention mechanism allows the model to focus on the most relevant parts of the input for each decision.
For fraud, this could mean focusing on certain past transactions or specific features that are indicative of risk in context.
This yields high detection performance, especially for “hard fraud” that evades simpler models.</p>

<p>Another strength is multitasking capabilities.
A large foundation model can be trained once and then used for various related tasks such as fraud, credit risk, or marketing predictions simply by fine-tuning or prompting, rather than maintaining separate models for each.
This “one model, many tasks” approach can simplify the system and leverage cross-task learning (e.g., learning what a risky transaction looks like might also help predict chargebacks or customer churn).</p>

<p>Moreover, transformers can handle heterogeneous data relatively easily.
One can concatenate different feature types and the self-attention will figure out which parts to emphasize.
For example, Stripe’s model encodes each transaction as a dense vector capturing numeric fields, categorical fields, etc., all in one embedding.</p>

<p>Finally, foundation models can enable few-shot or zero-shot fraud detection.
Imagine detecting a new fraud pattern that wasn’t in the training data.
A pre-trained model that has generally learned “how transactions usually look” might pick up the anomaly better than a model trained only on past known frauds.</p>

<h2 id="weaknesses-3">Weaknesses</h2>

<p>The obvious downsides are resource intensity and complexity.
Training a transformer on billions of transactions is a monumental effort, requiring distributed training, specialized hardware (TPUs/GPUs), and careful tuning.
This is typically only within reach of large organizations or collaborations.
In production, serving a large transformer in real-time can be challenging due to model size and latency.
Transformers can have millions of parameters, and even if each inference is 50-100ms on a GPU, at very high transaction volumes (thousands per second) this could be costly or slow without scaling out.
Techniques like <a href="https://huggingface.co/docs/optimum/en/concept_guides/quantization">model quantization</a>, <a href="https://www.ibm.com/think/topics/knowledge-distillation">knowledge distillation</a>, or efficient transformer variants (e.g., <a href="https://huggingface.co/papers/2403.20041">Transformer Lite</a>) might be needed.</p>

<p>Another concern is explainability.
Even more so than a standard deep network, a giant foundation model is a black box.
Understanding its decisions requires advanced explainable AI methods, like interpreting attention weights or using SHAP on the embedding features, which is an active research area.
For regulated industries, one might still use a simpler surrogate model to justify decisions externally, while the transformer works under the hood.</p>

<p>Overfitting and concept drift are also concerns.
A foundation model might capture a lot of patterns, including some that are spurious or not causally related to fraud.
If fraudsters adapt, the model might need periodic re-training or fine-tuning with fresh data to unlearn outdated correlations.
For example, the Stripe model is self-supervised (no fraud labels in pre-training) which helps it generalize, but any discriminative fine-tuning on fraud labels will still need updating as fraud evolves.</p>

<h2 id="real-time-suitability-3">Real-Time Suitability</h2>

<p>Surprisingly, with the right engineering, even large transformers can be used in or near real-time.
For example, optimizing the embedding generation via GPU inference or caching mechanisms.
One strategy is to pre-compute embeddings for entities (like a card or user) so that only incremental computation is needed per new transaction.
Another strategy is two-stage scoring: use a smaller model to thin out events, then apply the heavy model to the most suspicious subset.
If real-time means sub-second (say &lt;500ms), a moderately sized transformer model on modern inference servers can fit that window, especially if batch processing a few transactions together to amortize overhead.
Cloud providers also offer accelerated inference endpoints (like AWS Inferentia chips or Azure’s ONNX runtime with GPU) to deploy large models with low latency.</p>

<p>That said, not every company will want to deploy a 100M+ parameter model for each transaction if a simpler model would do.
There is a trade-off between maximum accuracy and infrastructure cost/complexity.
In many cases, a foundation model could be used to periodically score accounts offline (to detect emerging fraud rings) and a simpler online model handle immediate decisions, combining their outputs.</p>

<h2 id="examples-3">Examples</h2>

<p>Use cases and research for transformers in fraud:</p>

<ul>
  <li><strong>Stripe’s Payments Foundation Model:</strong> A transformer-based model trained on billions of transactions, now used to embed transactions and feed into Stripe’s real-time fraud systems. It improved certain fraud detection rates from 59% to 97% and enabled detection of subtle sequential fraud patterns that were previously missed.</li>
  <li><strong>Tabular transformers:</strong> Studies like Chang et al.<sup id="fnref:18:1"><a href="#fn:18" class="footnote" rel="footnote" role="doc-noteref">22</a></sup>  applied a transformer to the Kaggle credit card dataset and compared it to SVM, Random Forest, XGBoost, etc. The transformer achieved comparable or superior Precision/Recall, demonstrating that even on tabular data a transformer can learn effectively.</li>
  <li><strong>Sequence anomaly detection:</strong> Some works use transformers to model time series of transactions per account. A transformer may be trained to predict the next transaction features; if the actual next transaction diverges significantly, it could flag an anomaly. This is analogous to language model use (predict next word).</li>
  <li><strong>Cross-entity sequence modeling:</strong> Transformers can also encode sequences of transactions across entities, e.g., tracing a chain of transactions through intermediary accounts (useful in money laundering detection). The recent FraudGT model<sup id="fnref:27"><a href="#fn:27" class="footnote" rel="footnote" role="doc-noteref">24</a></sup> combines ideas of GNNs and transformer to handle transaction graphs with sequential relations.</li>
  <li><strong>Foundation models for documents and text in fraud:</strong> While not the focus here, note that transformers (BERT, GPT) are heavily used to detect fraud in textual data (e.g., scam emails, fraudulent insurance claims text, etc). In a holistic fraud system, a foundation model might take into account not just the structured transaction info but also any unstructured data, like customer input or messages, to make a decision.</li>
</ul>

<p>Transformer-based models and foundation models represent the frontier of fraud detection modeling.
They offer unparalleled modeling capacity and flexibility, at the cost of high complexity.
Early results, especially from industry leaders, indicate they can substantially raise the bar on fraud detection performance when deployed thoughtfully.
As these models become more accessible (with open-source frameworks and possibly smaller specialized versions), more fraud teams will likely adopt them, particularly for large-scale, multi-faceted fraud problems where simpler models hit a ceiling.</p>

<h1 id="appendix">Appendix</h1>

<h2 id="public-datasets">Public Datasets</h2>

<p>Research in fraud detection often relies on a few key <strong>public datasets</strong> to evaluate models, given that real financial data is usually proprietary.</p>

<p>Below I summarize some of the most commonly used datasets, along with their characteristics:</p>

<ul>
  <li>
    <p><a href="https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud">:globe_with_meridians: Credit Card Fraud Detection (Kaggle, 2013)</a>: A classic dataset containing real European credit card transactions over two days. Its key characteristics are its extreme class imbalance (0.172% fraud) and anonymized features (28 PCA components), making it a standard benchmark for testing algorithms on imbalanced data.</p>
  </li>
  <li>
    <p><a href="https://www.kaggle.com/c/ieee-fraud-detection">:globe_with_meridians: IEEE-CIS Fraud Detection (Kaggle, 2019)</a>: A large, rich dataset from an e-commerce provider, released for a Kaggle competition. It features ~300 raw features (device info, card details, etc.), missing values, and a moderate imbalance (3.5% fraud). It is ideal for evaluating complex feature engineering and ensemble models for card-not-present fraud.</p>
  </li>
  <li>
    <p><a href="https://www.kaggle.com/datasets/ealaxi/paysim1">:globe_with_meridians: PaySim (Kaggle, 2016)</a>: A large-scale synthetic dataset that simulates mobile money transactions. It contains over 6 million transactions and is useful for testing model scalability in a controlled environment. Because it is synthetic, models may achieve unrealistically high performance.</p>
  </li>
  <li>
    <p><a href="https://www.kaggle.com/datasets/ellipticco/elliptic-data-set">:globe_with_meridians: Elliptic Bitcoin Dataset (Kaggle, 2019)</a>: A temporal graph of over 200,000 Bitcoin transactions, where nodes are transactions and edges represent fund flows. It is a key benchmark for evaluating graph-based fraud detection methods like GNNs. Only a small fraction of nodes are labeled as illicit, presenting a challenge.</p>
  </li>
</ul>

<p>⚠️ Due to high imbalance, accuracy is not informative (e.g., the credit card dataset has 99.8% non-fraud, so a trivial model gets 99.8% accuracy by predicting all non-fraud!). Hence, papers report metrics like AUC-ROC, Precision/Recall, or F1-score. For instance, on the Kaggle credit card data, an AUC-ROC around 0.95+ is achievable by top models, and PR AUC is much lower (since base fraud rate is 0.172%). In IEEE-CIS data, top models achieved about 0.92–0.94 AUC-ROC in the competition. PaySim being synthetic often yields extremely high AUC (sometimes &gt;0.99 for simple models) since patterns might be easier to learn. When evaluating on these sets, it’s crucial to use proper cross-validation or the given train/test splits to avoid overfitting (particularly an issue with the small Kaggle credit card data).</p>

<p>Overall, these datasets have driven a lot of research.
However, one should be cautious when extrapolating results from them to real-world performance.
Real production data can be more complex (concept drift, additional features, feedback loops).
Nonetheless, the above datasets provide valuable benchmarks to compare algorithms under controlled conditions.</p>

<h1 id="external-resources">External Resources</h1>

<ul>
  <li><a href="https://github.com/safe-graph/graph-fraud-detection-papers/"><i class="fab fa-github"></i></a> <a href="https://github.com/safe-graph/graph-fraud-detection-papers/">Awesome Graph Fraud Detection Papers</a></li>
  <li><a href="https://github.com/safe-graph/DGFraud"><i class="fab fa-github"></i></a> <a href="https://github.com/safe-graph/DGFraud">DGFraud: A Deep Graph-based Toolbox for Fraud Detection</a></li>
  <li><a href="https://github.com/junhongmit/FraudGT"><i class="fab fa-github"></i></a> <a href="https://github.com/junhongmit/FraudGT">FraudGT: A Simple, Effective, and Efficient Graph Transformer for Financial Fraud Detection</a></li>
</ul>

<h1 id="footnotes">Footnotes</h1>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p>Oztas, Berkan, et al. “<em><a href="https://www.sciencedirect.com/science/article/pii/S0167739X24002607">Transaction monitoring in anti-money laundering: A qualitative analysis and points of view from industry.</a></em>” Future Generation Computer Systems (2024). <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>G. Praspaliauskas, V. Raman (2023). <em>“<a href="https://aws.amazon.com/blogs/machine-learning/real-time-fraud-detection-using-aws-serverless-and-machine-learning-services/">Real-time fraud detection using AWS serverless and machine learning services</a>.</em> AWS Machine Learning Blog – outlines a serverless architecture using Amazon Kinesis, Lambda, and Amazon Fraud Detector for near-real-time fraud prevention.” <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:34">
      <p>Desai, Ajit, Anneke Kosse, and Jacob Sharples. “<em><a href="https://www.sciencedirect.com/science/article/pii/S2405918825000157">Finding a needle in a haystack: a machine learning framework for anomaly detection in payment systems.</a>.</em>” The Journal of Finance and Data Science 11 (2025): 100163. <a href="#fnref:34" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:28">
      <p>R-ring collusion is a form of coordinated behavior where multiple accounts, potentially belonging to different individuals or groups, engage in fraudulent activities that benefit each other. <a href="#fnref:28" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:16">
      <p>For a Python library dedicated to handling imbalanced datasets and techniques, see <em><a href="https://imbalanced-learn.org/stable/">imbalanced-learn</a></em>, which provides tools for oversampling, undersampling, and more. <a href="#fnref:16" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:29">
      <p>Service Level Agreement (SLA) is a commitment between a service provider and a client that outlines the expected level of service, including performance metrics and response times. <a href="#fnref:29" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:17">
      <p>Afriyie, Jonathan Kwaku, et al. <em>“<a href="https://doi.org/10.1016/j.dajour.2023.100163">A supervised machine learning algorithm for detecting and predicting fraud in credit card transactions.</a></em> Decision Analytics Journal 6 (2023): 100163.” <a href="#fnref:17" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:19">
      <p>Onyeoma, Chidinma Faith, et al. “<em><a href="https://ieeexplore.ieee.org/abstract/document/10838456">Credit Card Fraud Detection Using Deep Neural Network with Shapley Additive Explanations</a>.</em>” 2024 International Conference on Frontiers of Information Technology (FIT). IEEE, 2024. <a href="#fnref:19" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:13">
      <p>Kandi, Kianeh, and Antonio García-Dopico. “<em><a href="https://www.mdpi.com/2504-4990/7/1/20">Enhancing Performance of Credit Card Model by Utilizing LSTM Networks and XGBoost Algorithms.</a></em>” Machine Learning and Knowledge Extraction 7.1 (2025): 20. <a href="#fnref:13" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:13:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:33">
      <p>Cherif, Asma, et al. “<em><a href="https://www.sciencedirect.com/science/article/pii/S1319157824000922">Encoder–decoder graph neural network for credit card fraud detection.</a></em>” Journal of King Saud University-Computer and Information Sciences 36.3 (2024). <a href="#fnref:33" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:21">
      <p>Alshameri, Faleh, and Ran Xia. “<em><a href="https://www.sciopen.com/article/10.26599/BDMA.2023.9020035">An Evaluation of Variational Autoencoder in Credit Card Anomaly Detection</a>.</em>” Big Data Mining and Analytics (2024). <a href="#fnref:21" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:22">
      <p>Charitou, Charitos, Artur d’Avila Garcez, and Simo Dragicevic. “<em><a href="https://ieeexplore.ieee.org/document/9206844">Semi-supervised GANs for fraud detection</a>.</em>” 2020 International Joint Conference on Neural Networks (IJCNN). IEEE, 2020. <a href="#fnref:22" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:32">
      <p>Huang, Huajie, et al. “<em><a href="https://www.sciencedirect.com/science/article/abs/pii/S156849462400142X">Imbalanced credit card fraud detection data: A solution based on hybrid neural network and clustering-based undersampling technique.</a></em>” Applied Soft Computing 154 (2024). <a href="#fnref:32" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:24">
      <p>Branco, Bernardo, et al. “<em><a href="https://doi.org/10.1145/3394486.3403361">Interleaved sequence RNNs for fraud detection</a>.</em>” Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining. 2020. <a href="#fnref:24" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:25">
      <p>Masihullah, Shaik, et al. “<em><a href="https://link.springer.com/chapter/10.1007/978-3-031-14463-9_10">Identifying fraud rings using domain aware weighted community detection</a>.</em>” International Cross-Domain Conference for Machine Learning and Knowledge Extraction. Cham: Springer International Publishing, 2022. <a href="#fnref:25" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:35">
      <p>Boyapati, Mallika, and Ramazan Aygun. “<em><a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608024008554">BalancerGNN: Balancer Graph Neural Networks for imbalanced datasets: A case study on fraud detection.</a>.</em>” Neural Networks 182 (2025): 106926. <a href="#fnref:35" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:23">
      <p>Motie, Soroor, and Bijan Raahemi. “<em><a href="https://doi.org/10.1016/j.eswa.2023.122156">Financial fraud detection using graph neural networks: A systematic review</a>.</em>” Expert Systems with Applications (2024). <a href="#fnref:23" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:30">
      <p>Shih, Yi-Cheng, et al. “<em><a href="https://www.sciencedirect.com/science/article/abs/pii/S0957417424020785">Fund transfer fraud detection: Analyzing irregular transactions and customer relationships with self-attention and graph neural networks.</a></em>” Expert Systems with Applications. 2025. <a href="#fnref:30" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:31">
      <p>Tong, Guoxiang, and Jieyu Shen. “<em><a href="https://www.sciencedirect.com/science/article/abs/pii/S1568494623010025">Financial transaction fraud detector based on imbalance learning and graph neural network.</a></em>” Applied Soft Computing 149 (2023): 110984. <a href="#fnref:31" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>Jian Zhang et al. (2022). <em>“<a href="https://aws.amazon.com/blogs/machine-learning/build-a-gnn-based-real-time-fraud-detection-solution-using-amazon-sagemaker-amazon-neptune-and-the-deep-graph-library/">Build a GNN-based real-time fraud detection solution using Amazon SageMaker, Amazon Neptune, and DGL</a>.</em> AWS ML Blog – explains how graph neural networks can be served in real-time for fraud detection, noting challenges in moving from batch to real-time GNN inference.” <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:3:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:3:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p>
    </li>
    <li id="fn:4">
      <p>Summer Liu et al. (2024). <em>“<a href="https://developer.nvidia.com/blog/supercharging-fraud-detection-in-financial-services-with-graph-neural-networks/">Supercharging Fraud Detection in Financial Services with GNNs</a>.</em> NVIDIA Technical Blog.” <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:4:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:18">
      <p>Yu, Chang, et al. “<em><a href="https://arxiv.org/pdf/2406.03733v2">Credit Card Fraud Detection Using Advanced Transformer Model</a>.</em>” 2024 IEEE International Conference on Metaverse Computing, Networking, and Applications (MetaCom). IEEE, 2024. <a href="#fnref:18" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:18:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:26">
      <p>Krutikov, Sergei, et al. “<em><a href="https://arxiv.org/html/2405.13692v1">Challenging Gradient Boosted Decision Trees with Tabular Transformers for Fraud Detection at Booking.com</a>.</em>” arXiv preprint arXiv:2405.13692 (2024). <a href="#fnref:26" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:27">
      <p>Lin, Junhong, et al. “<em><a href="https://dl.acm.org/doi/abs/10.1145/3677052.3698648">FraudGT: A Simple, Effective, and Efficient Graph Transformer for Financial Fraud Detection</a>.</em>” Proceedings of the 5th ACM International Conference on AI in Finance. 2024. <a href="#fnref:27" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>César Soto Valero</name><email>cesarsotovalero@gmail.com</email></author><category term="ai" /><summary type="html"><![CDATA[Financial transaction fraud is a pervasive problem costing institutions and customers billions annually. This survey reviews the current state-of-the-art in real-time transaction fraud detection, spanning both academic research and industry adopted solutions.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.cesarsotovalero.net/img/posts/2025/2025-04-03/justitiabrunnen_cover.jpg" /><media:content medium="image" url="https://www.cesarsotovalero.net/img/posts/2025/2025-04-03/justitiabrunnen_cover.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Building GenAI Applications Today</title><link href="https://www.cesarsotovalero.net/blog/building-genai-applications-today.html" rel="alternate" type="text/html" title="Building GenAI Applications Today" /><published>2024-11-17T00:00:00-08:00</published><updated>2025-07-29T11:52:25-07:00</updated><id>https://www.cesarsotovalero.net/blog/building-genai-applications-today</id><content type="html" xml:base="https://www.cesarsotovalero.net/blog/building-genai-applications-today.html"><![CDATA[<figure class="badge"><a href="&lt;https://commons.wikimedia.org/wiki/File:NMA.0039209_Emigration._Svenskar_i_Amerika._Guldvaskare_vid_Black_Foots_River,_Montana.jpg&gt;"><img src="/img/badges/Svenskar_i_Amerika._Guldvaskare_vid_Black_Foots_River,_Montana.jpg" style="width:140px;max-width:100%;" alt="badge" /></a></figure>

<p>The AI fever has been around for a while now (4 years as I can count).
It reminds me of the <a href="https://en.wikipedia.org/wiki/Gold_rush">gold rush</a> and subsequent <a href="https://en.wikipedia.org/wiki/California_Dream">Californian Dream</a> from the 19th century.
The new “happy idea” is that today any single individual could get rich almost instantly by leveraging the novel AI’s-based capabilities the right way.
The potential opportunities to apply Generative AI (GenAI) for profit span almost all areas of development, from pure arts to fundamental science, from medical diagnosis to engineering, and so on.
<a href="https://en.wikipedia.org/wiki/Intelligent_agent">AI agents</a> can now <a href="https://www.sciencedirect.com/science/article/pii/S2352847823001557">generate</a> new scientific hypotheses.
Yet, as history seems to repeat itself, every time OpenAI releases a new model offering more powerful capabilities, many solo entrepreneurs and small startups flounder and fade.
For instance, I’ve attended many hackathons where teams have tried to build AI-powered solutions for problems that could be solved with a simple rule-based system.
There are many many reasons why GenAI projects fail, and less clear reasons (to me) of why some succeed.
So, if you’re a developer or entrepreneur itching to dive into the GenAI space for fun or profit, this post is my step back to reflect on what (I believe) are good fits for this technology and what <em>shouldn’t</em> be built with it.
Let’s dive in!</p>

<figure class="jb_picture">
  



<img width="100%" style="border: 0px solid #808080;" src="/assets/resized/twisted-revolver-640x373.jpg" alt="Time to stop shutting flies with AI powered bullets" data-srcset="/assets/resized/twisted-revolver-640x373.jpg 640w,/assets/resized/twisted-revolver-768x448.jpg 768w,/assets/resized/twisted-revolver-1024x597.jpg 1024w,/assets/resized/twisted-revolver-1366x797.jpg 1366w,/assets/resized/twisted-revolver-1600x933.jpg 1600w," class="blur-up lazyautosizes lazyload" />
  <figcaption class="stroke">
   &#169; Stop shutting flies with AI-powered bullets, sometimes just use flyswatter instead. Picture of a sculpture located in <a href="https://maps.app.goo.gl/9tWekqJTscsLmkgd9">Hötorgshallen 50</a>, in Stockholm city.
  </figcaption>
</figure>

<h1 id="what-seems-to-work">What Seems to Work</h1>

<p>I’m not an entrepreneur myself (yet), but I’m overall an enthusiast of the startup ecosystem. I’ve listened to the <a href="https://www.indiehackers.com/podcasts">Indie Hackers Podcasts</a>, read <a href="https://techcrunch.com/">TechCrunch</a>, and still check cool launches on <a href="https://gumroad.com">Gumroad</a> from time to time. Over the years, I’ve seen many startups succeed and fail along the way for diverse reasons. Some of them failed due to overhyped tech, unrealistic business models, or poor timing (e.g., Pets.com, Theranos, or Rdio).</p>

<p>Obviously, whenever there is a new technology, there is a wave of startups trying to profit from it.
Do you remember the blockchain hype 10 years ago?
I can’t forget about the many crypto millionaires that emerged from it (especially because I wasn’t one of them).</p>

<p>I’m now convinced that GenAI is a powerful tech (much broader than blockchain) that can <a href="https://www.cesarsotovalero.net/blog/surviving-the-ai-revolution-as-a-software-engineer.html">transform whole industries</a> and create new new ones.
GenAI will first optimize existing processes and then transform them.
First, everything related to handling paperwork and repetitive tasks is going to be completely automated (and that’s actually good because these boring tasks were not meant for humans in the first place anyway).
Second, GenAI creates new opportunities for everyone, including artists and creators, to express themselves in ways that were not possible before.
Third, GenAI helps us understand the world better by analyzing data in an unprecedented manner, generating new hypotheses, and planning new experiments, thus propelling the scientific discoveries that ultimately transform our lives for the better.</p>

<h2 id="success-factors">Success Factors</h2>

<p>The success of any GenAI project is extremelly hard to predict.
However, the ones that succeed share some common traits that set them apart from the noise.
Below are factors to keep in mind when evaluating whether an AI idea is worth your time and resources.</p>

<p><strong>Clear Value Proposition.</strong>
Before jumping into the development of any AI system, ask yourself: Does this app solve a real-world problem?
And is it doing so in a way that’s better, faster, or cheaper than existing solutions?
The best AI ideas are those that tackle well-defined issues, making people’s lives easier or helping businesses streamline operations.
Without a compelling value proposition, even the most advanced AI will struggle to gain traction.</p>

<p><strong>Scalability.</strong>
While niche markets can be tempting, they are often not the best starting point for an AI system unless you have a very clear, long-term vision.
Aim for ideas that can scale, reaching wider audiences or applying across various industries.
Scalability isn’t just about expanding your user base.
It’s about adapting the technology to different use cases, which will increase the likelihood of long-term success.</p>

<p><strong>Ethical Design.</strong>
Every AI system comes with a responsibility: to design it with ethics in mind. Consider the potential negative uses of your AI. While technologies like deepfake generators show immense creative potential, they also pose significant risks for misuse. It’s essential to build safeguards and establish ethical boundaries to prevent your AI from being used for harmful purposes.</p>

<h2 id="examples">Examples</h2>

<p>Below are three applications of GenAI today that I would like to see more of:</p>

<ol>
  <li>
    <p><strong>Personalized Financial Advisors:</strong> Apps that analyze spending habits, investment opportunities, and financial health using AI are gold. These tools cater to the rising demand for financial literacy and can scale personalized advice to millions (i.e., tangible ROI for users). For example, <a href="https://www.wealthfront.com/">Wealthfront</a>’s AI-driven financial planning features are transforming personal finance management.</p>
  </li>
  <li>
    <p><strong>Content Creation Tools:</strong> GenAI is a boon for creators. Platforms that assist with scriptwriting, graphic design, or even video editing are gaining more and more traction because they save time, amplify human creativity, and have clear market demand. For example, <a href="https://www.adobe.com/products/firefly.html">Adobe’s Firefly</a> enhances creativity by automating repetitive tasks.</p>
  </li>
  <li>
    <p><strong>AI in Healthcare:</strong> There’s a growing emphasis on preventive health solutions that are scalable and cost-effective. Tools like AI-powered symptom checkers or personalized fitness coaches empower users to manage their health better. For example, <a href="https://www.myfitnesspal.com/">MyFitnessPal</a> leverages AI for smarter diet recommendations.</p>
  </li>
</ol>

<p>While we are still in the early days of GenAI, it seems to me that what works the best is still to focus on solving real world problems in a scalable way.
If you’ve worked on a GenAI product, then you don’t need to be remembered that no matter the tech, is <em>the product</em> what matters the most.</p>

<h1 id="what-to-avoid">What to Avoid</h1>

<p>Most GenAI ideas are not worth pursuing.
Numerous AI-driven apps fail due to over-saturation and lack of resources to compete against the tech giants that already have customer trust and a large user base.
Other fail due to technical glitches, poor product pivots, or simply because they don’t solve a real problem.
There is another set that fail because of premature obsolescense (e.g., a friend built a now dead startup 4 years ago about simplifying email through summarization).
Failure is hard to predict when one truly believes in a proyect.</p>

<h2 id="lessons-from-ai-failures">Lessons from AI Failures</h2>

<p>Building a successful AI system isn’t just about using the best foundational models and feeding them with data.
It’s about learning from the mistakes of others.
A few notable failures provide crucial insights into what can go wrong, and what to avoid in the development process.
Below is a small compilation.</p>

<p><strong>Bias Is The Biggest Enemy.</strong>
One of the most dangerous pitfalls in AI is <a href="https://en.wikipedia.org/wiki/Algorithmic_bias">bias</a>, which can unintentionally emerge through data or algorithmic design.
GenAI systems that perpetuate bias can quickly damage a company’s reputation, especially in sensitive industries.
A classic example of this is <a href="https://en.wikipedia.org/wiki/Tay_(chatbot)">Microsoft’s Tay chatbot</a>, which was launched in 2016 to interact with users on Twitter.
Unfortunately, it was quickly hijacked by biased and offensive content due to insufficient safeguards.
Current AI systems must be designed with bias mitigation in mind, ensuring that they are fair and ethical.</p>

<p><strong>Privacy Is Non-Negotiable.</strong>
When it comes to AI, privacy is not a feature, it’s a fundamental requirement.
Especially in sectors like healthcare and finance, mishandling sensitive user data can lead to significant regulatory penalties and, perhaps worse, the loss of customer trust.
Companies must prioritize data protection, making sure that systems are secure and that users’ privacy is never compromised.
This includes adhering to global privacy standards like <a href="https://en.wikipedia.org/wiki/General_Data_Protection_Regulation">GDPR</a> or <a href="https://oag.ca.gov/privacy/ccpa/regs">CCPA</a>, and being transparent about how user data is collected and used.</p>

<p><strong>Overpromising.</strong>
While the potential of AI can be captivating, it’s essential to remain grounded and transparent about the technology’s current capabilities.
Many companies have fallen into the trap of hyping up their AI solutions, only to disappoint users with results that fall short of expectations.
For example, <a href="https://www.businessinsider.com/healthcare-startup-forward-shutdown-carepod-adrian-aoun-2024-11">Forward Health</a> promised a futuristic healthcare experience with its CarePods but failed to deliver on its ambitious vision.
To avoid overpromising, companies should set realistic goals, communicate openly with users, and focus on incremental improvements rather than grandiose claims.</p>

<p>In addition to the primary lessons above, there are a few more pitfalls that commonly arise in AI development.
These may not always be immediately obvious but are equally critical in creating a successful AI application.</p>

<p><strong>Use GenAI When You Don’t Need It.</strong>
GenAI is incredibly powerful, but it’s not a one-size-fits-all solution. As the saying goes, “Not everything is a nail.” Sometimes, simpler solutions like linear programming or basic algorithms can solve the problem more efficiently and cost-effectively. For example, optimizing energy consumption with a basic schedule based on electricity prices is far more effective and cheaper than running the same data through a complex language model (as noted by AI expert Chip Huyen). Before opting for GenAI, carefully consider if it’s the best approach.</p>

<p><strong>Confusing Bad Product with Bad AI.</strong>
A common misconception is that poor user experiences or ineffective AI solutions are the result of faulty algorithms. In reality, many issues arise from poor product design or a lack of attention to the user interface (UI).
For example, a chatbot might function perfectly but fail to engage users simply because they don’t know how to interact with it.
A well-designed UX can transform even a mediocre AI into something genuinely useful.
<a href="https://www.intuit.com/intuitassist/">Intuit’s chatbot</a>, for instance, was able to enhance the overall experience through smart design choices, demonstrating that good design can elevate AI performance.</p>

<p><strong>Lack of Model Customization.</strong>
While pre-trained open-source models offer a quick and easy starting point, relying on them without tailoring them to your specific use case is a mistake.
Using these models without fine-tuning them is akin to trying to run a marathon in flip-flops.
It might work to some extent, but you’re not going to achieve optimal results.
Customizing and fine-tuning models allows them to meet the specific needs of your application, making the difference between an average app and one that delivers real value to users.</p>

<h2 id="examples-1">Examples</h2>

<p>Here are four GenAI development directions that I would advise against pursuing:</p>

<ol>
  <li>
    <p><strong>Overhyped General-Purpose Chatbots:</strong> Chatbots are ubiquitous, and most fail to differentiate themselves. Unless your bot is solving a specific problem better than existing solutions, it’s just another chatbot. For example, <a href="https://www.analyticsvidhya.com/blog/2023/01/top-5-failures-of-ai-till-date-reasons-solution/">Microsoft’s Tay chatbot</a> famously spiraled out of control due to poor moderation and biased training data.</p>
  </li>
  <li>
    <p><strong>Poorly Thought-Out Healthcare Applications:</strong> While healthcare is promising, it’s also highly regulated. Products that don’t comply with data protection laws or fail to address ethical concerns will face backlash. For example, <a href="https://www.businessinsider.com/healthcare-startup-forward-shutdown-carepod-adrian-aoun-2024-11">Forward Health’s CarePods</a> failed due to technical glitches and poor location choices.</p>
  </li>
  <li>
    <p><strong>Tools Targeting Hyper-Niche Markets:</strong> Niche markets often lack the scale needed to make an app profitable. AI tools for ultra-specific tasks, like “AI for knitting pattern generation,” may not justify the investment. For example, <a href="https://selldone.com/blog/major-startup-failures-2024-824">Tally’s credit management platform</a> collapsed due to limited scalability and poor product pivots.</p>
  </li>
</ol>

<p>In summary, avoid building AI apps that don’t solve a real problem, are overhyped, or target markets that are too small to scale.</p>

<h1 id="final-thoughts">Final Thoughts</h1>

<p>The AI gold rush is far from over. Many GenAI startups are rushing headfirst into this space, hoping to strike gold, but failing to dig deep enough to find the right nuggets. In the future, the winners will be those who embrace simplicity, scale, and ethics, while staying grounded in real-world needs. To make AI a lasting success, focus on creating products that truly solve problems for the user, rather than just jumping on the bandwagon. Who knows? Maybe your next AI project will end up as the one everyone’s talking about. Or maybe it will be yet another “failed startup” story.</p>

<p>You will never know until you try.</p>

<h1 id="external-resources">External Resources</h1>

<ul>
  <li><a href="https://www.analyticsvidhya.com/blog/2023/01/top-5-failures-of-ai-till-date-reasons-solution/">:globe_with_meridians: Analytics Vidhya: Top 5 AI Failures</a></li>
  <li><a href="https://www.businessinsider.com/healthcare-startup-forward-shutdown-carepod-adrian-aoun-2024-11">:globe_with_meridians: Business Insider: Inside Forward’s Failure</a></li>
  <li><a href="https://selldone.com/blog/major-startup-failures-2024-824">:globe_with_meridians: Selldone: Major Startup Failures 2024</a></li>
  <li><a href="https://huyenchip.com/2025/01/16/ai-engineering-pitfalls.html">:globe_with_meridians: Chip Huyen: Common pitfalls when building generative AI applications</a></li>
  <li><a href="https://huyenchip.com/llama-police">:globe_with_meridians: Chip Huyen: List of Open Source LLM Tools</a></li>
  <li><a href="https://cloud.google.com/transform/101-real-world-generative-ai-use-cases-from-industry-leaders">:globe_with_meridians: 101 Real-World Generative AI Use Cases from Industry Leaders</a></li>
</ul>]]></content><author><name>César Soto Valero</name><email>cesarsotovalero@gmail.com</email></author><category term="ai" /><summary type="html"><![CDATA[Generative AI has taken the world by storm, offering endless opportunities for innovation. But as with any new technology, there are plenty of pitfalls to avoid. In this post, I dive into the current state of AI startups, shedding light on what works, what doesn't, and why. If you're looking to build something with AI, let’s step back and reflect on where it makes sense to innovate and where it doesn't. From avoiding overhyped general-purpose chatbots to understanding the limitations of AI in niche markets, this post offers practical insights to help you navigate the AI hype more effectively.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.cesarsotovalero.net/img/posts/2024/2024-11-17/twisted-revolver_cover.jpg" /><media:content medium="image" url="https://www.cesarsotovalero.net/img/posts/2024/2024-11-17/twisted-revolver_cover.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Why GenAI Will NOT Replace Software Engineers</title><link href="https://www.cesarsotovalero.net/blog/why-genai-will-not-replace-software-engineers-just-yet.html" rel="alternate" type="text/html" title="Why GenAI Will NOT Replace Software Engineers" /><published>2024-08-19T00:00:00-07:00</published><updated>2025-07-29T11:52:25-07:00</updated><id>https://www.cesarsotovalero.net/blog/why-genai-will-not-replace-software-engineers-just-yet</id><content type="html" xml:base="https://www.cesarsotovalero.net/blog/why-genai-will-not-replace-software-engineers-just-yet.html"><![CDATA[<p>The field of Generative Artificial Intelligence (GenAI) has made incredible strides in recent years, particularly in <a href="https://ml4code.github.io/papers.html">source code analysis and generation</a>.<br />
The excitement is well justified.<br />
Today’s GenAI systems can perform common software development tasks more efficiently than many human engineers.</p>

<aside class="youtube">
        <a href="https://www.youtube.com/watch?v=kw7fvHf4rDw"><div class="box">
        <img src="https://i.ytimg.com/vi/kw7fvHf4rDw/mqdefault.jpg" alt="YouTube video #kw7fvHf4rDw" />
        <div class="play">
          <img src="/img/icons/youtube-play20px.svg" alt="Play Video" class="youtube-logo" />
        </div>
        </div></a>
        <div>AI will NOT replace Software Engineers (for now);
        7 December 2024.</div></aside>

<p>I’ve experienced the power of this technology firsthand.<br />
<a href="https://chatgpt.com/?model=gpt-4o">ChatGPT-4o</a> is truly impressive at fixing bugs, refactoring code, and even adding entirely new features to my software projects.<br />
However, the excitement about speeding up software development isn’t universally shared across the industry.<br />
Many developers are concerned that GenAI systems will soon replace them, rendering their skills obsolete.<br />
With its powerful capabilities, GenAI seems poised to disrupt the software development job market, potentially taking over a wide range of engineering roles.</p>

<p>Over the past few months, I’ve attended several academic and <a href="https://www.linkedin.com/posts/cesarsotovalero_ai-sweden-activity-7255582747417571328-oGDN">industrial conferences</a> and read a plethora of <a href="https://ml4code.github.io/papers.html">research papers on this topic</a>.<br />
Yet, even among world-class experts, there’s no clear consensus on where we’re headed.<br />
Amid the hype, massive AI investments, and pervasive <a href="https://en.wikipedia.org/wiki/Fear_of_missing_out">fear of missing out</a>, confusion and uncertainty about AI’s current and future impact on the software industry abound.</p>

<p>In this blog post, I’ll share my perspective on the capabilities of today’s GenAI systems to see how they measure up to the demands of real-world software development.</p>

<p>Spoiler: I’m on the optimists’ side. So let’s dive in!</p>

<figure class="jb_picture">
  



<img width="100%" style="border: 0px solid #808080;" src="/assets/resized/armillary-sphere-640x373.jpg" alt="Photo of a garden sundial in the form of an armillary sphere in Skansen, Stockholm, Sweden." data-srcset="/assets/resized/armillary-sphere-640x373.jpg 640w,/assets/resized/armillary-sphere-768x448.jpg 768w,/assets/resized/armillary-sphere-1024x597.jpg 1024w,/assets/resized/armillary-sphere-1366x797.jpg 1366w,/assets/resized/armillary-sphere-1600x933.jpg 1600w," class="blur-up lazyautosizes lazyload" />
  <figcaption class="stroke"> 
    &#169; Since ancient times, we've tried to anticipate the future with certain levels of pessimism, and (ironically) we've been proven wrong most of the time. Photo of a garden sundial in the form of an <a href="https://en.wikipedia.org/wiki/Armillary_sphere">armillary sphere</a> in <a href="https://maps.app.goo.gl/61UCZPas6UNWWWS67">Skansen</a>, Stockholm, Sweden.
  </figcaption>
</figure>

<h1 id="behind-the-sensational-headlines">Behind The Sensational Headlines</h1>

<p>If you follow the latest <a href="https://tldr.tech/">tech newsletters</a> (as I do), you’ll find out that the media seems excited to push out sensational news declaring that GenAI is set to take over a wide range of software engineers jobs.<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> For those who make a living <del>pretending to write</del> writing source code, headlines like <em>“<a href="https://brainhub.eu/library/software-developer-age-of-ai">Is There a Future for Software Engineers?</a>”</em> make it easy to feel like the ground is shifting beneath our feet.</p>

<p>The concern is understandable.
Advancements in <a href="https://link.springer.com/article/10.1007/s11042-022-13428-4">natural language processing (NLP)</a> have powered GenAI systems to tackle tasks once deemed uniquely human.
These tasks not only include writing code but also other hardcore software engineering activities, such as gathering requirements, creating documentation, and even identifying user needs.</p>

<p>Consequently, we’re starting to develop a deep fear on the possibility that our professional careers might soon be eclipsed by machines capable of churning out entire software applications faster than we can.<sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">2</a></sup></p>

<p>So, if you work in tech, you’re probably wondering:</p>

<ol>
  <li>Is the career I’ve worked so hard to build at risk?</li>
  <li>Do GenAI tools truly grasp the complexities of a software project?</li>
  <li>Can GenAI tools interpret vague business requirements, understand user needs, and make informed trade-offs like a human engineer?</li>
</ol>

<h1 id="the-catch">The Catch</h1>

<p>The idea of AI kicking out software engineers is not new.
For instance, in 2022 an article in Nature Journal <a href="https://www.nature.com/articles/d41586-022-04383-z">predicted this very phenomenon</a>. 
The authors let it clear that <a href="https://openai.com/index/chatgpt/">ChatGPT</a> and <a href="https://alphacode.deepmind.com/">AlphaCode</a> would replace software engineers in the coming years.
But let’s be honest, many companies have tried to automate software development in the past, <a href="https://www.monperrus.net/martin/startups-machine-learning-code">and most of them have failed</a>.<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>
So, this kind of prediction is not new.</p>

<blockquote>
  <p>“Most AI startups and companies are primarily focused on creating <em>wrappers</em> around existing AI models without offering real value. I believe most of them will fail in the coming years.”</p>
</blockquote>

<p>Let’s consider the following scenario: a single person without any programming experience today can leverage an AI-powered IDE, like the popular <a href="https://www.cursor.com/">Cursor AI</a>, to develop a full mobile app for personal finance management. 
All from scratch!</p>

<p>Impressive, right?</p>

<p>But there is a catch. 
The developed app likely doesn’t introduce any groundbreaking innovation because its building blocks (libraries, APIs, and templates) already exist and are probably very well established in the market. 
Current GenAI code assistants merely reassemble these components to fit a well-known specific use case.
While undeniably useful, tools like ChatGPT or GitHub Copilot still don’t have the ability to fully understand the context of a software project from the business, technical, and user perspectives.</p>

<p>This distinction is vital: GenAI excels at recombination of existing knowledge, but genuine <a href="https://en.wikipedia.org/wiki/Innovation">innovation</a> (i.e., the ability to transform abstract ideas into novel practical solutions that deliver unique value) requires more than that. 
It demands a profound understanding of the problem domain, which includes grasping complex trade-offs, navigating edge cases, and adapting to evolving constraints.</p>

<p>Having diverse viewpoints and broad experience in managing a complex set of challenges is something that GenAI systems are still far from achieving.</p>

<h1 id="genai-isnt-replacing-engineers">GenAI Isn’t Replacing Engineers</h1>

<p>GenAI systems operate within well-defined boundaries set by the algorithms powering them and the data they’ve been trained on. 
They’re astonishingly effective at recognizing patterns, generating text, and synthesizing existing information in seemingly novel ways. 
However, when it comes to the messy, chaotic, and unpredictable world of software development, GenAI falls short in several key areas due to its inherent reasoning limitations.</p>

<figure class="jb_picture">
  



<img width="75%" style="border: 1px solid #808080;" src="/assets/resized/reciting-vs-reasoning-640x619.png" alt="TODO" data-srcset="/assets/resized/reciting-vs-reasoning-640x619.png 640w,/assets/resized/reciting-vs-reasoning-768x742.png 768w,/assets/resized/reciting-vs-reasoning-1024x990.png 1024w,/assets/resized/reciting-vs-reasoning-1366x1320.png 1366w,/assets/resized/reciting-vs-reasoning-1600x1547.png 1600w," class="blur-up lazyautosizes lazyload" />
  <figcaption class="stroke"> 
    Reciting vs. Reasoning: GenAI can find patterns quickly, but it can't reason like a human. Source: <a href="https://news.mit.edu/2024/reasoning-skills-large-language-models-often-overestimated-0711">MIT News</a>.
  </figcaption>
</figure>

<p>Here are some key limitations of GenAI in the context of software engineering:</p>

<ul>
  <li><strong>No Contextual Understanding:</strong> GenAI lacks the domain expertise and intuition required to assess edge cases or fully grasp the implications of design decisions.</li>
  <li><strong>No True Creativity:</strong> While GenAI can recombine existing elements in unexpected ways, its “creativity” stems from probabilistic reasoning, not from subjective experience, intentionality, or insight.</li>
  <li><strong>No Accountability:</strong> Software projects don’t live in isolation, they exist to solve real-world problems. An engineer must account for business needs, user behavior, and technical feasibility, which requires human judgment and responsibility.</li>
</ul>

<p>Let’s take software architecture as an example. 
A GenAI model can suggest patterns or frameworks based on existing designs.
However, defining the architecture for a mission-critical system that has not been done before and for which there is not available training data is a task that requires human extrapolation (e.g., balancing scalability, performance, and security).
This is because <a href="../blog/what-does-it-take-to-become-a-software-architect.html">architecture</a> is not just about choosing the right tools or patterns, it’s about understanding the problem space, anticipating future needs, and making informed trade-offs decisions.</p>

<p>As I mentioned in a <a href="../blog/ai-doesnt-make-me-the-same-coffee.html">previous blog post</a>, GenAI powered tools are not a silver bullet.
They are exceptional at <em>augmentation</em> of human capabilities rather than outright replacement.
They can speed up repetitive tasks, such as generation of boilerplate code, fix well-known bugs or code smells, or doing basic refactorings, but lack the capacity for <em>strategic thinking</em> or <em>long-term planning</em>.
Besides, getting code written so easily feels somehow like cheating, which decreases the <em>perceived value</em> of the resulting output.</p>

<h1 id="the-essence-of-software-engineering">The Essence of Software Engineering</h1>

<p>At its core, software engineering is about understanding real-world needs and finding software solutions to them. 
It’s about translating chaotic, ambiguous problems into clean, structured systems.
Software engineering is a job of high cognitive complexity because it not only requires technical skills but also a deep understanding of human behavior, business goals, and the broader context in which software operates.</p>

<p>For example, let’s say the goal is to build a mobile application for booking fitness classes. 
One solution might be to create a simple interface that allows users to choose a class and reserve a spot. 
But is that really enough? 
What if users also want to see instructor profiles, class reviews, or even receive reminders based on their schedule? 
Should it integrate with their fitness tracker or offer personalized class recommendations?
The possibilities are endless.</p>

<p>As in any other software project, the search space for building a useful app like the one mentioned before is vast.
The key point is that software engineers don’t just dive into coding without fully understanding the deeper needs of the user. 
Humans have the ability to grasp context, consider variables like user preferences, time, convenience, and even emotions.
This is something machines can’t do without a clear direction from human insight.</p>

<p>And this is precisely where GenAI falls short. 
Sure, GenAI can generate code, but it doesn’t truly <em>understand</em> the problem at hand.
It doesn’t think <em>as a human would do</em>, it lacks specific business context, and that make it fundamentally different. 
It cannot sit down with stakeholders and clarify their needs. 
It can’t challenge assumptions or deal with conflicting requirements. 
All it can do is guess based on patterns it’s been trained on. 
And often, that guess is way off the real needs of the users, because those needs are chaotic, and according to my experience, most users don’t really know what they want.</p>

<figure class="jb_picture">
  



<img width="75%" style="border: 1px solid #808080;" src="/assets/resized/jobs-quote-640x301.png" alt="People don't know what they want until you show it to them." data-srcset="/assets/resized/jobs-quote-640x301.png 640w,/assets/resized/jobs-quote-768x361.png 768w," class="blur-up lazyautosizes lazyload" /> 
  <figcaption class="stroke"> 
    Steve Jobs understood the importance of human creativity in software development.
  </figcaption>
</figure>

<p>In my opinion, this is where human engineers shine. 
We understand nuance. 
We can think creatively and adapt to changing environments.
And why does this matter? 
Because the world needs critical thinking. 
As long as humans remain as complex, messy creatures with changing needs, there will always be a demand for someone who can turn those messy needs into clean software solutions.</p>

<figure class="jb_picture">
  



<img width="50%" style="border: 1px solid #808080;" src="/assets/resized/ai-dumbness-example-640x1131.jpg" alt="Example of chatbot messing out with human context." data-srcset="/assets/resized/ai-dumbness-example-640x1131.jpg 640w,/assets/resized/ai-dumbness-example-768x1357.jpg 768w,/assets/resized/ai-dumbness-example-1024x1809.jpg 1024w," class="blur-up lazyautosizes lazyload" />
  <figcaption class="stroke"> 
    A dummy example showing the limitations of current GenAI in understanding context.
  </figcaption>
</figure>

<p>Even if we stop writing code ourselves and rely fully on natural language interfaces to generate it, the essence of software engineering will remain the same.
Source code is just the way we found to instruct computer so that they can solve human-centered problems.
Therefore, I believe we should focus on developing our problem-solving skills instead of writing code.
Our ability to define requirements and solve human problems (using AI systems) is going to be much more valuable in the coming years.</p>

<figure class="jb_picture">
  



<img width="100%" style="border: 0px solid #808080;" src="/assets/resized/requirements-engineering-in-the-age-of-ai-640x371.png" alt="TODO" data-srcset="/assets/resized/requirements-engineering-in-the-age-of-ai-640x371.png 640w,/assets/resized/requirements-engineering-in-the-age-of-ai-768x445.png 768w,/assets/resized/requirements-engineering-in-the-age-of-ai-1024x593.png 1024w,/assets/resized/requirements-engineering-in-the-age-of-ai-1366x791.png 1366w," class="blur-up lazyautosizes lazyload" />
  <figcaption class="stroke"> 
    Example of a loop of requirement engineering in the age of AI. Requirement engineers, stakeholder, engineers and domain experts cooperate with AI agents to define the requirements of a software project.
  </figcaption>
</figure>

<p>Now you might be thinking: What about the software engineering tasks that are more straightforward and annoying for humans? Could GenAI take over there?</p>

<h1 id="engineering-value-is-in-the-details">Engineering Value Is in the Details</h1>

<p>One of the biggest challenges in software development is that it’s error-prone at almost every step. 
If you misunderstand a customer need, you end up solving the wrong problem. 
Misinterpret a functional requirement, and you will end up designing a system that’s overkill, or worse, one that under-delivers.</p>

<p>For example, imagine you’re tasked with building a real-time messaging system for a customer support team handling thousands of chats per day concurrently. 
The natural assumption might be to scale up with advanced <a href="../blog/design-for-microservices.html">microservices</a> and expensive infrastructure to ensure immediate response times.
But in the prototyping phase, a more practical solution could involve implementing a simple queuing system to handle chat overflow during peak hours, reducing the need for costly infrastructure while keeping the user experience intact.</p>

<figure class="jb_picture">
  



<img width="75%" style="border: 1px solid #808080;" src="/assets/resized/notification-system-architecture-640x475.jpg" alt="Example of a notification system architecture." data-srcset="/assets/resized/notification-system-architecture-640x475.jpg 640w,/assets/resized/notification-system-architecture-768x570.jpg 768w,/assets/resized/notification-system-architecture-1024x761.jpg 1024w," class="blur-up lazyautosizes lazyload" /> 
  <figcaption class="stroke"> 
    Example of a notification system architecture using a queuing system to handle peak hours. Source:
    <a href="https://www.cometchat.com/">CometChat</a>.
  </figcaption>
</figure>

<p>A GenAI powered engineer, on the other hand, would probably go with the more “typical” solution, like adding servers and microservices, because that’s how the majority of documentation resources to build these kinds of system explain how it’s done.
GenAI models doesn’t have the creativity to consider alternative options.
They follow the existing data, the information on what is has been trained on.
I foresee this as a potential problem in the future, because it could lead to a homogenization of software solutions, where everyone uses the same tools and the same approaches, and that’s not good for <a href="https://en.wikipedia.org/wiki/Software_diversity">software diversity</a> and innovation.</p>

<p>Personally, I’m seeing too much GenAI generated code in my own work and on GitHub these days.
The difference between a good solution and a great solution often comes down to human ingenuity, finding that less obvious, but more effective approach.
I believe the real magic in engineering happens in those transitions, from needs to requirements, from designs to code.
These steps are where human insight makes all the difference.</p>

<blockquote>
  <p>“My entire career working in AI/ML has shown me that domain knowledge is the highest leverage investment. Most of the tooling for AI/agents/etc. really is still in the <em>technical</em> realm, intended for AI/ML people or maybe engineers. But imagine a world where the domain expert had the agency to iterate and improve an AI, without having to go through an <em>AI engineer intermediary</em>.
That would open the door to create incredibly powerful AI systems quickly. Keep a close eye on the product they are building, because I think it will close this gap.” – <em><a href="https://www.linkedin.com/in/skylar-payne-766a1988/">Skylar Payne</a></em></p>
</blockquote>

<p>Of course, automating the boring parts is a great thing.
That’s what engineers have been doing since the early days, right?
However, today is even more important to look for smarter ways to solve problems, because GenAI might offer a set of solutions, but it’s our job to consider all the possibilities and make a critical evaluation of each of them.</p>

<p>So, what does this mean for the whole software industry, where things are constantly in flux? Can GenAI keep up with those changes?</p>

<h1 id="the-dynamic-nature-of-software-development">The Dynamic Nature of Software Development</h1>

<p>The concept of agile software development is all about flexibility.
This means adapting to change, whether that change comes from shifting market conditions, evolving user needs, or even new regulatory requirements.
It’s a fast-paced, ever-changing process, and that exactly where current GenAI systems based on Large Language Models (LLMs) struggle the most.</p>

<figure class="jb_picture">
  



<img width="75%" style="border: 1px solid #808080;" src="/assets/resized/agile-software-development-640x389.png" alt="TODO" data-srcset="/assets/resized/agile-software-development-640x389.png 640w,/assets/resized/agile-software-development-768x466.png 768w,/assets/resized/agile-software-development-1024x622.png 1024w,/assets/resized/agile-software-development-1366x829.png 1366w," class="blur-up lazyautosizes lazyload" />
  <figcaption class="stroke"> 
    Agile software development is all about adapting to change.
  </figcaption>
</figure>

<p>For example, imagine you are midway through a project when suddenly, the market shifts. The client wants to pivot. 
You need to revise your priorities and adjust the entire architecture of your software. Can a GenAI system do that? Yes, it can!
But what if the conditions are completely new because of an event that has never ever happened before? For example, during a market crash or a natural disaster.
It’s hard for me to imagine a GenAI system that can adapt to those kinds of unpredictable situations.</p>

<p>AI operates based on past data and predefined assumptions. 
It’s not going to sit in a meeting with you, hear the client’s concerns, and propose a totally new approach.
But part of our work is finding out what the client really needs, even when they don’t know it themselves!</p>

<p>Now, don’t get me wrong.
Current GenAI can assist in agile development, especially when it comes to routine tasks like generating boilerplate code or testing. 
But when it comes to adapting to new, unexpected demands, they show some limitations.</p>

<p>AI will not replace programmers but will fundamentally change the development landscape, making human creativity and problem-solving capabilities more essential than ever. 
Adaptability is one of the most valuable skills in software engineering. 
As long as the world keeps changing, there will always be a need for people who can pivot quickly and come up with creative solutions.</p>

<p>Now, let’s take a closer look at the situations where GenAI actually excels in software engineering, and how we can leverage that.</p>

<h1 id="where-genai-can-actually-help">Where GenAI Can Actually Help</h1>

<p>Take code reviews, for example. 
I think it’s a very tedious but necessary task.
You’ve got to sift through line after line of someone else’s work, looking for bugs, performance issues, or security vulnerabilities. 
<a href="https://en.wikipedia.org/wiki/Intelligent_agent">GenAI agents</a>, like those integrated on GitHub or other platforms to review pull-requests, can help automate parts of this process, flagging potential issues before a human ever gets involved.</p>

<p>AI can help with tasks that are repetitive, like generating boilerplate code, adding unit tests, or writing documentation. 
It can analyze large datasets and suggest optimizations. 
Indeed, many surveys suggest that developers are eager to get away from these tasks to GenAI assistants.</p>

<p>But when it comes to making the final call, the human touch is still essential.</p>

<blockquote>
  <p>“With LLMs, coding won’t be enough to differentiate as an engineer, you’ll need to think about the product, business KPIs, strategy etc. 
You need to think about solutions to problems, not software tools. 
And PMs are going to be expected to get more technical. 
Nothing is stopping them now, LLMs help you code. 
They will need to use their product knowledge and combine it with technicals to give engineers more tangible instructions. 
Those sound pretty similar, and the result will be less fragmentation and miscommunication between PMs and engineers, something that’s far too common today. 
They’ll start speaking more similar languages.”</p>
</blockquote>

<p>In my opinion, the future isn’t about GenAI taking over but about humans learning how to work alongside GenAI to be more efficient and effective. And the engineers who can collaborate with GenAI agents (or whatever we’ll call them) will be the ones who thrive in the future.</p>

<figure class="jb_picture">
  



<img width="100%" style="border: 1px solid #808080;" src="/assets/resized/stackoverflow-dev-survey-2024-ai-developer-tools-ai-complex-social-640x296.png" alt="Stack Overflow Developer Survey 2024 results on the perceived ability of AI tools to handle complex tasks." data-srcset="/assets/resized/stackoverflow-dev-survey-2024-ai-developer-tools-ai-complex-social-640x296.png 640w,/assets/resized/stackoverflow-dev-survey-2024-ai-developer-tools-ai-complex-social-768x355.png 768w,/assets/resized/stackoverflow-dev-survey-2024-ai-developer-tools-ai-complex-social-1024x474.png 1024w,/assets/resized/stackoverflow-dev-survey-2024-ai-developer-tools-ai-complex-social-1366x632.png 1366w,/assets/resized/stackoverflow-dev-survey-2024-ai-developer-tools-ai-complex-social-1600x740.png 1600w,/assets/resized/stackoverflow-dev-survey-2024-ai-developer-tools-ai-complex-social-1920x888.png 1920w," class="blur-up lazyautosizes lazyload" />
  <figcaption class="stroke"> 
    Stack Overflow Developer Survey 2024 results on the perceived ability of AI tools to handle complex tasks.
  </figcaption>
</figure>

<p>We should integrate GenAI into our workflows where it makes sense, use it to handle the boring stuff, so we can focus on the creative and complex tasks.</p>

<p>And I know what you may be thinking, if GenAI can assist in low level tasks, that means it could eventually replace less experienced engineers. Isn’t it?</p>

<h1 id="can-genai-replace-entry-level-engineers">Can GenAI Replace Entry Level Engineers?</h1>

<p>Well, the answer is… yes/maybe.</p>

<p>But…</p>

<p>Less experienced engineers are not just doing the easy tasks.
They’re learning, growing professionally, and building the experience they need to tackle more complex problems down the road. 
GenAI might be able to take over some of the simpler tasks, but it can’t replace the learning process.</p>

<p>For example, if you have been around for a while in this industry, think about how much you learned from your first projects. 
It wasn’t just about writing down the code (programmers are not typists). 
It was about understanding the problem, communicating with other developers, figuring out how to structure the solution, and dealing with unexpected bugs along the way.</p>

<p>If we remove the possibility of learning that stuff, and put GenAI systems in place, then it’ll be harder for the young people to gain that kind of experience or develop the necessary intuition to solve real-world problems.</p>

<p>What should we do then?
Well… if you’re an entry level engineer, your goal should not be to compete with AI. 
It should be to delegate the tasks that GenAI can efficiently handle, and moving on to the bigger, more interesting challenges.</p>

<p>So, keep pushing yourself.
Don’t get too comfortable with the easy tasks and just delegate those to GenAI systems.
Focus on learning <a href="../blog/surviving-the-ai-revolution-as-a-software-engineer.html">the skills</a> that only humans can master.</p>

<h1 id="final-thoughts">Final Thoughts</h1>

<p>As software <del>developers</del> creators, the key is to think critically about where GenAI adds value and where it falls short.
Instead of viewing GenAI as a <em>competitor</em>, we should treat it as a <em>collaborator</em>. 
By leveraging GenAI for grunt work, we can focus on higher-value tasks, such as crafting innovative designs, optimizing user experiences, and solving problems that GenAI simply cannot understand at a conceptual level (i.e., leveraging <em>the human experience</em>).</p>

<figure class="jb_picture">
  



<img width="100%" style="border: 1px solid #808080;" src="/assets/resized/stackoverflow-dev-survey-2024-ai-efficacy-and-ethics-ai-threat-social-640x248.png" alt="Stack Overflow Developer Survey 2024: Are AI tools a threat to software engineers?" data-srcset="/assets/resized/stackoverflow-dev-survey-2024-ai-efficacy-and-ethics-ai-threat-social-640x248.png 640w,/assets/resized/stackoverflow-dev-survey-2024-ai-efficacy-and-ethics-ai-threat-social-768x298.png 768w,/assets/resized/stackoverflow-dev-survey-2024-ai-efficacy-and-ethics-ai-threat-social-1024x397.png 1024w,/assets/resized/stackoverflow-dev-survey-2024-ai-efficacy-and-ethics-ai-threat-social-1366x529.png 1366w,/assets/resized/stackoverflow-dev-survey-2024-ai-efficacy-and-ethics-ai-threat-social-1600x620.png 1600w,/assets/resized/stackoverflow-dev-survey-2024-ai-efficacy-and-ethics-ai-threat-social-1920x744.png 1920w," class="blur-up lazyautosizes lazyload" />
  <figcaption class="stroke"> 
    Stack Overflow Developer Survey 2024 results about the perceived threat of AI tools for software engineers.
  </figcaption>
</figure>

<p>The truth is, your skills remain highly relevant, so long as you <em>adapt</em> to the new order. 
The engineers who thrive in this new landscape will be those who augment their capabilities with GenAI while continuing to bring their unique human creativity, expertise, and empathy to the table.</p>

<p>In conclusion, I don’t think GenAI is going to replace software engineers (at least not anytime soon).
And sure, it can handle repetitive tasks and assist in certain areas, but the real value coming from software development still requires human intuition, creativity, and problem-solving.</p>

<p>Because, let me say it again, software development isn’t just about writing code, it’s about solving problems.
These problems are often really messy, poorly defined, and which solutions are deeply tied to human intuition and creativity. 
The main task of engineers is to balance conflicting priorities, adapt to ever-changing environments, and navigate the ambiguous waters between business needs and technical solutions.</p>

<p>If there is one way to keep our pockets safe, I think it is to stay ahead of the curve.</p>

<h1 id="footnotes">Footnotes</h1>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p>This is understandable once we realize that the traditional media is in the business of creating sensational headlines and monetizing our attention rather than providing accurate information. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>Not to mention the negative impact of AI related layouts on our <del>highly overvalued</del> software engineering pockets. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>I still remember Dreamweaver from <a href="https://en.wikipedia.org/wiki/Macromedia">Macromedia</a> (later purchased by Adobe). It was a no code app that promised to make web development just a matter of throwing components to a canvas. It didn’t work out as expected, and today we are still writing HTML, CSS and JavaScript, ouch! <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>César Soto Valero</name><email>cesarsotovalero@gmail.com</email></author><category term="ai" /><summary type="html"><![CDATA[GenAI systems are becoming more and more capable of performing complex cognitive tasks that were once thought to be uniquely human. In particular, LLMs are proven to be very good at writing code. With all the buzz around GenAI replacing software engineers, are our jobs really at risk? In this article, I dive deep into AI's current capabilities and how they stack up against the demands of real-world software development. I discuss the actual potential of GenAI to assist in common development activities, and where it still falls short when it comes to creative problem solving and human intuition.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.cesarsotovalero.net/img/posts/2024/2024-08-19/armillary-sphere_cover.jpg" /><media:content medium="image" url="https://www.cesarsotovalero.net/img/posts/2024/2024-08-19/armillary-sphere_cover.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">The Not-So-Strange Case of Cargo Cult in Computer Science Research</title><link href="https://www.cesarsotovalero.net/blog/the-not-so-strange-case-of-cargo-cult-in-computer-science-research.html" rel="alternate" type="text/html" title="The Not-So-Strange Case of Cargo Cult in Computer Science Research" /><published>2024-07-13T00:00:00-07:00</published><updated>2025-07-29T11:52:25-07:00</updated><id>https://www.cesarsotovalero.net/blog/the-not-so-strange-case-of-cargo-cult-in-computer-science-research</id><content type="html" xml:base="https://www.cesarsotovalero.net/blog/the-not-so-strange-case-of-cargo-cult-in-computer-science-research.html"><![CDATA[<aside class="youtube">
        <a href="https://www.youtube.com/watch?v=N7pY8VAt6g4"><div class="box">
        <img src="https://i.ytimg.com/vi/N7pY8VAt6g4/mqdefault.jpg" alt="YouTube video #N7pY8VAt6g4" />
        <div class="play">
          <img src="/img/icons/youtube-play20px.svg" alt="Play Video" class="youtube-logo" />
        </div>
        </div></a>
        <div>Cargo Cult in Computer Science: An Uncomfortable Truth for Researchers;
        22 September 2024.</div></aside>

<p>Everywhere, I see computer scientists that look like they are working on the right thing. 
They spent a long time doing the thing, writing code to run it (in the best cases), and collecting results.
Then, they <em>publish</em> the thing.
However, many times “the thing” is either worthless or just doesn’t work as claimed.
In fact, according to <a href="https://www.semanticscholar.org/paper/Measuring-Reproducibility-in-Computer-Systems-Collberg/075d12c8c9e295cc3227d12ecff7f5c16d8a8613">a study made in 2014</a>: more than 75% of the results published in <em>prestigious</em> computer science journals are not reproducible at all!
This is a huge problem for science.
The fact that a lot of human and non-human time and resources are spent on research that doesn’t produce any value is something to look at with microscopic lens.
Unfortunately, this problem is nothing new.
Back in 1974, the American physicist <a href="https://en.wikipedia.org/wiki/Richard_Feynman">Richard Feynman</a> coined the term “cargo cult science” referring to this exact same phenomenon.
Now, I want to revisit his ideas, 50 years later, from the lens of someone who has gone through the hustle of producing science “the modern way.”
In this article, I’ll cover the many traps leading to cargo cult science, and provide my personal view on how to stay away from it. 
Let’s dive in!</p>

<figure class="jb_picture">
  



<img width="100%" style="border: 0px solid #808080;" src="/assets/resized/vortice-art-640x382.webp" alt="Picture of a dark vortices ring" data-srcset="/assets/resized/vortice-art-640x382.webp 640w,/assets/resized/vortice-art-768x459.webp 768w,/assets/resized/vortice-art-1024x612.webp 1024w,/assets/resized/vortice-art-1366x816.webp 1366w,/assets/resized/vortice-art-1600x956.webp 1600w," class="blur-up lazyautosizes lazyload" />
  <figcaption class="stroke"> 
   &#169; Cargo cult rituals trap scientists in an endless vicious cycle of unproductivity. Picture taken at the <a href="https://en.wikipedia.org/wiki/Odenplan_station">metro station of Odenplan</a>, in Stockholm city.
  </figcaption>
</figure>

<h1 id="the-origins-of-cargo-cult-science">The Origins of Cargo Cult Science</h1>

<figure class="badge"><a href="http://calteches.library.caltech.edu/51/2/CargoCult.pdf"><img src="/img/badges/cargo-cult-science.png" style="width:140px;max-width:100%;" alt="badge" /></a></figure>

<p>The term “cargo cult science” was coined by the American physicist <a href="https://en.wikipedia.org/wiki/Richard_Feynman">Richard Feynman</a> in 1974 during his commencement speech at Caltech university.
It’s <del>probably</del> one of the greatest speeches about science of all time.</p>

<p>Cargo cult science refers to practices that seem scientific but don’t truly follow the <a href="https://en.wikipedia.org/wiki/Scientific_method">scientific method</a>.<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>
Feynman highlighted this concept by comparing it to the cargo cult rituals of Melanesian and Micronesian societies.
There, the natives imitated the behaviors of World War II soldiers, hoping to bring back the material wealth they had seen during the war.</p>

<blockquote>
  <p>“In the South Seas, there is a cargo cult of people. During the war, they saw airplanes land with lots of good materials, and they want the same thing to happen now. So they’ve arranged to make things like runways, to put fires along the sides of the runways, to make a wooden hut for a man to sit in, with two wooden pieces on his head like headphones and bars of bamboo sticking out like antennas (he’s the controller) and they wait for the airplanes to land. They’re doing everything right. The form is perfect. It looks exactly the way it looked before. But it doesn’t work. No airplanes land.”
— <cite>Richard Feynman</cite></p>
</blockquote>

<p>This metaphor brilliantly showcases the need for genuine understanding and accountability in science.
It serves as an almost comical reminder of how badly things can go wrong when we follow procedures without applying critical thinking. 
In today’s world, this message is particularly relevant as the fear of being wrong and the spread of non-so-ethical practices continues to grow.</p>

<p>The temptation to take the easy path of doing poor science is constantly knocking on the researchers’ doors.</p>

<p>Feynman states that researchers should not ignore the principle of scientific integrity to avoid falling into the abyss of cargo cult science.
He mentions that it is important to make scientific conclusions while providing all the information on the research, including possible faults and unexpected results.
According to Feynman, <a href="https://en.wikipedia.org/wiki/Scientific_integrity">scientific integrity</a> is a principle of scientific thought that corresponds to a kind of utter honesty.</p>

<blockquote>
  <p>“I think the educational and psychological studies I mentioned are examples of what I would like to call Cargo Cult Science. […] So I call these things Cargo Cult Science, because they follow all the apparent precepts and forms of scientific investigation, but they’re missing something essential, because the planes don’t land.”
— <cite>Richard Feynman</cite></p>
</blockquote>

<p>Scientists should put extra effort to not miss any important things while concluding their findings.
This allows not fooling oneself and provide complete information to other scientists.</p>

<p>It’s important to report all findings and results, not just those that support a particular conclusion. Cargo cult scientists often omit results that don’t align with their experiments, ignoring evidence that could diminish the perceived novelty and value of their work.</p>

<blockquote>
  <p>“You must not fool yourself (and you are the easiest person to fool). So you have to be very careful about that. After you’ve not fooled yourself, it’s easy not to fool other scientists. You just have to be honest in a conventional way after that.”
— <cite>Richard Feynman</cite></p>
</blockquote>

<p>In other words, if there is no a sound “Threats to Validity” section in a paper, or at least a strong discussion around the validity of the results, it could indicate a typical case of cargo cult science.</p>

<h2 id="poor-science">Poor Science</h2>

<figure class="badge"><a href="https://en.wikipedia.org/wiki/Surely_You%27re_Joking,_Mr._Feynman!"><img src="/img/badges/you-are-joking-mr-feynman.png" style="width:140px;max-width:100%;" alt="badge" /></a></figure>

<p>Feynman determines a range of characteristics that can be used to define poor science, pseudoscience, or cargo cult science.</p>

<p>First, he mentions the existence of a seemingly scientific way of presenting investigations, which attempts to hide the existence of one essential feature that prevents the idea from working.
One of the most important characteristics of pseudo-scientific researches is the lack of scientific integrity as a kind of leaning over backward.
To avoid it, it is necessary to discuss the research details and results from all the perspectives, while paying much attention to faults, biases, and wrong assumptions.</p>

<p><strong>Example:</strong> In 1989, two electrochemists, Martin Fleischmann and Stanley Pons, <a href="https://cen.acs.org/articles/94/i44/Cold-fusion-died-25-years.html">announced</a> they had achieved to create “<a href="https://en.wikipedia.org/wiki/Cold_fusion">cold fusion</a>” at room temperature. This type of nuclear fusion, typically requiring extremely high temperatures, could potentially provide a virtually limitless source of clean energy. The announcement created a sensation because if true, it would have revolutionized energy production. However, it turn out that the results were not real. The term “cold fusion” became a synonym for junk science.</p>

<p>Second, it is the ignorance of repeating the experiment by oneself. 
Those researchers who choose to use the results received by the other investigators ignore the opportunity to make their own research more credible while supporting or rejecting the known findings.</p>

<p><strong>Example:</strong> In the early days of deep learning, many researchers and companies <a href="https://doi-org.focus.lib.kth.se/10.1016/j.caeai.2023.100143">made bold claims</a> that AI models could teach themselves or autonomously learn complex tasks without needing extensive human intervention. These claims were often based on specific experiments that showed impressive results, such as a neural network learning to recognize objects in images after being exposed to vast amounts of unlabeled data. Practitioners often integrated these models into their projects without independently replicating the original experiments, leading to disappointing results when the models failed to generalize as promised. Over time, it became clear that these AI models were not as autonomous or generalizable as initially claimed. When others attempted to use these “self-taught” models in different domains or with less curated data, the models often failed to perform as expected. It turned out that much of the success in the original experiments was due to the specific data used or additional, often unacknowledged, human intervention during the training process.</p>

<p>Last but not less, Feynman states that pseudo-scientists can be very anxious for new results because they could be contradictory in relation to their predictions. 
A single negative data point made in the past could create enough doubt and uncertainty to smash the perceived value of current experiments.
Consequently, it is characteristic of cargo cult scientists to ignore the results which do not support their experiments and focus on those which sound appropriate.</p>

<p><strong>Example:</strong> In 2011, the OPERA experiment in Italy <a href="https://en.wikipedia.org/wiki/2011_OPERA_faster-than-light_neutrino_anomaly">reported results</a> suggesting that neutrinos might be traveling faster than the speed of light, which would contradict Einstein’s theory of relativity. Some researchers and media outlets were eager to embrace these results because of their sensational nature, which could have upended established physics. However, this eagerness led to a dangerous tendency to overlook potential errors or alternative explanations that might invalidate the findings. There was significant pressure to confirm the extraordinary results, leading to a focus on the data that seemed to support the faster-than-light claim while neglecting inconsistencies or mundane explanations. Further investigation revealed that the results were due to a faulty connection in the GPS system used to measure the neutrinos’ travel time. Once corrected, the faster-than-light result disappeared, and the experiment confirmed that neutrinos do not travel faster than light.</p>

<p>Sometimes focusing on sensational results while ignoring contradictory evidence can lead to the premature and incorrect validation of scientific claims
The eagerness to confirm groundbreaking findings often overshadows the need for rigorous scrutiny. 
This tendency causes cargo cult scientists to neglect critical evidence that contradicts their desired outcomes.</p>

<h2 id="the-reproducibility-crisis">The Reproducibility Crisis</h2>

<p>A defining characteristic of the scientific method, distinguishing it from belief or faith, is the requirement that others must be able to independently replicate the results of an experiment.</p>

<aside class="quote">
    <em>“Other experimenters will repeat your experiment and find out whether you were wrong or right.”</em> 
</aside>

<p>Put it in another way: the findings of an experiment can be accepted as valid <em>only if</em> they are supported by the results of other researchers who conduct it independently.
That is why experiments should be repeated in order to state the credibility of the results and the integrity of the overall investigation.
Cargo cult scientists usually ignore repeating experiments arguing lack of time, resources, or just because they don’t want to support certain assumptions.</p>

<p>An ethical scientist should feel the need to demonstrate that a valid experiment is reproducible.</p>

<p>Feynman notes that para-psychologists or other cargo cult scientists look for experiments that can be repeated because they need to receive the same effect.
However, these scientists look for <em>repeated results</em>, not for the repeatable experiments.
This is because if an experiment is repeatable, its results can be different, and they do not always satisfy the expectations of cargo cult scientists. 
The rejection of the repeatable experiment is considered as characteristic for those scientists who intend to ignore the idea of integrity.
Such behavior cannot be discussed as acceptable in the scientific world.</p>

<p>In computing science, <a href="http://reproducibility.cs.arizona.edu/tr.pdf">it’s clear</a> that we do not take reproducibility seriously enough.</p>

<p>It is mind-blowing that today most papers do not release their experimental data or even the software artifacts presented as part of their contributions.
Open-sourcing tools and datasets should be a prerequisite for publishing any kind of research paper.<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>
In fact, some researchers will laugh and admit that published results might just be the outcome of a graduate student finally getting their code to run once and getting some data to produce a graph that makes sense.</p>

<p>But don’t get me wrong: achieving true reproducibility is a tough problem in computer science research.</p>

<p>For example, a real obstacle to achieving reproducible results in computer science is the rapid pace at which the field evolves. 
Even finding a computer with similar hardware to the one used just five years ago to generate a result is not easy. 
Running the software with the exact same configurations, memory, operating system, and CPU is even more challenging.
Solutions like Docker help, but what guarantees do we have that Docker will still exist in the next 20 or 30 years?</p>

<p>For the time being, it’s unlikely that conferences will require researchers to submit their hardware along with their software.
However, as scientists, we can take reproducibility as one of the key deciding factors when <a href="../blog/how-i-peer-review-research-papers.html">peer-reviewing research papers</a>.
The more we value reproducibility when deciding if accepting or rejecting a paper, the more we will see a change in the way reproducible research is conducted.</p>

<h2 id="the-academic-trap">The Academic Trap</h2>

<p>The <a href="../blog/the-last-paper-myth.html">pressure to publish</a>, combined with rapid progress of the computer science field, has distorted priorities, leading to publications at any cost.</p>

<p>This has given rise to widespread academic misconduct, including paper mills, where existing research is haphazardly combined to create seemingly new results that might get published somewhere, even if not in top-tier journals.
In fields like medicine, this pressure has become so severe that entire bodies of research <a href="https://doi.org/10.4103%2F0019-5545.82529">have been retracted</a> after being exposed as based on faulty or fraudulent data.</p>

<p>Feynman laments that the kind of integrity he talks about isn’t baked into the science education system.
This hardly comes as a surprise, given it’s largely a system premised on certitude at all costs.</p>

<p>As we know, it’s not the admission of ignorance that fuels science.</p>

<blockquote>
  <p>“This long history of learning how not to fool ourselves (of having utter scientific integrity) is, I’m sorry to say, something that we haven’t specifically included in any particular course that I know of. We just hope you’ve caught on by osmosis.”
—  <cite>Richard Feynman</cite></p>
</blockquote>

<p>The computer science field is driven by rapid innovation and fierce competition for funding and publication.
This particular sense of fast speed, together with “the fear of missing out,” exacerbates the temptation to fall into cargo cult behaviors.</p>

<p>Here are some ways in which it could manifest:</p>

<ul>
  <li><strong>Chasing trends over substance</strong>: Often, researchers might feel pressured to work on “hot topics” not because they are convinced of their importance, but because they attract attention, citations, and funding. An example is <a href="https://rdcu.be/dRprN">the increasing interest</a> in Generative AI research that we are seeing right now.</li>
  <li><strong>P-hacking and selective reporting</strong>: In the quest for significant results, there’s a risk of manipulating data or selectively reporting results that support the hypothesis, rather than presenting the full picture of the research findings. An example is when ignoring the effect of some variables to get a <a href="https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation">sense of causation from what it is mere correlation</a>.</li>
  <li><strong>Reinventing the wheel</strong>: Sometimes, due to a lack of thorough literature review or out of a desire to claim novelty, researchers might end up replicating existing studies without adding real value or new insights. This has happened many times, for example, the concept of <code class="language-plaintext highlighter-rouge">MapReduce</code> is credited to Google, but the concepts behind where published <a href="https://doi.org/10.1145/129888.129894">decades before</a> by researchers from the fields of parallel databases and functional programming.</li>
  <li><strong>Over-reliance on tools and frameworks</strong>: It’s not uncommon to see studies where advanced tools or algorithms are used without a proper understanding of their underlying principles or limitations, simply because they are popular or well-regarded. Think about deep learning and the fact that <a href="https://en.wikipedia.org/wiki/Explainable_artificial_intelligence">we don’t really know </a> how these black-boxed complex neural networks actually operate to deliver their results.</li>
</ul>

<p>Having navigated <a href="../blog/my-phd-as-a-comic.html">the rocky terrains</a> of doing a PhD, I’ve experienced firsthand the challenges that push even the well-intentioned scientist towards these pitfalls. 
The pressure to “publish or perish” and the fierce competition for grants can sometimes blur the lines between rigorous investigation and the rush to produce novel results.</p>

<p>Moreover, the lack of a good work-life balance, the anxiety of job (in)security in academia, and the often solitary nature of deep research can exacerbate these issues. 
These challenges not only affect the mental health and well-being of researchers but can also compromise the integrity of the research itself.</p>

<p>Nevertheless, there are ways to make good research while staying away from cargo cult behaviors.</p>

<h2 id="the-seek-of-rigor">The Seek of Rigor</h2>

<p>As a rule of thumb: if the results presented in a paper seem too good to be true, they probably aren’t true.</p>

<p>Researchers must be cautious when presenting technical arguments. 
It’s surprisingly easy to convince the public that your approach is scientific, even when it isn’t.
Therefore, it is our moral duty to rigorously scrutinize our own arguments before sharing them. 
We should all strive to gather data that challenges our beliefs and respond to it as objectively as possible. 
This includes refraining from censoring or ignoring opposing views, and from penalizing those who disagree with our perspectives.</p>

<p>So, how do we combat these tendencies?</p>

<p>The answer lies in fostering a culture of integrity and transparency in research. 
Here are some key points:</p>

<ul>
  <li><strong>Education and awareness</strong>: Early career researchers especially need guidance on the ethical dimensions of scientific research, including the importance of rigorous, unbiased experimental design and the pitfalls of cargo cult science.</li>
  <li><strong>Encouraging reproducibility</strong>: As I mentioned before, journals and conferences should promote and reward the replication of studies and the sharing of negative results to counteract the bias towards only publishing positive findings. For example, by increasing acceptance rates or creating especial <a href="https://2024.msrconf.org/track/msr-2024-data-and-tool-showcase-track">artifact and data showcase tracks</a> in conferences (e.g., as in the <em>International Conference of Mining Software Repositories</em>).</li>
  <li><strong>Institutional support</strong>: Universities and funding bodies must recognize the pressures leading to these practices and offer support systems to encourage quality over quantity. For example, having a tenure and promotion criteria <a href="../blog/how-phd-profiles-are-evaluated.html">that value the impact of research</a> over mere publication metrics.</li>
  <li><strong>Community and collaboration</strong>: Fostering a more collaborative rather than competitive environment helps researchers share methods, findings, and validations more openly, reducing the isolation that allure taking unethical shortcuts.</li>
</ul>

<p>So, it ultimately comes down to establishing mechanisms or systems that promote rigorous science by adhering to well-established principles, such as careful experimentation, critical analysis, reproducibility, and transparency.</p>

<h2 id="the-finding-of-hope">The Finding of Hope</h2>

<aside class="quote">
    <em>“A real scientist, not a cargo cult scientist, should follow the principle of integrity.”</em> 
</aside>

<p>As from the Feynman speech, the three takeaways are straightforward:</p>

<ol>
  <li>Get your fundamentals very clear.</li>
  <li>Don’t aim for fast results with superficial learning.</li>
  <li>Have skin in the game: hypothesize, experiment, explore, and remember to follow the scientific method.</li>
</ol>

<p>I’ll end this article quoting one the Feynman’s last takeaways:</p>

<blockquote>
  <p>“I have just one wish for you: the good luck to be somewhere where you are free to maintain the kind of integrity I have described, and where you do not feel forced by a need to maintain your position in the organization, or financial support, or so on, to lose your integrity. May you have that freedom.”
—  <cite>Richard Feynman</cite></p>
</blockquote>

<p>“… May you have that freedom.”</p>

<p>Isn’t that such a sublime sentence to end this post?</p>

<h1 id="external-resources">External Resources</h1>

<ul>
  <li><a href="http://calteches.library.caltech.edu/51/2/CargoCult.pdf">“Cargo Cult Science”</a>. Feynman, Richard P. (June 1974).</li>
  <li><a href="https://en.wikipedia.org/wiki/Surely_You%27re_Joking,_Mr._Feynman!">:globe_with_meridians: “Surely You’re Joking, Mr. Feynman!”</a>. Feynman, Richard (1997).</li>
  <li><a href="https://queue.acm.org/detail.cfm?id=3674499">:globe_with_meridians: Repeat, Reproduce, Replicate</a>. <em>ACM Queue</em></li>
  <li><a href="https://www.latimes.com/business/hiltzik/la-fi-mh-feynman-20131028-story.html">:globe_with_meridians: More on the crisis in research: Feynman on ‘cargo cult science’</a>. <em>Los Angeles Times</em></li>
  <li><a href="https://queue.acm.org/detail.cfm?id=3595860">:globe_with_meridians: Cargo Cult AI: Is the ability to think scientifically the defining essence of intelligence?</a>. <em>ACM Queue</em></li>
</ul>

<h1 id="footnotes">Footnotes</h1>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:2">
      <p>There is also <a href="https://en.wikipedia.org/wiki/Cargo_cult_programming">cargo cult programming</a>: the ritual inclusion of code or program structures that serve no real purpose. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:1">
      <p>The Association of Computing Machinery (ACM) has introduced a <a href="https://www.acm.org/publications/policies/artifact-review-and-badging-current">badging system</a> for software artifacts since 2020. I still find it difficult to distinguish the difference between a “repeatable,” “reproducible,” and “replicable” experiment or tool. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>César Soto Valero</name><email>cesarsotovalero@gmail.com</email></author><category term="science" /><summary type="html"><![CDATA[Arguably, computers and faster communication systems have changed the world more than any other technology. Yet there have been many scientific failures along the way, some prominent, many minor, and a widening gap between aspirations and reality. This article discusses the problem of pseudoscience in computer science. I discuss Richard Feynman's coined term "cargo cult science," as he said: "Everywhere I see computer scientists that look like they are working on the right thing, but the thing actually doesn't work." I offer guidance on how to stay away of this dangerous practice, from a scientific standpoint.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.cesarsotovalero.net/img/posts/2024/2024-07-13/vortice-art_cover.webp" /><media:content medium="image" url="https://www.cesarsotovalero.net/img/posts/2024/2024-07-13/vortice-art_cover.webp" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Some Things to Remember Before Acting Pretentiously</title><link href="https://www.cesarsotovalero.net/blog/some-things-to-remember-before-acting-pretentiously.html" rel="alternate" type="text/html" title="Some Things to Remember Before Acting Pretentiously" /><published>2024-06-07T00:00:00-07:00</published><updated>2025-07-29T11:52:25-07:00</updated><id>https://www.cesarsotovalero.net/blog/some-things-to-remember-before-acting-pretentiously</id><content type="html" xml:base="https://www.cesarsotovalero.net/blog/some-things-to-remember-before-acting-pretentiously.html"><![CDATA[<p>A few days ago, I was asked: <em>“Where do you see yourself in 10 or 20 years?”</em>
I replied:
<em>“I would like to get some sort of recognition for my contributions… maybe to be known as an expert in something, or someone who is consulted and listened to before important decisions are made.”</em>
Now that I think about it, I realize my answer was both bold and pretentious.
But things went even worse. 
Immediately after, I was asked:
<em>“So, what do you really want? Is it the act of achieving something extraordinary (like winning the Nobel Prize), or the recognition that comes with it?”</em>
My answer was something like the following:
<em>“It’s the fact of overcoming challenges that brings me joy. For example, I don’t care about the number of paper citations I have, but I would like to know that I did something remarkable, and it’s great when others recognize my achievements.”</em>
In retrospect, what such a pretentious answer that was!
What the hell?
Seeking for “recognition” is clearly a selfish goal.
I wonder why did I answer in that way.
So I’ve been thinking deeply about this question and what my true answer should have been.
To my relief, I realize that I don’t really want to be recognized in any way.
<strong>What I really want is to help and inspire others, so they can achieve great things!</strong><sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>
I’m glad I realized that, and I hope to never forget the long and arduous road that brought me here.
Or am I starting to forget?
Maybe I should recap how insignificant I am and how much I owe to others.
Maybe now it’s a good time to remember my humble roots.
Seriously.</p>

<figure class="jb_picture">
  



<img width="100%" style="border: 0px solid #808080;" src="/assets/resized/remember-your-roots-640x373.jpg" alt="A hand scratching the surface of the earth" data-srcset="/assets/resized/remember-your-roots-640x373.jpg 640w,/assets/resized/remember-your-roots-768x448.jpg 768w,/assets/resized/remember-your-roots-1024x597.jpg 1024w,/assets/resized/remember-your-roots-1366x797.jpg 1366w,/assets/resized/remember-your-roots-1600x933.jpg 1600w," class="blur-up lazyautosizes lazyload" />
  <figcaption class="stroke"> 
    &#169; Remember your roots and stay humble César! Photo taken at <a href="https://maps.app.goo.gl/7Urts86N3j5t8eEX9">Skälderviksplan's park</a> in Stockholm. 
  </figcaption>
</figure>

<h1 id="childhood">Childhood</h1>

<p>Remember that you grew up eating wild frogs from swamps. Your father brought them home and cut them, while your mother fried them and told you they were chicken. You loved it. <em>Sometimes, it is better not to know the origin of certain things.</em></p>

<p>Remember when you were a child and broke that toy to look into it because you were curious about how it worked. You cried a lot when your mom said it couldn’t be fixed. <em>Think twice before taking the risk of breaking things that just can’t be fixed.</em></p>

<p>Remember that the forest was your playground during childhood. You swam in the wild waters of the river after the hurricane, fished and hunted with the equipment you made yourself, played football with coconuts, and played baseball with almond fruits as balls and a tree stick as the bat. You had so little back then, and yet, you were so happy. <em>There is no need to have many things or do something outstanding to be happy.</em></p>

<p>Remember that you were the last kid to be picked for teams in all sports. You were never fast enough, strong enough, or talented enough. It was a big deal for you back then, but now you <a href="https://www.linkedin.com/posts/cesarsotovalero_seb-running-health-activity-7206615823736537089-0h83">run 10k in 48 minutes</a> with ease and don’t care about such things. The importance of things changes drastically over time, so don’t worry too much about your limitations. <em>You’ll forget about almost all of your “big” problems in a few years.</em></p>

<p>Remember the good old days when the whole family would sit together at the same table to celebrate the New Year. We played dominoes and listened to music from cassettes and later from CD-ROMs. Those were probably the happiest moments of your life, and you didn’t even know it. <em>It’s likely that you’re living the best moment of your life right now without even realizing it.</em></p>

<p>Remember your hometown: there were almost no paved roads, and people used horses as a means of transportation. That was great because the streets were safer, so you could play baseball in the streets with other kids. <em>There is always some way to turn our limitations into opportunities.</em></p>

<h1 id="family">Family</h1>

<p>Remember that your dad harvested rice and fed pigs with his own hands for many years so that you could have the best possible food served on your plate every day. He was a great civil engineer, but he spent more than half of his lifetime just searching for food, to provide for the family. Times were difficult back then. <em>You should do something worthwhile with all the free time you have available now.</em></p>

<p>Remember how many people your mom helped and how hard she worked, cleaning your non-disposable diapers at 5:00 every single day. She was a generous pharmacist, making, collecting, and giving medicines to those in need. <em>You are living such an easy life compared to your parents, and yet, you are not giving back to others in the same proportion.</em></p>

<p>Remember your grandfather climbing dozens of palm trees every single day under the harsh sunshine. He had practically no safety equipment to collect the palm nuts so that others could feed and eat pigs. The palm trees were between 25 and 30 meters tall. <em>That was truly hard work, and to be fair, it seems like you are only pretending to work in comparison.</em></p>

<p>Remember the immutability of your father’s character. “Be quiet as a lake,” he said. Many years later, you learned that staying calm when times are toughest is the way to bravery. <em>Do not overreact or take important decisions when times are tough.</em></p>

<p>Remember your father; he was at a completely different level of intelligence than you, but he never realized his full potential. He was maybe too conservative. <em>You need to work hard and take certain risks from time to time if you want to achieve something remarkable.</em></p>

<h1 id="school">School</h1>

<p>Remember that your friends were always the noblest, the least problematic in the classroom, the less popular, and so were you. There was always some bullying here and there from time to time, but you were never alone. <em>It’s important not to be totally alone.</em></p>

<p>Remember when cartoons were available for only one or two hours a day. There were only two TV channels in the country, and you waited with joy for that moment to watch them on a black-and-white screen. Waiting for things to happen forges patience and brings great pleasure. <em>Stay away from instant gratification, it is really bad because it doesn’t bring lasting joy.</em></p>

<p>Remember that a few of your preschool friends played in the streets barefoot, but you always wore shoes. You never thought about it and instead wanted the toys of the richest kids. <em>It seems we tend to always look at those who have more and forget about those who have less.</em></p>

<p>Remember when you got admitted into that middle school where the best students went. You managed to get in by a very narrow margin. Your mom was very happy. Being at that particular school opened new doors and brought opportunities. <em>Education is invaluable, and we tend to underestimate its importance at a young age.</em></p>

<p>Remember that you are not, and never were, the smartest guy in the classroom. But that was great because it made you grow faster. <em>“If you are the smartest in the room, then you’re in the wrong room,” someone said.</em></p>

<p>Remember how many writing tests you had at that middle school. You were woken up before 6:00 AM, as a student you washed the dishes and cleaned the whole school restrooms, and at the same time, you had classes the whole day for 11 days in a row (alternating with manual work at the orange plantations), for 3 years. Nevertheless, you were extremely lucky to be at that school. <em>It’s good to invest in education, it has proven to pay off very well later on.</em></p>

<h1 id="adventures">Adventures</h1>

<p>Remember that you were often the target of bullying, and you didn’t even know the real meaning of that word. So you fought other kids many, many times with your fists to defend yourself, and your pride. It took you a while to understand that a few punches here and there were better than the daily humiliation of being a coward. <em>Later on, you learned that the worst kind of aggression is passive aggression, for which fists are not effective anymore.</em></p>

<p>Remember that you were separated from your parents to work on potato plantations for 40 days under child slavery conditions. You were 11, and that seemed normal to you. <em>The perception of what is socially acceptable or not is very subjective.</em></p>

<p>Remember that day you were sitting on the rooftop with your father, staring at the stars in the dark during one of the common blackouts. It was damn hot, and you used a newspaper to fight mosquitos. He told you there was no hope for the new generations in the country and that escaping the communist regime was the only viable option. You understood what that meant for you but not for him to say that. <em>The people who love you the most are those who prioritize your happiness over their own.</em></p>

<h1 id="dreams">Dreams</h1>

<p>Remember that you grew up listening to radio stations from the US and dreamed for decades about freedom of speech and a free market where you could buy toys and food beyond the government regulations. Now you enjoy all of these, but many people still live under dictatorial regimes and die without ever experiencing that kind of freedom. <em>Don’t take freedom for granted, you have the duty to protect it.</em></p>

<p>Remember the more than 150,000 Cubans who have died crossing the Straits of Florida seeking the same freedom, opportunities, and prosperity that you enjoy today. Say it again: more than 150,000 Cubans died in the quest of freedom. That number should be enough to remind you why your parents chose to stay and resist the communist regime instead of risking their lives in the sea. <em>Do not judge the decisions of others, you are prone to miss important context.</em></p>

<p>Remember that you have a very bad short-term memory. Indeed, you never managed to memorize the multiplication tables at school but learned to quickly do sums instead. <em>The lack of some abilities can certainly unleash new capabilities.</em></p>

<p>Remember that when you were a child, you once tried to become good at playing chess but soon realized that you were not smart enough for it. You have never been smart enough at any kind of mind game (or any game at all), and you probably never will be. No big deal, though. <em>Fortunately, the majority of people don’t have any special talent, they are as clumsy as you, so it’s better to lower down personal expectations.</em></p>

<p>Remember that your parents never knew about the internet, never used a smartphone, and certainly had no idea what AI, Bitcoin, or even a PhD is. All of these things were invented quite recently, and for your parents, life was simpler. But they were happy in their way. <em>Perhaps simplicity is a fundamental component of happiness.</em></p>

<h1 id="certainties">Certainties</h1>

<p>Remember how lucky you were to be able to skip the mandatory one-year military service and spend a year working in a library instead. <em>You were fortunate, but many others have sadly wasted so much time on such unproductive things.</em></p>

<p>Remember that the less popular students later became the most successful adults. You’re biased by social conventions and sometimes become blind to perceive reality in full. Popularity is temporary. Social status can go up and down very easily. <em>Quite often, you act as an insignificant Sapiens trying to fit into the masses, it’s better to be yourself.</em></p>

<p>Remember that you studied Computer Science without a PC at hand. You did practically all your exams at university on a piece of paper. Today, this is inconceivable. Kids now have computers and many technological devices. <em>You’re an oldie-minded man compared to the new generation of tech talents.</em></p>

<p>Remember that you started learning to code very late (you were 18). Most good engineers of your age in developed countries started coding very early (before 10). <em>You are way behind your peers in terms of practice, no matter what, so do not forget that.</em></p>

<p>Remember how many exams you failed at university. Sometimes it was because you were too lazy to study, and other times because you just didn’t get it. It’s surprising that you always managed to pass them on the second chance. <em>Thank you so much my dear education system for giving me so many second chances.</em></p>

<p>Remember all the time you wasted playing video games. There was certainly very little value in doing so. You were too young to think about it. Just imagine how much value and growth you could have achieved if all that time had been invested more wisely. <em>Wasting time is very easy these days, and we often realize it when it’s already too late.</em></p>

<p>Remember the times when you said “no” to life adventures and preferred to stay home. You always regretted it afterward. There is nothing like real-life experiences. You should never miss a party with good people. There is nothing as important as human experiences. <em>All the seemingly “important” things on your agenda can wait until you finish the family dinner.</em></p>

<p>Remember all those talented people around you who took the wrong path and never recovered. Sometimes very small decisions can have a dramatic impact on the future. The best path to take is unpredictable. <em>Unfortunately, bad things happen to good people all too often.</em></p>

<p>Remember those times when you messed up, said the wrong thing, or feared making a huge mistake. Be brave enough to say a sincere “I am sorry.” <em>Apologizing from the heart is a true act of bravery.</em></p>

<p>Remember how insignificant you are in the grand scheme of things. Great achievements in this world are not the result of one person’s work but of many people working together. <em>Scientific discoveries and all other truly impactful accomplishments are distilled from the cumulative knowledge of our species.</em></p>

<p>Remember when you started participating in programming contests at university. You were not good at all. Indeed, you only reached the local contests and never solved a hard problem during the actual competition. It seems that you have a hard time every time you try to be competitive. <em>If competing, it is better to do it against yourself only.</em></p>

<p>Remember how hard it was for you to understand [KeogRemember that you (eventually) become the same as the people you are with. If you surround yourself with great people, you become great. Beware that the opposite is also true. Also, be grateful for all the people who helped you and influenced your work. Your family, friends, and PhD supervisor deserve a big chunk of the credit. <em>Acknowledging others is not only fair but also brings a lot of joy.</em></p>

<p>Remember, there is no silver bullet, and nothing truly worthwhile can be gained in this life without investing time and putting in a ton of hard work. Like when you thought others would solve your problems. For example, you believed for years that some family members would help you to emigrate, but that didn’t happen. It’s good that you realized it and rectified it in time. Inaction tends to induce failure. <em>Fortunately, the path towards true success is largely within your control.</em></p>

<p>Remember that action precedes motivation. Your human sight and misled perception of time are too limited to foresee the long-term results of your actions. It’s easy to underestimate the importance of small but incremental improvements. <em>The compounding effect of daily efforts is what makes great things happen.</em></p>

<p>Remember that big opportunities only knock a few times in life. Like when you met that amazing girl who is now your wife. It’s good that you didn’t let the opportunity slip and took the risks to move forward. <em>Opportunities will show up occasionally, but it’s up to you to be ready and take the decision to not let them go.</em></p>

<p>Remember that simple things sometimes have the biggest impact. Like when you decided to focus your <a href="https://dx.doi.org/10.13140/RG.2.2.14806.65600">MS degree</a> on applying Computer Science to popular sports, just because you were truly interested on it. It was not the fanciest research work, but it proved surprisingly rewarding and people were surprisingly interested. <em>Impactful achievements don’t necessarily have to emerge from complex subjects.</em></p>

<p>Remember how easy it is to get off track these days. It is especially easy when you haven’t set a clear goal. Like when you were freelancing after university, you knew deep down that it was temporary, but money sometimes gives you a false sense of security. <em>It’s better to stay away from easy money tracks.</em></p>

<p>Remember that there is a whole global system built around transforming your time into profit. Sadly, some of the most brilliant minds of your time are working hard to find clever ways to monetize your attention. <em>It’s good to focus on consuming less and creating more.</em></p>

<p>Remember when you decided to take that hard <a href="../blog/how-i-beat-the-ielts-academic-with-just-a-month-of-self-training.html">English test</a> that drilled down your pocket. You had only enough money for one attempt. You met that private English teacher that day, and he told you that you needed one or two years to have a chance at passing. Yet, you studied intensively for one month alone and passed the test with the bare minimum. Some people underestimate the power of self-determination. <em>So don’t let the perception of others affect your willingness to achieve great things.</em></p>

<p>Remember again how bad you are as a programmer. You still don’t know how to implement a deep neural network. And despite that, somehow you managed to deliver acceptable code when the need and pressure were high enough. <em>It seems that people get superpowers for a short time when under heavy pressure.</em></p>

<h1 id="opportunities">Opportunities</h1>

<p>Remember that it was only after (and never before) you worked really hard to get out of <a href="../blog/the-cuban-revolution-explained.html">the dictatorship</a> that real opportunities arose. You shouldn’t waste time and energy complaining about what you cannot control. <em>Taking action is absolutely necessary to make things happen.</em></p>

<p>Remember how lucky you were to get that PhD student position. You worked hard to get there, yes, but ultimately it was a mix of luck and the help of other people that led you there. <em>Especially, do not forget about all those who were much more talented than you but weren’t so lucky.</em></p>

<p>Remember the value of having a strong supportive network. The only way that progress is made in life is by involving others. Be kind to everyone, as people change very quickly. You never know who might become an important person who could assist you in the future. <em>Build strong connections and be patient, let serendipity to work its magic.</em></p>

<h1 id="fears">Fears</h1>

<p>Remember when you were stopped just before taking your first plane and got surrounded by security personnel, they asked you questions about how is that you managed to leave the country. They threatened you as if you were a terrorist, and let you go just a few minutes before departure. <em>Sometimes, you find yourself in situations where you’re totally helpless and at the mercy of others.</em></p>

<p>Remember that you started from scratch as an immigrant in a country far, far away, with a different culture and language. With very little money, you ate pasta for weeks and couldn’t pay the rent in the last month. Only a few people offered help, and you never forgot those. <em>Never miss a chance to help someone in real need.</em></p>

<p>Remember how difficult the beginning of everything is. Like when you started your PhD and felt completely behind your colleagues (some of them coming from European universities, others from large tech companies). The truth is, you really were behind. <em>You’ll always be behind w.r.t others in whatever thing you pursue in life.</em></p>

<p>Remember to keep moving forward despite all the fears and failures. In particular, get rid of the fear of looking dumb. <em>Publicly admitting a lack of knowledge is a way of showing true professionalism.</em></p>

<p>Remember what real fear truly is. Like that time when your son was one year old, and he fell on the floor, hitting his lips, with blood on his face. You ran with him in your arms, and that was the biggest fear of your whole life. <em>Most fears are not real but made up from social imagination.</em></p>

<h1 id="growth">Growth</h1>

<p>Remember the big difference between knowing, understanding, and actually doing something about it. For example, reading an article casually is one thing, reading it and then pausing for five minutes to deeply think about its ideas is another; whereas reading it, thinking about it, and then actually putting that knowledge into action is something completely different. <em>The world is full of people who know and understand many things, but only a few of them actually do something useful with all the information they accumulate.</em><sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">2</a></sup></p>

<p>Remember to be respectful of others. There is so much context you’re missing all the time. Also, your understanding of other cultures is heavily influenced by the media and other forms of propaganda and socially accepted manipulation that exist. <em>We’re all susceptible to being manipulated by opinionated social media and similar communication channels.</em></p>

<p>Remember the danger of staying too long in that cozy place called the comfort zone. It’s not really a good place to be. Instead, it pays off to self-impose controlled doses of stress from time to time, e.g., by setting strict deadlines to accomplish new achievements. Like when you decided to <a href="https://youtu.be/Xn1ShZPrw2o?si=sP5U0PATsrdHmwK1">speak</a> at Jfokus, you were concerned about the challenge but so grateful for the opportunity in the end. <em>The times when you find yourself touching the boundaries of your capabilities are the ones that make you grow in the end.</em></p>

<p>Remember to be grateful every single day for all the good things you have today. Only ten years ago, ice cream was a luxury for you, drinkable water arrived once a month in hour home, electricity shortages happened every day or week, and cheap chocolate was practically unaffordable to you. <em>Don’t forget your humble roots and never take those small, great things you have now for granted.</em></p>

<p>Remember once again that more information is not more knowledge, and more knowledge doesn’t necessarily translates into more value. This is important. <em>What matters is the value you provide, and gathering information and knowledge are just necessary tools to produce such value.</em></p>

<p>Remember the many times you doubted your capacity to do a PhD. There are always doubts surrounding the great things in life. You managed to <a href="https://youtu.be/Ub684G_aM5Q?si=5Ow61XFEGmtP9ZZw">finish</a> it against the odds. <em>Maybe every time you doubt your capacity to do something, it is actually a good sign that you’re growing.</em></p>

<p>Remember the importance of connecting the dots. You’re not great at digging deep into something and tend to overestimate the value of niche technical knowledge. <em>Some of the greatest things are based on assembling existing ideas in a simple and novel way.</em></p>

<p>Remember that you are certainly too small as an individual to make any fundamental change in this large and complex world we live in. However, you can still inspire others to change their lives just as you did through scientific research. <em>Remember that helping people is what really makes you happy.</em><sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">3</a></sup></p>

<p>Remember that you (eventually) become the same as the people you are with. If you surround yourself with great people, you become great. Beware that the opposite is also true. Also, be grateful for all the people who helped you and influenced your work. Your family, friends, and PhD supervisor deserve a big chunk of the credit. <em>Acknowledging others is not only fair but also brings a lot of joy.</em></p>

<p>Remember that you never achieved anything for the sake of getting recognition. To be fair, most things you have done so far have been for your own benefit. It’s always a good time to give back to the world for being so generous with you. <em>It’s always a good time to send your luck in the other direction.</em></p>

<p>Remember to be more understanding and forgiving about the imperfections of the social system you live in. Sometimes people are hostile for your own benefit, to educate you. Like your parents were when they punished you, like formal education is by forcing you to do frequent exams, and like your PhD supervisor was by setting high quality standards. <em>Until you know the true purpose, it is unfair to make negative judgments about the established systems.</em></p>

<p>Remember that being positive works wonders. But be careful of showing too much energy and enthusiasm. Others could perceive such behaviors as pretentious. <em>Even for good things, there are certain limits.</em></p>

<p>Remember that right now there is so much suffering and injustice in the world. You’re very lucky to be far from the terrible things going on out there, such as wars and military dictatorships. You’re living a happy life in a great country with freedom and human rights. <em>You have so much now, don’t forget that.</em></p>

<p>Remember that in the end, you will not remember your failures at all. Only the successes will be remembered. Only the happy times with great people will be remembered. <em>There are always people involved in your good memories, so think about it.</em></p>

<hr />

<p style="text-align: center;">TO BE CONTINUED THE NEXT TIME I'M PRETENTIOUS</p>

<h1 id="footnotes">Footnotes</h1>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p>BTW, I don’t believe that throwing away shiny rocks is a good way to solve the fundamental problems of humanity. Instead, I support <a href="https://www.effectivealtruism.org/">effective altruism</a>, increasing opportunities, and creating a system that rewards good behavior. I also believe that the higher a person is in the social hierarchy, the more chances they have to create opportunities for others and make a larger impact. Thus, personal development is a driver for good. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>Perhaps a good approach is to force ourselves to create something valuable from all the information we consume. I believe that by doing so, I’ll eventually become more selective about what I spend my time on. This is because I’ll be aware of the time I will need to invest later in creating something useful from it. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>I strongly believe in the possibilities of science as a life changer. Right now, there are many opportunities in academic research for people around the world to change their lives. In fact, pursuing academic research has changed my life! <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>César Soto Valero</name><email>cesarsotovalero@gmail.com</email></author><category term="career" /><summary type="html"><![CDATA[A few days ago, I had a conversation about my ambitions for the future. At one point, I mentioned that I would like to achieve something remarkable and gain public recognition for it. In retrospect, my answer was quite pretentious. After reflecting deeply on this topic, I realized that what truly drives me is the desire to help and inspire others so they can achieve great things. This blog post lists all the thoughts that crossed my mind as I came to this conclusion.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.cesarsotovalero.net/img/posts/2024/2024-06-07/remember-your-roots_cover.jpg" /><media:content medium="image" url="https://www.cesarsotovalero.net/img/posts/2024/2024-06-07/remember-your-roots_cover.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">The Last Paper’s Myth</title><link href="https://www.cesarsotovalero.net/blog/the-last-paper-myth.html" rel="alternate" type="text/html" title="The Last Paper’s Myth" /><published>2024-05-05T00:00:00-07:00</published><updated>2025-07-29T11:52:25-07:00</updated><id>https://www.cesarsotovalero.net/blog/the-last-paper-myth</id><content type="html" xml:base="https://www.cesarsotovalero.net/blog/the-last-paper-myth.html"><![CDATA[<aside class="youtube">
        <a href="https://www.youtube.com/watch?v=-3txwKTHe4Y"><div class="box">
        <img src="https://i.ytimg.com/vi/-3txwKTHe4Y/mqdefault.jpg" alt="YouTube video #-3txwKTHe4Y" />
        <div class="play">
          <img src="/img/icons/youtube-play20px.svg" alt="Play Video" class="youtube-logo" />
        </div>
        </div></a>
        <div>The Last Paper’s Myth: PhD Student Expectations vs. Reality at the End of a PhD;
        28 July 2024.</div></aside>

<p>Last week, I met a colleague from a renowned research lab.
She is currently doing the last year of her PhD.
I asked the usual:
“<em>So, how are you doing?</em>
<em>Writing your thesis?</em>”
She replied with a wistful smile: “<em>Not started yet, I’m working on my very last paper, or so I hope…</em>”
Her face couldn’t disguise a mix of fatigue and resignation.
It was clear to me that she was under serious pressure to finish <strong>just one last paper more</strong>.
Certainly, I know she has many papers published already, so why the need for one more?
Why imposing such unnecessary pressure at the end of her PhD?
These are two reasonable questions with no easy answers.
Nevertheless, I’m not surprised.
I understand the situation because I have been there too.
It’s not the first time I encounter this scenario, and it won’t be the last one either.
I call it the “<strong>last paper’s myth</strong>,” and it’s a recurrent issue among PhD students who are about to graduate.
A rough and pervasive experience in academia, to be honest.
I think this phenomenon reflects a broader conflict of interests between the science makers and their stakeholders.
So, in this blog post, I attempt to uncover this issue from the perspective of PhD students.
I focus on the expectations versus reality at the end of the PhD journey.
In addition, I dive into a more philosophical discussion on the pervasiveness of this phenomenon, beyond academic circles.
My intention is to provide actionable insights for PhD students and researchers on how to identify, address, and navigate this particular pressure.
Whether you’re in the last year of your PhD and plan to stay in academia, or you decide to engage in other professional endeavors, this post could be useful.
So let’s start by discussing expectations.</p>

<figure class="jb_picture">
  



<img width="100%" style="border: 0px solid #808080;" src="/assets/resized/aspuden-metro-terminal-640x373.png" alt="Not free to go" data-srcset="/assets/resized/aspuden-metro-terminal-640x373.png 640w,/assets/resized/aspuden-metro-terminal-768x448.png 768w,/assets/resized/aspuden-metro-terminal-1024x597.png 1024w,/assets/resized/aspuden-metro-terminal-1366x797.png 1366w,/assets/resized/aspuden-metro-terminal-1600x933.png 1600w," class="blur-up lazyautosizes lazyload" />
  <figcaption class="stroke"> 
    &#169; Do you really think you're free to go? Not too fast, there is always a chance to give you one more block to carry on. Sculpture of a penguin carrying a cube at the <a href="https://en.wikipedia.org/wiki/Aspudden_metro_station">metro station of Aspudden</a>, in Stockholm.
  </figcaption>
</figure>

<h1 id="expectations">Expectations</h1>

<p>For most PhD students, the doctoral journey is envisioned as a quest for <a href="../blog/seven-reasons-to-go-for-a-phd-in-computer-science.html">knowledge and further professional opportunities</a>.
It’s a personal challenge, where intellectual curiosity meets the rigorous demands of <a href="https://en.wikipedia.org/wiki/Scientific_method">scientific research</a>.
This idyllic vision of fulfilled scientists, however, has evolved dramatically over the last years.
The modern high education system is currently shaped by the dual pressures of squeezing a little more out of the public money pipeline, and ranking high in <a href="https://www.topuniversities.com/university-subject-rankings/engineering-technology">global publication metrics</a> to get more money in return.<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>
It’s a <strong>vicious cycle</strong>, I know.
Such deviation from its original purpose influences not only the students and supervisors’ expectations alike, but also profoundly impact the whole experience for PhD students.</p>

<pre><code class="language-mermaid">%%{init: {'theme':'base'}}%%
flowchart TB
    A(⚙️ Higher Education System) --&gt; B[📈 Maximize Public Funding]
    B --&gt; C[📈 Increase Publications &amp; Impact]
    C --&gt; D(🏦 Public Funds)
    D -- 💰cash flow --&gt; A
</code></pre>

<p>I still remember entering the final year of my PhD.
It was such a relief to think about the happy idea of successfully ending the whole thing.
The need to take a break after years of hard work seemed like a priority to me.
By that time, I had the expectation that my primary task was to synthesize my research into a thesis and call it all done.
I guess this hope is shared among most PhD students in their final round.
Honestly, I was in a comfortable position.
Having published <a href="../publications.html">a few papers</a> targeting a single knowledge gap, I felt that I had done a <em>real</em> PhD.
These papers were the core of my scientific contributions.
They seemed to push the boundaries of human knowledge just the necessary little bit.
In such a position, after five years of struggles with the whole process, it was reasonable for me to assume that <strong>I should now focus only on writing my thesis</strong>.
All the tears and sweat poured into the research were already done, so it was time to wrap it up.
Isn’t it?
Well, not so fast.
It turns out that my PhD contributions opened new research avenues and opportunities… for writing just more papers!
A hard-to-resist temptation for workaholic academics.</p>

<aside class="quote">
    <em>“A successful PhD requires finding the delicate balance between the students' academic output and their supervisors' expectations regarding the number and quality of papers.”</em>
</aside>

<p>We need to understand that the end of the PhD journey is often a battle of contending expectations.
On one hand, PhD students reach the end of the line exhausted and eager to finish the PhD.
On the other hand, there are the expectations of supervisors and the heavy pressures imposed by the academic institutions above them.
After a certain number of papers,<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup> supervisors recognize the increasing proficiency and output of experienced students who are about to graduate.
These students are a valuable resource that took years to shape.
Why let them go so easily when they seem at their peak?
That’s when the pressure to produce “one last paper” kicks in.</p>

<blockquote>
  <p>“The last paper’s myth emerges once academic supervisors realize that the student is about to get the PhD and leave. It’s one last stronghold made by supervisors to capitalize on the time and effort they have invested in their students’ scientific training.”</p>
</blockquote>

<p>Students in this position are often led to believe that submitting just one more paper will fulfill their academic obligations.
This belief stems from an academic culture that prioritizes work output over well-being and even mental health.
As a result, the most capable students frequently find themselves the most burdened. 
The myth of the last paper traps PhD students in their final year into mistakenly thinking that they don’t have enough results to defend.
It leads to a continuous cycle of craving to produce “just one more paper,” under the belief that this will finally conclude their academic responsibilities. 
Unfortunately, this is far from reality.</p>

<h1 id="reality">Reality</h1>

<aside class="quote">
    <em>“How do you prove that a paper is good? Hard to tell.”</em> 
</aside>

<p>The idea that a specific number of papers could determine the conclusion of a PhD journey is a misconception.
It just doesn’t work like that.
In science, measuring productivity and impact through raw metrics such as the number of papers published or citations <a href="https://academic.oup.com/spp/article/44/2/246/2525560">has proven to be extremely difficult</a>.
The quality of research, the impact of findings, and the depth of contributions are subjective and vary greatly from one individual’s perspective to another.
Moreover, the academic machinery is designed to perpetuate the publication cycle.
Like capital is to capitalism, citations and prestige are all for academic institutions.
The system maximizes for it, with students bearing the brunt of the workload.
This is the reality of how the scientific world is organized these days, and <strong>there is no way around it</strong>.</p>

<p>At a more individual level, the push for additional papers often reflects a conflict of interests.
The PhD student’s need to obtain their diploma and transition to professional roles clashes with the supervisor’s desire for continued academic output to enhance their personal prestige.
This misalignment leads to the “last paper myth” reality.
Not to forget that academic supervisors already decided (probably a long time ago) to stay in academia, so they have a vested interest in boosting their publication records.<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>
Generally, a young supervisor with a growing group may exert more pressure for research results than a more established researcher with a large group.
Young supervisors rely more heavily on students’ results for their own career progress and often work more closely with their students.</p>

<aside class="quote">
    <em>“The pressure to produce "one last paper" can unnecessarily extend the duration of PhD programs, adding significant financial and emotional strain on students.”</em>
</aside>

<p>Recent studies indicate that while some <a href="https://link.springer.com/article/10.1007/s10734-019-00473-6">students benefit from the additional push</a>, getting more publications and enhancing their resumes, others feel <a href="https://link.springer.com/article/10.1007/s12529-020-09867-8">exploited and stretched too thin</a>.
The pressure to produce “one last paper” can unnecessarily extend the duration of PhD programs, adding significant financial and emotional strain on students.
It’s worth noting that upon completing their PhD, students typically have papers under review.
Clearly, there is no real pressure to publish more papers if staying in academia is not the goal.
However, even when students can opt out of this cycle, the sunk cost of throwing away so much time and effort, and (sometimes) the professional benefits of having published papers, often compel them to see the process through.
This unplanned work involves receiving feedback from peer reviewers, addressing their comments, and submitting revisions, a process that can extend several months of <strong>unpaid work</strong> post-graduation.
Personally, I would never let one of my written papers go unpublished, but I know about the existence of large paper cemeteries.
The “last paper myth” is one of the reasons why these cemeteries exist.
This reality seems unbreakable, but there are ways, sometimes, to circumvent and survive the system with a few fewer wounds.</p>

<h1 id="countermeasures">Countermeasures</h1>

<p>To address the “last paper myth” and its associated pressures, several strategies can be implemented at both the individual and institutional levels.
These strategies aim to empower students, promote dialogue, and foster better communication practices within a supportive academic environment.
For example, at the individual level, I think that students could benefit from setting clear boundaries with their supervisors, establishing realistic expectations for their final year, and seeking support from peers and mentors.
Asking “What are you going to do next after completing the PhD?” can be a good way to determine if having more papers could really be worth it or not.
By communicating openly about their needs and concerns, students can navigate the challenges of the final year more effectively.
At the institutional level, academic departments can implement policies that promote transparency, fairness, and accountability during the last year of the PhD program.
These policies could include <strong>clear guidelines</strong> in terms of schedule, mentorship responsibilities, and student rights.
By creating a supportive and equitable environment for PhD students, institutions can help mitigate the negative effects of the “last paper myth” and foster a culture of academic integrity and well-being.</p>

<aside class="quote">
    <em>“Academic supervisors tend to act in their own interests, and provoking their students to burn out only results in wasted time and resources for them.”</em> 
</aside>

<p>If you find yourself writing papers when you were supposed to be writing your thesis, I suggest taking a step back and reevaluating your priorities.
It could be that you are procrastinating on the thesis because it is a daunting task, and writing papers is a more manageable way to feel productive.
If it isn’t the case, you can let your supervisor know that you’re really stressed and that you need to focus on your thesis.
Take into account that researchers are extremely intelligent people.
They know very well how much they can push. 
They are perfectly aware that if pushing too much, they could provoke a burnout that is not beneficial for anyone.
So, it is in their best interest to keep you in good shape.</p>

<p>Settled accounts keep old friends.
I would try to negotiate upfront the number of papers that are necessary and keep to this number.
Let your supervisor know.
It might be worth trying something like this:</p>

<figure class="highlight"><pre><code class="language-plaintext" data-lang="plaintext">Subject: Summary of our last meeting

Hi [Supervisor's Name],

I wanted to summarize our discussion from our last meeting regarding
the number of papers I need to publish before completing my PhD. We
agreed that I will aim to publish [Number of Papers] papers by the end
of my program. I understand that this number may be subject to change
based on the progress of my research and feedback from reviewers. Please
let me know if you have any additional expectations or if you would like
to discuss this further.

Cheers,
[Your Name]</code></pre></figure>

<p>In most cases, supervisors will appreciate the proactive approach and the clarity of communication.
However, sometimes they might see this as a preventive sign of potential future conflicts.
It could be seen as a good initiative or a red flag instead.
It depends on the kind of relationship you have established.
So be careful.</p>

<p>Another strategy is to <strong>demonstrate extreme focus and engagement</strong> with the thesis writing process.
I would recommend postponing all other commitments, even personal ones, and focus solely on your thesis.
More importantly: let everyone know!
The trick here is to show such a high level of enthusiasm that it naturally blocks any attempts to push you towards other commitments.
By making it clear that you are in “thesis writing mode,” and therefore unable to take on additional tasks, you can create a shield that will protect from any pressure to write that last “unexpected” paper.</p>

<h1 id="beyond-academia">Beyond Academia</h1>

<aside class="quote">
    <em>“Individuals in positions of power have a natural tendency to exercise their authority over subordinates in ways that prioritize their own personal ambitions.”</em> 
</aside>

<p>The dynamics of expectations vs. realities reflect broader patterns of authority and control in various professional environments beyond academia.
I think individuals in positions of power have a natural tendency to exercise their authority over subordinates in ways that prioritize their personal gains.
In some cases, this is good for everyone, but most of the time it is not beneficial for the well-being and professional development of those ranked lower on the career ladder.
This relationship sets a complex stage for ambitious leadership and cravings for achievements across different sectors.
In other words, the pressure to perform and deliver continuous results in all layers of the professional world significantly impacts employees’ career trajectories, personal lives, and mental health.</p>

<p>In the corporate world, for example, talented employees who demonstrate high levels of productivity and skill often become invaluable to their managers.
This value paradoxically leads to these prominent employees ending up with heavier workloads.
As project deadlines get closer, managers tend to put extra pressure on these employees to guarantee satisfactory results.
This mirrors the academic pressure to publish one more paper before a student graduates.
So, the “last paper’s myth” could then be extrapolated as the “last workload push” in most corporate environments.
In some cases, the urge for rapid product development can lead to questions about the true value of such advancements.
Are these efforts genuinely aimed at improving the product, or merely at escalating the career ladder and gaining more visibility?
I think the latter is more likely.</p>

<blockquote>
  <p>“The focus on speedy outcomes, combined with personal cravings, often results in products that are less refined, potentially leading to a glut of features that diminish user satisfaction and detract from the core utility of the product.”</p>
</blockquote>

<p>Sometimes, the never-ending cycle of meeting targets, coupled with the lack of adequate support from the organization, places immense stress on individuals.
The resemblance to the academic cycle of writing, submission, and revision is stark.
It shows the human cost of high productivity.
The extra push on deadlines, whether in academia or industry, under the guise of efficiency, profit, or scholarly output, is an issue that calls for a critical examination of ethical practices within all professional fields.
I think there is a real need for institutions (corporate, academic, or otherwise) to reassess their policies and practices to ensure the promotion of more supportive work environments.</p>

<h1 id="conclusions">Conclusions</h1>

<p>We have delved into what I call the “last paper’s myth,” a situation that drives academic supervisors to exercise extra pressure in the last year of their PhD students.
It’s a matter of contending interests, where the students’ need for completion clashes with the academic system’s mechanisms.
The pressure to produce “one last paper” can unnecessarily burden students, extending the duration of PhD programs and adding significant financial and emotional strain.
I think <strong>increasing self-awareness</strong> on this issue and <strong>setting clear boundaries</strong> to protect work-life balance can help students navigate the challenges of the final year more effectively.
Interestingly, the most capable PhD students face the highest risk.
So, the mere fact of being pressured at the end is ofter (surprisingly!) a synonymous with success.
It’s a paradox that extends beyond academia.
For example, in the corporate world, talented employees often find themselves burdened with heavier workloads as they become more valuable to their managers.
In any case, I believe that clear and measurable expectations should be put in place to ensure a happy leaving experience, regardless of the professional field.</p>

<h1 id="external-resources">External Resources</h1>

<ul>
  <li><a href="https://rachaellappan.github.io/surviving-final-year/">:globe_with_meridians: Surviving the final year</a></li>
  <li><a href="https://www.nature.com/articles/s41562-019-0685-4">:globe_with_meridians: Forcing PhD students to publish is bad for science</a></li>
</ul>

<h1 id="footnotes">Footnotes</h1>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p>“Publish <em>or</em> perish” is the motto. There is just not workaround for it. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>The expected number of papers necessary to get the PhD degree varies greatly between supervisors, universities, and research fields. In Computer Science this number ranges between 3 and 5 papers for most respected institutions. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>Not to forget that academic supervisors are technically <em>forced</em> to play the academic game. They have (in most cases) temporary contracts, are underpaid, and have a heavy workload. This is such a dense topic that deserves a blog post on its own. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>César Soto Valero</name><email>cesarsotovalero@gmail.com</email></author><category term="science" /><summary type="html"><![CDATA[The "last paper myth" emerges once academic supervisors realize that a student is about to earn their PhD and leave. It's one last stronghold for supervisors to capitalize on the time and effort they have invested in their students' scientific training. This relentless push not only detracts from the true purpose of academic research but also imposes significant personal and mental strain on the most successful students. This article explores the expectations versus reality at the end of the PhD journey and provides actionable insights for students on how to identify, address, and navigate this particular pressure.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.cesarsotovalero.net/img/posts/2024/2024-05-05/aspuden-metro-terminal-cover.png" /><media:content medium="image" url="https://www.cesarsotovalero.net/img/posts/2024/2024-05-05/aspuden-metro-terminal-cover.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">On the Rise, Decline, and Persistence of Web Technologies</title><link href="https://www.cesarsotovalero.net/blog/the-evolution-of-the-web-from-html-to-webassembly.html" rel="alternate" type="text/html" title="On the Rise, Decline, and Persistence of Web Technologies" /><published>2024-04-27T00:00:00-07:00</published><updated>2025-07-29T11:52:25-07:00</updated><id>https://www.cesarsotovalero.net/blog/the-evolution-of-the-web-from-html-to-webassembly</id><content type="html" xml:base="https://www.cesarsotovalero.net/blog/the-evolution-of-the-web-from-html-to-webassembly.html"><![CDATA[<p>I’ve been doing some frontend web development lately as part of my daily job.<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>
Along the way, I’ve been thinking on how the whole thing is built under the hood. 
The web technologies that we all use every day have changed and evolved over time.
While digging into this topic, I realized that some of the underlying technical solutions that fuel the World Wide Web are really brilliant, while others are still a mess.
From an engineering perspective, there are many lessons we can learn from it.
So I decided to write about it in this post.
What follows is my modest attempt to do so.
As a disclaimer, this blog post is not about <a href="https://en.wikipedia.org/wiki/History_of_the_Internet">the history of the internet</a>, which has already been well documented.
Instead, I focus on the different architectures and patterns that have radically changed the way web technologies are developed.
This includes new paradigms that have opened up possibilities for what was previously impossible, like Single Page Applications (SPAs) and AJAX.
From those that became obsolete, like Java Applets, to those that have stood the test of time, like Static HTML, to the recent emergence of WebAssembly and the ongoing attempt to decentralize the web.
If you are a <del>nerd</del> engineer, a web developer, or just curious about how the web works internally, you’re likely to find this post interesting.
Those who don’t know tech history are doomed to repeat past engineering mistakes.
This is gonna be a long one, so grab a coffee and let’s go!</p>

<figure class="jb_picture">
  



<img width="" style="border: ;" src="/assets/resized/infinite-park-640x373.jpg" alt="The World Wide Web, and infinite park of data continuously streamed" data-srcset="/assets/resized/infinite-park-640x373.jpg 640w,/assets/resized/infinite-park-768x448.jpg 768w,/assets/resized/infinite-park-1024x597.jpg 1024w,/assets/resized/infinite-park-1366x797.jpg 1366w,/assets/resized/infinite-park-1600x933.jpg 1600w," class="blur-up lazyautosizes lazyload" />
  <figcaption class="stroke"> 
    &#169; The World Wide Web is a seeming infinity data dumping and pumping platform. It has been transformed and evolved at unprecedented speed over the last 30 years. The picture shows an art piece titled "Infinite Park" exposed at <a href="https://maps.app.goo.gl/75AGyCg5QpzjQeFR8">Stockholm's Paradox Museum</a>.
  </figcaption>
</figure>

<h1 id="the-world-wide-web">The World Wide Web</h1>

<p>The <a href="https://en.wikipedia.org/wiki/World_Wide_Web">World Wide Web (WWW)</a>, as we know it today, is basically a vast collection of data accessible through <strong>the internet</strong>.
The internet ensures that the data (i.e., <code class="language-plaintext highlighter-rouge">0</code>s and <code class="language-plaintext highlighter-rouge">1</code>s), is constantly transmitted from servers to clients and vice versa.
Most users perceive the web through the data that is rendered by <strong>web browsers</strong>.
Browsers are software applications that rely on various <strong>web technologies</strong> to transform the data into more readable and human-friendly web pages.
These three core layers (highlighted above with bold font) are responsible for keeping the web alive.
This post dives deep into the last layer: web technologies.
But before that, let’s take a look at the big picture.</p>

<pre><code class="language-mermaid">%%{init: {'theme':'base'}}%%
graph TB;
subgraph "World Wide Web"
  B -- data --&gt; C[💻 Web Browsers] 
  A[(🌐 Internet)] -- data --&gt; B([🔧 Web Technologies])
  C -- data --&gt; B
  B -- data --&gt; A
end
</code></pre>

<h2 id="the-internet">The Internet</h2>

<figure class="badge"><a href="https://www.lk.cs.ucla.edu/data/files/Kleinrock/Information%20Flow%20in%20Large%20Communication%20Nets.pdf"><img src="/img/badges/leonard-kleinrock-dissertation.png" style="width:140px;max-width:100%;" alt="badge" /></a></figure>

<p>Most sources agree that the internet was created in the 1960s by <a href="https://en.wikipedia.org/wiki/ARPANET">ARPANET</a>, a research project supported by the <a href="https://en.wikipedia.org/wiki/DARPA">DARPA</a> agency of the United States.<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>
The concept of “the internet” itself dates back to the first paper about packet switching written in 1961 by the american engineer <a href="https://www.lk.cs.ucla.edu/index.html">Leonard Kleinrock</a>.
In his PhD thesis titled “<a href="https://historyofinformation.com/detail.php?id=788">Information Flow in Large Communication Nets</a>,” he proposed <strong>a network of computers</strong> that could send data to each other. 
That’s exactly what the modern internet does at its core!</p>

<p>The internet and the web, while often used interchangeably, refer to different concepts.<br />
The internet is the global network of interconnected computers that can communicate with each other by agreeing on using the same <strong>data transfer protocol</strong>.
For example, the <a href="https://en.wikipedia.org/wiki/Internet_Protocol">Internet Protocol (IP)</a> is the most used to transmit data across the internet.
Here the sense of “global” is key, as it transcends geographical barriers between countries (a more restrictive version of the internet is called “intranet” instead).<sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">3</a></sup></p>

<p>The internet is built thanks to physical infrastructure <a href="http://thescienceexplorer.com/technology/our-wi-fi-world-internet-still-depends-undersea-cables">like submarine cables</a>, which are crucial for its operation.
The web, on the other hand, refers to the information that can be accessed through the internet.
It consists of the protocols and technologies designed to support data transfer between servers.
Essentially, the web is just one of several services that utilize the internet infrastructure to disseminate information.
But note that there are other services as well, such as file transfer services, email, VoIP, or streaming platforms.</p>

<figure class="jb_picture">
  



<img width="50%" style="border: 1px solid #808080;" src="/assets/resized/submarine-cable-map-640x414.png" alt="Submarine cable map" data-srcset="/assets/resized/submarine-cable-map-640x414.png 640w,/assets/resized/submarine-cable-map-768x497.png 768w,/assets/resized/submarine-cable-map-1024x662.png 1024w,/assets/resized/submarine-cable-map-1366x883.png 1366w,/assets/resized/submarine-cable-map-1600x1035.png 1600w,/assets/resized/submarine-cable-map-1920x1242.png 1920w," class="blur-up lazyautosizes lazyload" />
  <figcaption class="stroke"> 
    Figure 1. The internet, originated in 1961, is now mostly driven by submarine cables, not satellites. This could be a good point to justify the mess of cables in my desk, the whole world cannot get rid of them! <a href="https://www.submarinecablemap.com/">Source</a>
  </figcaption>
</figure>

<p>The idea of a <strong>global internet</strong> has been around since the early 1970s. 
However, initially accessing documents remotely involved complex processes using services such as <a href="https://en.wikipedia.org/wiki/File_Transfer_Protocol">FTP</a>, <a href="https://en.wikipedia.org/wiki/Network_News_Transfer_Protocol">NNTP</a>, and the <a href="https://en.wikipedia.org/wiki/Gopher_(protocol)">Gopher protocol</a>. 
These protocols, still in use today, provide a simple directory structure from which users can navigate and select files to download, but they do not support a direct visualization of the information being transferred. 
This limitation highlighted the need for a new protocol that would enable users to render documents directly through dedicated software applications.
It was this (probably) the main incentive that led to the creation of the World Wide Web (WWW). 
Today’s WWW is predominantly made up of HTML documents and other resources, interconnected through <a href="https://en.wikipedia.org/wiki/URL">URLs</a>.
These resources are accessed and retrieved using a wonderful piece of software called “web browser,” which communicates back and forward with the internet servers and displays content to the end users.</p>

<h2 id="web-browsers">Web Browsers</h2>

<p>In the early 1990s, the development of web browsers rapidly transformed the internet.<sup id="fnref:6"><a href="#fn:6" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>
Sir <a href="https://en.wikipedia.org/wiki/Tim_Berners-Lee">Tim Berners-Lee</a>,<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">5</a></sup> at CERN, created the first web browser and web editor named “<a href="https://en.wikipedia.org/wiki/WorldWideWeb">WorldWideWeb</a>,” later renamed “Nexus,” in 1990 (another poof that long camel cased names are a bad idea in general).
This browser initially ran on the <a href="https://en.wikipedia.org/wiki/NeXTSTEP">NeXTSTEP operating system</a> (yeah, that’s the OS of the NeXT Computer founded by Steve Jobs, which didn’t go anywhere, as neither the browser did).
Despite its failure, it laid the groundwork for building public web by allowing users to both browse and edit web pages. 
By February 1992, Berners-Lee’s team at CERN released the <a href="https://en.wikipedia.org/wiki/Line_Mode_Browser">Line Mode Browser</a>, the first multi-platform browser.
Between 1992 and 1995, several browsers such as Viola, Mosaic, Cello, Netscape Navigator, Opera, and Internet Explorer 1.0 were launched.
The emergence of diverse browsers supported in different platforms dramatically opened up internet access to anyone with a computer and an internet connection.</p>

<figure class="jb_picture">
  



<img width="50%" style="border: 1px solid #808080;" src="/assets/resized/nexus-web-browser-640x475.png" alt="Submarine cable map" data-srcset="/assets/resized/nexus-web-browser-640x475.png 640w,/assets/resized/nexus-web-browser-768x571.png 768w,/assets/resized/nexus-web-browser-1024x761.png 1024w," class="blur-up lazyautosizes lazyload" />
  <figcaption class="stroke"> 
    Figure 2. Nexus, the first web browser, as it was in 1994. It didn't make it, despite its cool name. <a href="https://digital-archaeology.org/the-nexus-browser/">Source</a>
  </figcaption>
</figure>

<aside class="quote">
    <em>“Web browsers are by far the most successful software application ever created in the history of computing.”</em>    
</aside>

<p>Over the last decades, web browsers have evolved to become complex software systems (e.g., Firefox and Chrome reported having roughly <a href="https://openhub.net/p/firefox">30M</a> and <a href="https://openhub.net/p/chrome/analyses/latest/languages_summary">40M</a> lines of code in total, respectively).
Modern browsers are capable of handling not just text and images, but also running complex applications like real-time video games, 3D animations, and PDF documents.
It required a lot of engineering effort to make this possible through web technologies.
For example, the <a href="https://v8.dev/">V8 JavaScript Engine</a> included in Chrome is renowned for its high performance due to just-in-time (JIT) compilation, efficient garbage collection, and optimizations for real-time execution in web browsers.
Web browsers are by far the most successful software application ever created in the history of computing.</p>

<h2 id="web-technologies">Web Technologies</h2>

<p>Initially created in the 1990s to link research papers via “hyperlinks,” <strong>the web</strong> quickly grew beyond its academic origins. 
The world’s first website and web server were hosted at CERN.<sup id="fnref:7"><a href="#fn:7" class="footnote" rel="footnote" role="doc-noteref">6</a></sup>
On April 30th of 1993, the CERN issued a <a href="https://home.cern/science/computing/birth-web/licensing-web">public statement</a> relinquishing all intellectual property rights to the World Wide Web.
Thus making it freely accessible without any fees.
This decision fostered an environment of unrestricted expansion and innovation in web technologies.
Berners-Lee left CERN in October 1994 to form the <a href="https://www.w3.org/">World Wide Web Consortium (W3C)</a>, which today includes around 500 member organizations from around the world.
The W3C works to create standards for web development and serves as a forum for discussing web usage and development globally.</p>

<figure class="jb_picture">
  



<img width="50%" style="border: 1px solid #808080;" src="/assets/resized/first-www-proposal-640x537.png" alt="Tim Berners-Lee's proposal for the World Wide Web" data-srcset="/assets/resized/first-www-proposal-640x537.png 640w,/assets/resized/first-www-proposal-768x645.png 768w,/assets/resized/first-www-proposal-1024x860.png 1024w,/assets/resized/first-www-proposal-1366x1147.png 1366w," class="blur-up lazyautosizes lazyload" />
  <figcaption class="stroke"> 
    Figure 3. Figure in the first page of Tim Berners-Lee's proposal for the World Wide Web in March 1989. <a href="https://www.w3.org/History/1989/proposal.html">Source</a>
  </figcaption>
</figure>

<p>I would like to remark that since its massification, the web landscape has undergone dramatic transformations due to the strategic pushes made by companies and nations to standardize certain web technologies. 
These technologies have added new capabilities that allow web applications to scale and operate differently from traditional software. 
For example, today’s web applications do not require installation or updates. 
They simply need a device with a web browser to run.
This has led to the rise of web applications that can be accessed from any device with an internet connection, making them more accessible thanks to the wide spread of mobile phones.</p>

<p>Its rapid development has led to the rise and decline (almost obsolescence) of various technologies over time. 
In particular, there’s still an ongoing debate about whether to place the execution logic that consumes most of the computing power on the server or in the client. 
This decision impacts how applications on the web are built and interacted with. 
For example, Server-Side Rendering (SSR) is a technique where the server generates and processes the HTML content that is sent to the client (e.g., using any kind of programming language like Java, C, or Ruby). 
On the other hand, Client-Side Rendering (CSR) is a technique where the client’s browser generates the HTML content, mostly using JavaScript.
SSR offers faster initial page loads and SEO benefits by pre-rendering HTML on the server.
CSR provides a more dynamic and interactive user experience at the expenses of slower initial loads and SEO challenges due to content being rendered on the client’s browser.
The choice between SSR and CSR is a trade-off that developers must consider when building web applications.</p>

<h1 id="timeline-of-web-technologies">Timeline of Web Technologies</h1>

<p>The following timeline illustrates the evolution of web technologies from the 1990s to the present, highlighting key developments that have changed the modern web landscape.</p>

<pre><code class="language-mermaid">%%{init: {'theme':'base'}}%%
timeline
  section 1990s
    1990s to present: Static HTML Pages
                    : Server-Side Rendering
                    : Client-Side Rendering
    1995s to 2010: Java Applets and Plugins
    1995 to present: CSS and Dynamic Web Design
  section 2000s
    2005 to present: SPA and AJAX
    2008 to present: WebSockets
  section 2010s
    2017 to present: Decentralized Web
                   : WebAssembly    
</code></pre>

<p>In the following sections, I delve into each of these technologies in more detail.</p>

<h2 id="static-html-pages">Static HTML Pages</h2>

<p>The first version of HTML was written by Tim Berners-Lee <a href="https://www.washington.edu/accesscomputing/webd2/student/unit1/module3/html_history.html">in 1993</a>.
HTML is a <a href="https://en.wikipedia.org/wiki/Markup_language">markup language</a> based on <a href="https://en.wikipedia.org/wiki/Standard_Generalized_Markup_Language">SGML</a> that structures content by defining elements like headings, paragraphs, lists, and links using tags like <code class="language-plaintext highlighter-rouge">h1</code>, <code class="language-plaintext highlighter-rouge">p</code>, <code class="language-plaintext highlighter-rouge">ul</code>, and <code class="language-plaintext highlighter-rouge">a</code>, respectively.
These elements compose what is known as the <a href="https://www.w3.org/TR/WD-DOM/introduction.html">Document Object Model (DOM)</a>, a tree-like structure that browsers use to render web pages.</p>

<p>In the dawn of the 1990s, the World Wide Web was essentially HTML pages connected via hyperlinks without any styling or data persistence.
This early iteration of the web featured pages that were predominantly text-based, but later evolved to include images, GIFs, and other media.
The HTML supported them by adding more tags like <code class="language-plaintext highlighter-rouge">img</code>, <code class="language-plaintext highlighter-rouge">video</code>, <code class="language-plaintext highlighter-rouge">audio</code>, and <code class="language-plaintext highlighter-rouge">canvas</code>.
The simplicity of HTML was key to its success, making it possible for virtually anyone to create and publish content on the web without needing sophisticated technical skills.<sup id="fnref:5"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">7</a></sup></p>

<p>The following is an example of a simple HTML page that displays a heading title, a paragraph, and an image:</p>

<figure class="highlight"><pre><code class="language-html" data-lang="html"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="code"><pre><span class="cp">&lt;!DOCTYPE html&gt;</span>
<span class="nt">&lt;html&gt;</span>
<span class="nt">&lt;head&gt;</span>
  <span class="nt">&lt;title&gt;</span>My First Web Page<span class="nt">&lt;/title&gt;</span>
<span class="nt">&lt;/head&gt;</span>
<span class="nt">&lt;body&gt;</span>
  <span class="nt">&lt;h1&gt;</span>Welcome to My First Web Page<span class="nt">&lt;/hh1&gt;</span>
  <span class="nt">&lt;p&gt;</span>This is a simple web page created using HTML.<span class="nt">&lt;/p&gt;</span>
  <span class="nt">&lt;img</span> <span class="na">src=</span><span class="s">"image.jpg"</span> <span class="na">alt=</span><span class="s">"An image"</span><span class="nt">&gt;</span>
<span class="nt">&lt;/body&gt;</span>
<span class="nt">&lt;/html&gt;</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>The fundamental architecture of the web during this era was straightforward.
Web browsers requested files from servers, which functioned much like file servers connected to a network.
These servers did little more than retrieving and sending the requested files back to the browsers.
The browsers would then render these files, which might include links to images and GIFs, further enhancing the user experience.
This model was particularly effective for delivering content that rarely changed, such as a restaurant’s monthly menu or landing pages, for example.</p>

<pre><code class="language-mermaid">%%{init: {'theme':'base'}}%%
graph TB
subgraph "Static HTML"
  C[👩 User]
  B[🖥️ Server]
  A[💻 Browser]
  C -- Request URL --&gt; A
  B -- HTML Page --&gt; A
  A -- Render HTML --&gt; C  
  A -- HTTP Request --&gt; B 
end
</code></pre>

<aside class="quote">
    <em>“This blog is actually a bunch of static HTML pages.”</em> 
</aside>
<p>The economic advantage of using static HTML is undeniable.
Since the server’s only task was to deliver files without any processing, the overhead costs are minimal.
This efficiency meant that a static site could handle thousands or even millions of visits each month at a very low cost.
Indeed, this blog is actually a bunch of static HTML pages built using <a href="https://jekyllrb.com/">Jekyll</a> and hosted for free using <a href="https://pages.github.com/">GitHub Pages</a>.
Static HTML remains a viable option for many applications today, testament to the enduring utility of simple, effective web technologies developed in the early days of the internet.</p>

<h2 id="server-side-rendering">Server-Side Rendering</h2>

<p>By the mid 90s, the web began to reach thousands of new users each day.
There was an emerged a desire to expand its capabilities beyond merely sharing HTML content.
For example, users wanted to interact with websites by submitting forms, logging in, and viewing personalized content.
However, the statelessness of the web presented unique challenges, as each request was independent of others.
This led to the development of <a href="https://en.wikipedia.org/wiki/Server-side_scripting">server-side scripting</a>, which enabled persisting and rendering dynamic content on the web server before sending it to the client.
This approach allowed pages to be customized for individual users based on their previous interactions with the website.</p>

<p>The first notable scripting method was <a href="https://en.wikipedia.org/wiki/Common_Gateway_Interface">CGI (Common Gateway Interface)</a>, introduced in 1993.
Since then, various languages have been developed for dynamic content creation, including Perl, Python, PHP, Ruby, ASP, Java Servlet, and Cold Fusion.
Server-side scripting, when combined with a database, revolutionized web interactions by allowing users to register and log in to websites for the first time.
This advancement facilitated the rise of the first wave of web applications, commonly referred to as <a href="https://en.wikipedia.org/wiki/Create,_read,_update_and_delete">CRUD (Create, Read, Update, Delete)</a> applications.
CRUD directly corresponds to both database operations and HTTP requests, providing a structured framework for building dynamic websites.</p>

<blockquote>
  <p>“One of the early successes of this technology was the launch of eBay, initially known as AuctionWeb. Founded on September 3, 1995, in San Jose, California, by Pierre Omidyar, a French-born Iranian programmer, eBay started as an experiment. Omidyar was curious to see what would happen if there was a global marketplace accessible to everyone. To test his idea, he created an auction website where the first item sold was a broken laser pointer, which surprisingly fetched $14.83 from a collector.”</p>
</blockquote>

<p>HTML forms were developed to facilitate user interactions like contacting support, sending messages, or making purchases.
When a user submits a form, it typically sends a <code class="language-plaintext highlighter-rouge">POST</code> request to a server-side script.
One of the first examples was <a href="https://metacpan.org/pod/Mail::Sendmail">sendmail Perl</a>, which processes the information to send emails or perform other actions in response to a client’s request.</p>

<pre><code class="language-mermaid">%%{init: {'theme':'base'}}%%
graph TB
subgraph "Server-Side Rendering"
  C[👩 User]
  B[🖥 Server]
  A[💻 Browser]
  D[(🗄️ Database)]
  C -- Request Login Page --&gt; A
  B -- HTML + JavaScript --&gt; A
  A -- Render Login Page --&gt; C
  A -- HTTP Request --&gt; B
  B -- CRUD --&gt; D
  D -- Data --&gt; B
  B -- Processing --&gt; B
end
</code></pre>

<p>The pattern has proven to be incredibly robust over time, despite the processing overhead associated with generating dynamic content in the server (especially under heavy traffic workloads).
The rise of <a href="/blog/design-for-microservices.html">microservices and serverless architectures</a> has further refined server-side rendering, allowing developers to build scalable, efficient web applications.
Today, developers rely on <a href="https://en.wikipedia.org/wiki/HTTP_cookie">HTTP cookies</a>, sessions, and databases to store user data.
Web technologies like PHP, ASP, Ruby on Rails, Node.js, WordPress, and ASP.NET Core have all embraced server-side rendering due to its powerful capabilities.</p>

<h2 id="client-side-rendering">Client-Side Rendering</h2>

<figure class="badge"><a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript"><img src="/img/badges/JavaScript_Logo.svg" style="width:140px;max-width:100%;" alt="badge" /></a></figure>

<p>Client-side rendering is the process of generating web content directly in the user’s browser.
To do so, HTML was clearly no enough, a real programming language was necessary.
JavaScript was the answer.
The legend says that JavaScript was created in just ten days by <a href="https://en.wikipedia.org/wiki/Brendan_Eich">Brendan Eich</a> while working on Netscape in May 1995.
Nothing good can be done in ten days, and that may be one of the reasons why JavaScript has caused so many headaches to developers.
Anyway, JavaScript revolutionised the web by allowing the pages to respond dynamically to user inputs.
For example, it could immediately notify when their passwords did not meet certain validation requirements, instantly, directly in the browser. 
The user experience improved significantly because there was no need to reload page.<br />
Client-side rendering made web pages more interactive by eliminating the constant (and costly) server communication.</p>

<pre><code class="language-mermaid">%%{init: {'theme':'base'}}%%
flowchart
    subgraph Browser["Browser"]
        subgraph UserInteraction["User Interaction"]
            JS["JavaScript"]
        end
        subgraph RenderEngine["Rendering Engine"]
            HTML["HTML"] --&gt; CSS["CSS"]
        end
    end
    UserInteraction --&gt;|Manipulates| RenderEngine
    UserInteraction --&gt;|Updates DOM| HTML
    CSS
    K[👩 User] -- "HTTP Request" --&gt; Browser
    Browser -- "Rendered Content" --&gt; K
</code></pre>

<p>The following is an example of a simple JavaScript function <code class="language-plaintext highlighter-rouge">validatePassword</code> that takes a password input and alerts the user if it is less than eight characters long, note that all the execution of this logic is done in the browser:</p>

<figure class="highlight"><pre><code class="language-html" data-lang="html"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
</pre></td><td class="code"><pre><span class="cp">&lt;!DOCTYPE html&gt;</span>
<span class="nt">&lt;html&gt;</span>
  <span class="nt">&lt;head&gt;</span>
    <span class="nt">&lt;title&gt;</span>Password Validation<span class="nt">&lt;/title&gt;</span>
    <span class="nt">&lt;script&gt;</span>
    <span class="kd">function</span> <span class="nf">validatePassword</span><span class="p">()</span> <span class="p">{</span>
      <span class="kd">var</span> <span class="nx">password</span> <span class="o">=</span> <span class="nb">document</span><span class="p">.</span><span class="nf">getElementById</span><span class="p">(</span><span class="dl">"</span><span class="s2">password</span><span class="dl">"</span><span class="p">).</span><span class="nx">value</span><span class="p">;</span>
      <span class="k">if</span> <span class="p">(</span><span class="nx">password</span><span class="p">.</span><span class="nx">length</span> <span class="o">&lt;</span> <span class="mi">8</span><span class="p">)</span> <span class="p">{</span>
        <span class="nf">alert</span><span class="p">(</span><span class="dl">"</span><span class="s2">Password must be at least 8 characters long.</span><span class="dl">"</span><span class="p">);</span>
      <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="nf">alert</span><span class="p">(</span><span class="dl">"</span><span class="s2">Password is valid.</span><span class="dl">"</span><span class="p">);</span>
      <span class="p">}</span>
    <span class="p">}</span>
    <span class="nt">&lt;/script&gt;</span>
  <span class="nt">&lt;/head&gt;</span>
  <span class="nt">&lt;body&gt;</span>
    <span class="nt">&lt;h2&gt;</span>Password Validation<span class="nt">&lt;/h2&gt;</span>
    <span class="nt">&lt;p&gt;</span>Enter your password:<span class="nt">&lt;/p&gt;</span>
    <span class="nt">&lt;input</span> <span class="na">type=</span><span class="s">"password"</span> <span class="na">id=</span><span class="s">"password"</span> <span class="na">placeholder=</span><span class="s">"Enter password"</span><span class="nt">&gt;</span>
    <span class="nt">&lt;button</span> <span class="na">onclick=</span><span class="s">"validatePassword()"</span><span class="nt">&gt;</span>Validate Password<span class="nt">&lt;/button&gt;</span>
  <span class="nt">&lt;/body&gt;</span>
<span class="nt">&lt;/html&gt;</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>During JavaScript’s early days, tech giants like Microsoft and Adobe were also venturing into <strong>dynamic client-side scripting</strong>.
Microsoft introduced technologies such as <a href="https://en.wikipedia.org/wiki/ActiveX">ActiveX</a>, along with scripting languages like JScript and VBScript, 
while Adobe developed ActionScript for its Flash authoring tool. 
Interestingly, none of these technologies ended up going anywhere and today are practically deprecated. 
Indeed, JavaScript became the de facto standard for client-side scripting.
One of the reasons is that these scripting languages were proprietary, raising significant security concerns because they could execute any code downloaded from the internet, potentially harming the user’s system.
To mitigate these risks, browsers implemented strict containment measures.
However, some scripts could still bypass these protections through techniques that exploit browser vulnerabilities.</p>

<p>The potential of JavaScript extended significantly with the advent of <a href="https://www.w3schools.com/xml/ajax_intro.asp">AJAX (Asynchronous JavaScript and XML)</a>, particularly after its use in Google’s products like Gmail and Google Maps in the early 2000s.
AJAX was initially made possible by the <a href="https://developer.mozilla.org/en-US/docs/Web/API/XMLHttpRequest">XMLHttpRequest</a>, initially created by Microsoft for its Outlook Web App in 1998.
This technology allowed web pages to request additional data from servers without needing a page refresh, leading to more dynamic and responsive web applications. 
Today AJAX is a fundamental component of modern web development, enabling continuous data exchange between browsers and servers and significantly enhancing the user experience on the internet.</p>

<aside class="quote">
    <em>“Any application that <em>can</em> be written in JavaScript, <em>will</em> eventually be written in JavaScript”</em> 
    <cite><br /> ― <a href="https://blog.codinghorror.com/the-principle-of-least-power/">Atwood's Law</a></cite>
</aside>

<p>The development landscape took a dramatic turn with the creation of <a href="https://nodejs.org/en">Node.js</a> in May 2009.
Node.js is an open-source cross-platform runtime environment that allows developers to use JavaScript for server-side application development.
By integrating Google’s V8 JavaScript engine with an event loop and a non-blocking I/O API, Node.js facilitates the development of efficient and scalable web applications. 
This innovation was partly inspired by the need for better handling of file uploads, as seen in applications like <a href="https://www.flickr.com/">Flickr</a>, where the progress of file uploads was not visible to the user without querying the server.</p>

<p>Given the limitations and security issues associated with client-side scripting, many developers prefer server-side scripting to create dynamic content.
However, to enrich the user experience, websites occasionally implement a mix of minimal client-side scripting alongside server-generated content.
This approach helped in delivering a more interactive and engaging web experience without compromising security.</p>

<h2 id="java-applets-and-plugins">Java Applets and Plugins</h2>

<figure class="badge"><a href="https://www.oracle.com/java/technologies/javase/9-deprecated-features.html"><img src="/img/badges/JavaApplet_Loading.png" style="width:140px;max-width:100%;" alt="badge" /></a></figure>

<p>During the late 1990s, <a href="https://www.oracle.com/java/technologies/javase/9-deprecated-features.html">Java Applets</a> were widely believed to be the future of web applications. 
These applets allowed the execution of Java bytecode directly on the client side.
Apples integrated seamlessly within an HTML page through an <code class="language-plaintext highlighter-rouge">applet</code> tag.
This functionality promised to enrich the web experience by offering more sophisticated interactivity directly in the browser.
For example, Applets allowed playing games, filling interactive forms, and running complex web applications. 
However, to leverage these capabilities, users were required to install a plugin that would facilitate the execution of these applets on their devices.</p>

<p>Here’s an example of an HTML page embedding a Java Applet. 
Note that in the browser cannot execute the <code class="language-plaintext highlighter-rouge">FileApplet.class</code> natively. 
To do so, it needs to have the JVM and a dedicated plugin installed.
Therefore, Applets were platform-dependent:</p>

<figure class="highlight"><pre><code class="language-html" data-lang="html"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="code"><pre><span class="nt">&lt;html&gt;</span>
<span class="nt">&lt;head&gt;</span>
    <span class="nt">&lt;title&gt;</span>Simple File Reader Applet<span class="nt">&lt;/title&gt;</span>
<span class="nt">&lt;/head&gt;</span>
<span class="nt">&lt;body&gt;</span>
    <span class="nt">&lt;applet</span> <span class="na">code=</span><span class="s">"FileApplet.class"</span> <span class="na">width=</span><span class="s">"300"</span> <span class="na">height=</span><span class="s">"300"</span><span class="nt">&gt;</span>
        Your browser does not support Java Applets or Java is not enabled.
    <span class="nt">&lt;/applet&gt;</span>
<span class="nt">&lt;/body&gt;</span>
<span class="nt">&lt;/html&gt;</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>The reliance on plugins during this period was not unique to Java.
Using plugins to extend browser’s functionality was a common pattern back then.
For example, technologies such as Shockwave, Flash, Director, Real Player, and QuickTime also depended on plugins to enhance browser capabilities and deliver content that native web technologies at the time could not handle. 
These plugins provided rich multimedia experiences and interactive content on websites, filling a crucial gap in browser technology.
Yet, this approach came with significant drawbacks, particularly concerning security vulnerabilities, which were a constant source of headaches for developers and users alike.</p>

<pre><code class="language-mermaid">%%{init: {'theme':'base'}}%%
sequenceDiagram
    participant Browser
    participant Plugin
    participant JVM
    participant Applet

    Browser-&gt;&gt;+Plugin: Detect applet tag
    Plugin-&gt;&gt;+JVM: Initialize JVM
    JVM-&gt;&gt;+Applet: Load Applet (FileApplet.class)
    Applet--&gt;&gt;-JVM: Execute Applet
    JVM--&gt;&gt;-Plugin: Render Applet in Browser
    Plugin--&gt;&gt;-Browser: Display Applet Content

    Note over Browser, Applet: Plugin required for Applet execution
</code></pre>

<aside class="quote">
    <em>“The failure of web plugins ended up making browsers more powerful in the end.”</em> 
</aside>
<p>The plugin era began to wane with significant shifts.
Notably, in 2007 Apple released the iPhone and <a href="https://www.theregister.com/2007/10/29/no_java_for_leopard/">explicitly decided against supporting Java Applets</a>, Shockwave, Flash, and other plugins on the new device.
This pushed developer to rely only on JavaScript and HTML for creating web applications.
As mobile internet usage surged and security concerns grew, the industry moved towards more secure, native web technologies.
This transition underscored a broader trend towards enhancing browser capabilities and standardizing web technologies to create a safer and more seamless user experience.
So in the end, browsers became more powerful, and that was the end of the plugin paradigm!</p>

<h2 id="css-and-dynamic-web-design">CSS and Dynamic Web Design</h2>

<p>One significant limitation of HTML is its rudimentary style capabilities, confined mostly to basic alignments and text formatting.
Cascading Style Sheets (CSS) were introduced in 1996 to expand HTML’s styling functions. 
It fundamentally changed web design. 
CSS allowed for the separation of document content (written in HTML) from document presentation.
For example, CSS includes detailed specifications of fonts, colors, layouts, and other visual aspects.
This separation not only made web pages more visually appealing but also significantly streamlined the web development process by allowing styles to be defined once and reused across multiple pages.</p>

<p>For example, the following CSS code defines a style for a <code class="language-plaintext highlighter-rouge">div</code> element with a red background and white text:</p>

<figure class="highlight"><pre><code class="language-css" data-lang="css"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre><span class="nt">div</span> <span class="p">{</span>
  <span class="nl">background-color</span><span class="p">:</span> <span class="nx">red</span><span class="p">;</span>
  <span class="nl">color</span><span class="p">:</span> <span class="nx">white</span><span class="p">;</span>
<span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>This style can be shared across multiple HTML pages, ensuring a consistent look and feel throughout a website:</p>

<pre><code class="language-mermaid">%%{init: {'theme':'base'}}%%
flowchart TB  
  subgraph Website
    direction LR 
    subgraph HTML
        Page1[📄 Page 1]
        Page2[📄 Page 2]
        Page3[📄 Page 3]
        Page4[📄 Page 4]
    end
    subgraph CSS
        Style1[🎨 Style 1]
        Style2[🎨 Style 2]
    end
  end
  Page1 --- Style1
  Page2 --- Style1
  Page3 --- Style1
  Page4 --- Style2
</code></pre>

<p>CSS’s introduction was a response to the growing demand for more dynamic and stylistically diverse websites.
It allows developers to control the layout and appearance of web elements independently of HTML structure.
CSS enabled a more efficient way to design visually engaging websites. 
As CSS evolved, it became responsible for the majority of the styling on web pages, influencing everything from layout to animations, thereby enhancing the user’s visual experience and interaction with the web.</p>

<p>The concurrent rise of JavaScript further complemented the capabilities of CSS, providing a way to dynamically manipulate page elements and styles through the DOM. 
This synergy between JavaScript and CSS meant that web pages could not only look better but also change the visual elements in real-time as a response to user inputs.
JavaScript’s ability to interact with CSS properties brought about a new era in web design, where the static pages of the early web gave way to dynamic, interactive experiences.</p>

<h2 id="spa-and-ajax">SPA and AJAX</h2>

<p>The term <a href="https://www.w3schools.com/xml/ajax_intro.asp">AJAX (Asynchronous JavaScript and XML)</a>, coined in 2005, revolutionized web development.
It enables web applications to communicate with servers in the background, requesting and receiving data from a server after the page has loaded.
Although initially designed for XML, AJAX quickly adapted to other data formats like JSON, which became more prevalent due to its simplicity and lightweight nature. 
This asynchronous communication allowed web pages to update content dynamically without refreshing, unlocking new paradigms in web authoring and greatly enhancing user experiences with interactive and seamless interfaces.</p>

<p>By 2015, advancements in web technology led browser vendors to develop <a href="https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API">a more efficient method for making network requests</a>, known as the <code class="language-plaintext highlighter-rouge">fetch</code> API.
This modern approach provides a promise-based mechanism to handle responses, streamlining the process of asynchronous requests. 
By using <code class="language-plaintext highlighter-rouge">fetch</code>, developers could send a <a href="https://en.wikipedia.org/wiki/Single-page_application">single-page application (SPA)</a> to the client, which would then request data as needed from a server-side API. 
This data could seamlessly update specific parts of the web page, improving the app’s responsiveness and user engagement without reloading the entire page.</p>

<p>SPAs marked a significant shift in web development, emphasizing rich client-side interactivity. 
This model allowed parts of the webpage to update independently, signaling their loading status with animations such as spinning icons while waiting for data. 
This paradigm, heavily reliant on JavaScript frameworks like React and Angular, became incredibly popular for creating complex user interfaces that offered a desktop-like experience within a browser. 
SPAs managed to bring high interactivity and fluidity to web applications, albeit with challenges such as initial load times and state management complexities.</p>

<p>As mentioned, web frameworks became essential for managing large codebases and structuring applications efficiently. 
<a href="https://angularjs.org/">AngularJS</a>, developed in 2009, was one of the pioneers, followed by <a href="https://emberjs.com/">Ember.js</a> in 2011.
These frameworks provided robust architectures, embracing principles like “Convention over Configuration” and “Don’t Repeat Yourself (DRY),” to streamline development and maintenance of SPAs. 
They addressed many challenges of SPAs by offering structured ways to build and manage stateful, interactive applications efficiently.</p>

<p><a href="https://react.dev/">React</a>, developed by Facebook and released in 2013, introduced a novel approach to building SPAs by employing a one-way data flow and <a href="https://legacy.reactjs.org/docs/faq-internals.html">virtual DOM</a>. 
This architecture allowed React to manage UI updates efficiently—only re-rendering components when actual data changes occurred, minimizing the performance costs associated with DOM manipulation.
React’s component-based architecture also enabled developers to build reusable UI blocks, simplifying the development process, reducing bugs, and improving the maintainability of large applications.</p>

<p>The following is an example of a React component that toggles a button’s color between green and red when clicked. Note that the CSS styles are defined directly within the component, along with the JavaScript logic:</p>

<figure class="highlight"><pre><code class="language-jsx" data-lang="jsx"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
</pre></td><td class="code"><pre><span class="k">import</span> <span class="nx">React</span><span class="p">,</span> <span class="p">{</span> <span class="nx">useState</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">'</span><span class="s1">react</span><span class="dl">'</span><span class="p">;</span>

<span class="kd">function</span> <span class="nf">ToggleButton</span><span class="p">()</span> <span class="p">{</span>
<span class="c1">// State to keep track of the toggle status</span>
<span class="kd">const</span> <span class="p">[</span><span class="nx">isToggled</span><span class="p">,</span> <span class="nx">setIsToggled</span><span class="p">]</span> <span class="o">=</span> <span class="nf">useState</span><span class="p">(</span><span class="kc">false</span><span class="p">);</span>

<span class="c1">// Inline styles for different states</span>
<span class="kd">const</span> <span class="nx">buttonStyle</span> <span class="o">=</span> <span class="p">{</span>
  <span class="na">backgroundColor</span><span class="p">:</span> <span class="nx">isToggled</span> <span class="p">?</span> <span class="dl">'</span><span class="s1">lightcoral</span><span class="dl">'</span> <span class="p">:</span> <span class="dl">'</span><span class="s1">lightgreen</span><span class="dl">'</span><span class="p">,</span>
  <span class="na">padding</span><span class="p">:</span> <span class="dl">'</span><span class="s1">10px 20px</span><span class="dl">'</span><span class="p">,</span>
  <span class="na">borderRadius</span><span class="p">:</span> <span class="dl">'</span><span class="s1">5px</span><span class="dl">'</span><span class="p">,</span>
  <span class="na">border</span><span class="p">:</span> <span class="dl">'</span><span class="s1">none</span><span class="dl">'</span><span class="p">,</span>
  <span class="na">cursor</span><span class="p">:</span> <span class="dl">'</span><span class="s1">pointer</span><span class="dl">'</span><span class="p">,</span>
  <span class="na">color</span><span class="p">:</span> <span class="dl">'</span><span class="s1">white</span><span class="dl">'</span><span class="p">,</span>
  <span class="na">fontSize</span><span class="p">:</span> <span class="dl">'</span><span class="s1">16px</span><span class="dl">'</span><span class="p">,</span>
  <span class="na">fontWeight</span><span class="p">:</span> <span class="dl">'</span><span class="s1">bold</span><span class="dl">'</span><span class="p">,</span>
  <span class="na">outline</span><span class="p">:</span> <span class="dl">'</span><span class="s1">none</span><span class="dl">'</span><span class="p">,</span>
  <span class="na">transition</span><span class="p">:</span> <span class="dl">'</span><span class="s1">all 0.3s ease</span><span class="dl">'</span>
<span class="p">};</span>

<span class="c1">// Function to handle button click</span>
<span class="kd">const</span> <span class="nx">toggleButton</span> <span class="o">=</span> <span class="p">()</span> <span class="o">=&gt;</span> <span class="p">{</span>
  <span class="nf">setIsToggled</span><span class="p">(</span><span class="o">!</span><span class="nx">isToggled</span><span class="p">);</span>
<span class="p">};</span>
<span class="k">return </span><span class="p">(</span>
    <span class="p">&lt;</span><span class="nt">button</span> <span class="na">style</span><span class="p">=</span><span class="si">{</span><span class="nx">buttonStyle</span><span class="si">}</span> <span class="na">onClick</span><span class="p">=</span><span class="si">{</span><span class="nx">toggleButton</span><span class="si">}</span><span class="p">&gt;</span>
      <span class="si">{</span><span class="nx">isToggled</span> <span class="p">?</span> <span class="dl">'</span><span class="s1">ON</span><span class="dl">'</span> <span class="p">:</span> <span class="dl">'</span><span class="s1">OFF</span><span class="dl">'</span><span class="si">}</span>
    <span class="p">&lt;/</span><span class="nt">button</span><span class="p">&gt;</span>
  <span class="p">);</span>
<span class="p">}</span>

<span class="k">export</span> <span class="k">default</span> <span class="nx">ToggleButton</span><span class="p">;</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>React further innovated SPA development by integrating styles directly within components, a method that sparked considerable debate among developers. 
This approach argued for a modular system where style, structure, and logic are encapsulated within components, scaling more effectively for large applications.
By reducing the separation of concerns traditionally seen in CSS, HTML, and JavaScript, React’s inline styling philosophy promotes a more cohesive and maintainable codebase, especially in complex projects.
This “componentized” styling represents a significant evolution in how developers think about and build user interfaces for the web.</p>

<h2 id="websockets">WebSockets</h2>

<p>For many years, AJAX was the primary technology for asynchronous server communication in web development. 
However, AJAX had limitations, particularly in scenarios where the client needed to continously pull updates from the server. 
This gap was evident in applications requiring real-time data updates, as servers had to wait for every new client request to send data. 
<a href="https://en.wikipedia.org/wiki/WebSocket">WebSockets</a>, proposed by the W3C, addressed this challenge by establishing a bidirectional communication channel between the client and the server.
This protocol supported both text and binary data with significantly reduced overhead compared to traditional HTTP polling methods, enabling more dynamic and responsive web applications.</p>

<pre><code class="language-mermaid">%%{init: {'theme':'base'}}%%
sequenceDiagram
    participant Client
    participant Server

    Client-&gt;&gt;+Server: HTTP Request (Upgrade Header)
    Server--&gt;&gt;-Client: HTTP Response (101 Switching Protocols)

    Note over Client, Server: WebSocket Connection Established

    Client-&gt;&gt;Server: Send Message
    Server-&gt;&gt;Client: Receive Message
    Server-&gt;&gt;Client: Send Update
    Client-&gt;&gt;Server: Receive Update

    Note over Client, Server: Persistent Bi-Directional Communication
</code></pre>

<p>The evolution of WebSockets was a significant milestone in web technology. 
After several iterations and enhancements, the protocol was officially recognized as <a href="https://datatracker.ietf.org/doc/html/rfc6455">IETF protocol ‘RFC6455’</a> in December 2011, and quickly implemented across all major browsers.
This standardization marked a turning point, allowing developers to reliably use WebSockets to create highly interactive, real-time web applications that could compete with or even surpass desktop applications in functionality and performance.
The low latency and efficient data transfer capabilities of WebSockets made them ideal for applications that required constant data exchange and immediate user interaction.</p>

<p>WebSockets revolutionized how web applications were developed by facilitating a persistent, lightweight connection between the browser and the server. 
The connection remains open (usually over TCP port number <code class="language-plaintext highlighter-rouge">443</code>), allowing for instant data exchange without the need for repeated HTTP requests.
This architecture is particularly beneficial in real-time applications such as chat platforms, collaborative tools like Google Docs, and multiplayer online games.
The ability of WebSockets to allow multiple browsers to connect concurrently to the same service has opened up vast possibilities for real-time, collaborative experiences and applications on the web.</p>

<h2 id="decentralized-web">Decentralized Web</h2>

<p>The concept of <a href="https://en.wikipedia.org/wiki/Web3">Web 3.0</a> (or Web3) emerged in the late 2010s and early 2020s.
It became intrinsically linked with <a href="https://en.wikipedia.org/wiki/Blockchain">blockchain technology</a> and the broader cryptocurrency movement. 
This new era of the internet, often referred to as the decentralized web, prioritizes concepts such as decentralization, openness, and enhanced user privacy. 
Blockchain technology, at the core of this movement, provides a distributed ledger that records data transfers across multiple computers in such a way that the registered requests cannot be altered retroactively.
This immutable and transparent nature of blockchain facilitates trust and security.
Web3 is a fundamental shift towards a decentralized internet structure where power and control are distributed across the network rather than held by a few centralized entities owning the servers.</p>

<pre><code class="language-mermaid">%%{init: {'theme':'base'}}%%
graph TD
User1[👩 User 1] --&gt;|HTTP Request| dApp1[Decentralized Web Application]
User2[🧔 User 2] --&gt;|HTTP Request| dApp2[Decentralized Web Application]
User3[👴 User 3] --&gt;|HTTP Request| dApp1

subgraph "Blockchain"
    direction RL 
    Node1(Node1)
    Node2(Node2)
    Node3(Node3)
    Node4(Node4)

    Node1 -- Validate --- Node2
    Node1 -- Validate --- Node3
    Node1 -- Validate --- Node4
    Node2 -- Validate --- Node3
    Node2 -- Validate --- Node4
    Node3 -- Validate --- Node4
end    

dApp1 --&gt;|Validate Request| Node1
dApp2 --&gt;|Validate Request| Node2
</code></pre>

<p>As I mentioned, the incentive to have a descentralized web is to create a more secure and private web experience.
By leveraging decentralization, blockchain allows for data to be stored across a network of nodes.
A <a href="https://www.baeldung.com/cs/consensus-algorithms-distributed-systems">consensus algorithm</a> ruling the nodes ensures that no single point of failure can compromise the system’s integrity. 
This method enhances data security and user privacy significantly. 
Moreover, blockchain enables <a href="https://en.wikipedia.org/wiki/Smart_contract">smart contracts</a>, which are self-executing programs with the terms of the agreement among the nodes directly written into lines of code.
These contracts automate and enforce agreements without the need for intermediaries.
For the web, this means that the requests can be validated and executed without relying on a central authority.
For example, a <a href="https://en.wikipedia.org/wiki/Decentralized_application">decentralized web application (dApp)</a> operating as a social network can interact with a blockchain to ensure that each user identity is legit, and that their payments (or other types of transactions) are secure and transparent.
Indeed, the promise of decentralized finance (DeFi) platforms is to offer financial services without traditional banking institutions. 
Although descentralization is not a silver bullet, it has the potential to create a more user-centric internet, with full data ownership and privacy.</p>

<h2 id="webassembly">WebAssembly</h2>

<figure class="badge"><a href="https://webassembly.org/"><img src="/img/badges/WebAssembly_Logo.svg" style="width:140px;max-width:100%;" alt="badge" /></a></figure>

<p>JavaScript is (still) the predominant scripting language in modern web browsers.
However, it has significant limitations due to its inherent language characteristics. 
For example, each JavaScript engine must parse and recompile the code at runtime, which induces substantial overhead and contributing to lengthy website load times. 
Additionally, JavaScript’s lack of memory isolation poses security risks by potentially allowing the extraction of information from other processes. 
<a href="../blog/the-evolution-of-the-web-from-html-to-webassembly.html#java-applets-and-plugins">As I mentioned before</a>, attempts to replace or supplement JavaScript with other languages, such as Java Applets, Microsoft ActiveX, and Silverlight, have historically failed due to security concerns and lack of community consensus among browser vendors.</p>

<aside class="quote">
    <em>“WebAssembly is the latest attempt to reduce the pervasiveness of JavaScript on the web.”</em> 
</aside>

<p>In 2014, the <a href="https://emscripten.org/">Emscripten</a> technology represented the most solid attempt in overcoming JavaScript’s limitations.
It utilized <code class="language-plaintext highlighter-rouge">asm.js</code>, a <a href="https://en.wikipedia.org/wiki/Asm.js">strict subset of JavaScript</a> designed for compiling low-level languages like C into JavaScript.
This approach leveraged <a href="https://en.wikipedia.org/wiki/LLVM">LLVM’s ahead-of-time optimizations</a>, significantly enhancing performance over traditional JavaScript, as detailed in <a href="https://hacks.mozilla.org/2015/03/asm-speedups-everywhere/">Mozilla’s performance analysis</a>. 
<code class="language-plaintext highlighter-rouge">asm.js</code> streamlined JavaScript by restricting it to simple numeric types and memory operations.
This approach proved that client-side code performance could be substantially improved through careful language design and standardization. 
The success of Emscripted laid the groundwork for the development and standardization of <a href="https://webassembly.org/">WebAssembly</a> by the World Wide Web Consortium (W3C) in 2015.</p>

<blockquote>
  <p>“WebAssembly addresses the inefficiencies of transpiling languages like Java or TypeScript to JavaScript, which was traditionally the only way to run applications in browsers.”</p>
</blockquote>

<p>WebAssembly (a.k.a., wasm) was developed as a native runtime for the web.
It allows developers to write applications in languages such as C, Rust, or Ruby, and compile them into a <code class="language-plaintext highlighter-rouge">.wasm</code> file. 
This file is then served from a web server and executed in the browser, with JavaScript often playing a role in bootstrapping the application.
This innovation brought the possibility of running high-performance, native applications directly within the browser environment, reminiscent of the capabilities provided by Java Applets in the 1990s.</p>

<pre><code class="language-mermaid">%%{init: {'theme':'base'}}%%
flowchart TB
    src[Source Code in C/Rust/Other] --&gt;|Compile| wasmFile[.wasm File]
    wasmFile --&gt;|Serve| server[Web Server]
    server --&gt;|Download| browser[Browser]

    subgraph browser [Browser Execution Environment]
        js[JavaScript Bootstrap] --&gt; wasmModule[WebAssembly Module]
        wasmModule --&gt;|Interact| dom[DOM/APIs]
    end
</code></pre>

<p>One of the significant advantages of WebAssembly is its robust security model, which prevents binaries from accessing memory outside their allocated space through <a href="https://www.geeksforgeeks.org/what-is-software-fault-isolation/">Software Fault Isolation (SFI)</a> policies. 
This isolation makes WebAssembly more secure compared to traditional JavaScript environments.
Furthermore, WebAssembly modules are more compact and efficient than virtual machines or containers, making them particularly well-suited for environments where network efficiency is critical. 
Today, WebAssembly is used for a diverse range of browser tasks, from gaming to crypto-mining, and has proven especially effective for short-running tasks on backend platforms, such as offerings.</p>

<p>WebAssembly has significantly broadened the scope of what can be achieved with web applications, enabling complex software like AutoCAD and Adobe Photoshop to be ported directly into the browser as highlighted in <a href="https://twitter.com/Adobe/status/1453034805004685313?s=20&amp;t=Zf1N7-WmzecA0K4V8R69lw">Adobe’s announcement</a>. 
This capability transforms web applications to perform functions traditionally reserved for desktop applications, pushing the boundaries of web software and changing our understanding of what can be possible. 
As WebAssembly continues to evolve, it promises to further revolutionize the development and performance of applications across the web.</p>

<h1 id="summary">Summary</h1>

<aside class="quote">
    <em>“Despite many attempts, JavaScript seems impossible to replace for the time being.”</em> 
</aside>
<p>Valuable lessons emerge from the evolution of web technologies. 
These include <em>i)</em> the necessity for vendor agreement to ensure technology success, <em>ii)</em> the constant shifts between centralizing computing power in servers or distributing it among users (both of which contribute to technological breakthroughs), and <em>iii)</em> the critical need for a secure and efficient web runtime in the browser.
Moreover, we’ve learned that moving away from JavaScript is still quite challenging!</p>

<p>But, for now, I’d like to leave you with a few observations:</p>

<ol>
  <li><strong>Simplicity and Efficiency Hold</strong>: Foundational web technologies like static HTML and server-side rendering remain useful for delivering content that rarely changes or doesn’t require dynamic processing. These technologies were great in the 90s and continue to be effective and cost-efficient solutions even in 2024 for many use cases.</li>
  <li><strong>User Experience Matters</strong>: From the introduction of CSS for enhanced web aesthetics to the development of SPAs for dynamic user interactions, the focus on improving user experience has led to the creation of more dynamic technologies for real-time DOM manipulation and improved data fetching, leading to better user experience.</li>
  <li><strong>Interactivity and Real-Time Communication Are Essential</strong>: Technologies such as client-side rendering, AJAX, and WebSockets have redefined user engagement, providing experiences as smooth and responsive as those of desktop applications.</li>
  <li><strong>Security and Performance Are Continuous Priorities</strong>: Technologies like WebAssembly offer robust security models and enhanced performance, enabling developers to create high-performant applications with improved security features.</li>
  <li><strong>Innovation and Adaptability is Not Optional</strong>: Developers need to remain flexible and embrace new technologies like WebAssembly and dApps to create cutting-edge web applications that meet the expectations of modern users.</li>
</ol>

<h1 id="external-resources">External Resources</h1>

<ul>
  <li><a href="https://youtu.be/a_1cV7hg5G8?si=VIaAQnfTyR9nuUdc">The Evolution of Web Apps 1992-2024, by Dylan Beattie</a></li>
  <li><a href="https://youtu.be/41mnNyMxPOA?si=9NVNYd9jyopvFaXj">How We Got Here - The History of Web Development, by Richard Campbell</a></li>
  <li><a href="https://youtu.be/VPToE8vwKew?si=6IGNrHCNWJjao_vH">How We Made the Internet</a></li>
</ul>

<h1 id="footnotes">Footnotes</h1>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p>BTW, my sincere appreciation to you if you’re a frontend or UI web designer. What you do in the trenches is really hard. I <del>probably</del> never would be able to make a decent living out of it. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p><a href="https://www.darpa.mil/">DARPA</a> (Defense Advanced Research Projects Agency) is essentially a military research project funded by the US government. It still exists and is responsible for the development of not only the internet in the 60s but other technologies like GPS, drones, and even the first computer mouse. It’s sad that the internet was not created in a public university as it should, but that’s it. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>I browsed the web for the first time in 2011. It was during the second year at university. Back then, in Cuba the internet access was restricted to universities and research centres. Individual access was controlled using a quota-based system. I had 50MB per week to spend! I spent it navigating educational websites (the majority of other services were forbidden). <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6">
      <p>It’s good not to forget that the internet went from being a niche academic tool to becoming a mainstream technology in less than 30 years. That’s a blink of an eye in terms of human’s evolution! <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>Tim Berners-Lee was knighted by Queen Elizabeth II in 2004, so now we should call him “sir” to be more exact. People get promoted dear reader, the sky is the limit. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7">
      <p>The first web address was <a href="https://info.cern.ch/">info.cern.ch</a>, and the very first web page can still be visited at <a href="https://info.cern.ch/hypertext/WWW/TheProject.html">the project website</a>. <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5">
      <p>HTML and CSS are not programming languages. There’s no such as thing as an HTML programmer (nor anyone proud to be called as such). <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>César Soto Valero</name><email>cesarsotovalero@gmail.com</email></author><category term="web" /><summary type="html"><![CDATA[The web is (arguably) the only tech platform that has been continuously transformed and evolved at a global scale over the last 30 years. How was this possible? What can we learn from the past technical mistakes and successes? Over the years, I have witnesses the emergence of various web technologies. Some were revolutionary, while others didn't pass the test of time. In this article, I dive into the evolution of web technologies from the 1990s to the present. My focus is on the engineering paradigms that marked inflection points in web's history. Those who don't know tech history are doomed to repeat past engineering mistakes.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.cesarsotovalero.net/img/posts/2024/2024-04-27/infinite-park-cover.jpg" /><media:content medium="image" url="https://www.cesarsotovalero.net/img/posts/2024/2024-04-27/infinite-park-cover.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">The Most Relevant Technical Skill in My Career</title><link href="https://www.cesarsotovalero.net/blog/the-most-relevant-technical-skill-in-my-career.html" rel="alternate" type="text/html" title="The Most Relevant Technical Skill in My Career" /><published>2024-03-30T00:00:00-07:00</published><updated>2025-07-29T11:52:25-07:00</updated><id>https://www.cesarsotovalero.net/blog/the-most-relevant-technical-skill-in-my-career</id><content type="html" xml:base="https://www.cesarsotovalero.net/blog/the-most-relevant-technical-skill-in-my-career.html"><![CDATA[<p>Eleven years ago (do you still remember 2013?), I was in the last year of my undergraduate studies.
I started working on my thesis and soon realized that, to earn my diploma, I had to collect, analyze, and summarize data in a specific way.
Because at the end of the day, a thesis needs content, and content comes from data derived from some kind of experiment.
However, none of these skills were taught to me during the 5 years of my bachelor’s curriculum.
Getting the thesis done was a struggle, but <del>somehow</del> I succeeded.
A couple of years later, I reached the end of my Master degree, and the same data munging cycle repeated once again, as it did later when I ventured into academic research.
So, after writing 3 thesis, reading a lot of papers, and undertaking my own research projects, I realize that data analysis skills are essential, regardless of the complexity of the problem I’m addressing.
Not surprisingly, the need for the same skill surfaced again during my professional career.
No matter how advanced the programming language or sophisticated the frameworks, the ability to handle data (beyond Excel and DBs) is crucial.
In fact, I think that Exploratory Data Analysis (EDA) has been, and still is, the most important technical skill in my career.
I consider EDA an essential skill for any professional, especially for those working in jobs that demand informed decision-making.
This post is about my journey, the lessons learned, and the resources I have used to enhance my EDA skills over time.</p>

<figure class="jb_picture">
  



<img width="" style="border: ;" src="/assets/resized/paint-640x373.jpg" alt="Artwork made by my son when he was 3 years old" data-srcset="/assets/resized/paint-640x373.jpg 640w,/assets/resized/paint-768x448.jpg 768w,/assets/resized/paint-1024x597.jpg 1024w,/assets/resized/paint-1366x797.jpg 1366w,/assets/resized/paint-1600x933.jpg 1600w," class="blur-up lazyautosizes lazyload" />
  <figcaption class="stroke"> 
    &#169; We collect information, explore our environments, and express ourselves since very early ages. The picture shows an artwork made by my son when he was 3 years old.
  </figcaption>
</figure>

<h1 id="what-is-eda">What is EDA?</h1>

<p>The main purpose of <a href="https://en.wikipedia.org/wiki/Exploratory_data_analysis">EDA</a> is to aggregate, analyze, summarize, and visualize data with the objective of finding and/or showcasing valuable information.
EDA excels at the beginning of a research project, enabling a quick understanding and explanation of the data before moving on to more sophisticated analysis or modeling techniques.</p>

<p>I believe that EDA is, in essence, <strong>a form of scientific-based communication</strong>.
It is a <del>fundamental</del> necessary skill for professionals in any field.
EDA is key for any job that involves decision-making, because it allows seeing the big picture in the data through visualizations, pattern identification, and causal reasoning, which favors making better decisions in something known as “<a href="https://www.linkedin.com/pulse/data-driven-organizations-quick-review-ahmed-elrouby/">data-drive organizations</a>.”
Specifically, a strong foundation in EDA is great for scientists seeking answers to complex questions for which empirical studies are required.</p>

<p>EDA today is mostly associated to Data Science roles, but its roots belong to the field of traditional statistics. 
For example, here are some questions that EDA helps to answer:</p>

<ul>
  <li>What is the <strong>distribution</strong> of the data?</li>
  <li>How do variables <strong>relate</strong> to each other?</li>
  <li>Are there any <strong>outliers</strong>?</li>
  <li>What <strong>patterns</strong> emerge from the data?</li>
  <li>What <strong>trends</strong> can be observed?</li>
  <li>Where are the <strong>missing</strong> values?</li>
  <li>How do variables <strong>correlate</strong>?</li>
  <li>What <strong>causal relationships</strong> exist between variables?</li>
  <li>What are the <strong>implications</strong> suggested by the data?</li>
  <li>Based on the data, what <strong>predictions</strong> can be made?</li>
  <li>What <strong>opportunities</strong> does the data reveal?</li>
</ul>

<p>To answer these questions, in a typical EDA we begin by defining a hypothesis (or a series of questions around a particular subject) that we want to answer.
Next, we collect the relevant data, explore it, clean it, visualize it, and finally build a report. 
This is a straightforward process, as shown in the following diagram:</p>

<pre><code class="language-mermaid">%%{init: {'theme':'base'}}%%
flowchart TB;
subgraph EDA [.]
  A([🛢 Data Collection])
  B([🔍 Data Exploration])
  C([🧹 Data Cleaning])
  D([🛠️ Data Visualization])
  E([📈 Report Building])
  A --&gt; B
  B --&gt; C
  C --&gt; D
  D --&gt; E
end
I([🤔 Hypothesis])
F([💁️ End User or Decision Maker])
I --&gt; A
E --&gt; F
</code></pre>

<p>Coming from an academic background, I relate EDA to the <a href="https://en.wikipedia.org/wiki/Scientific_method">scientific method</a>.
Viewed from a practical standpoint, such as that of a problem solver or stakeholder, EDA is crucial for understanding the problem before developing any solution to address it.
This preliminary step ensures that the solution is effectively tailored to the intricacies of each particular problem.</p>

<h1 id="eda-the-ubiquitous-technical-skill">EDA: The Ubiquitous Technical Skill</h1>

<p>I’ve been practicing EDA for over 12 years now.
I began dealing with data during my undergraduate studies in Computer Science, and later when working on my <a href="../files/thesis/cesar-fulltext.pdf">PhD</a>.
Interestingly, I’ve done it not because I particularly enjoy it (which I do), but because it was essential for my research.
On the other hand, working as a software developer now, my data analysis skills have opened doors to new opportunities to showcase my research abilities.
The two following section focus this journey.</p>

<h2 id="a-swiss-army-knife-in-academia">A Swiss Army Knife in Academia</h2>

<p>During my undergraduate studies in Computer Science, I received a lot of heavy maths subjects, including several semesters of calculus, linear algebra, discrete mathematics, advanced geometry, optimization models, and so on.
Among them, there was only one about statistics, which we considered to be <em>soft</em> compared to the rest.
I remember that the lectures were very theoretical and focused on the formulas, rather than the practical aspects.
It was not until I had to write my thesis that I realized the importance of statistics.
There I was introduced to SPSS and R.
After looking at examples of the other thesis, I noticed that statistical tests was something that they all had in common. 
Clearly, a good EDA is strictly necessary to craft a good thesis.</p>

<aside class="quote">
    <em>“EDA is a fundamental skill at all levels of high education.”</em> 
</aside>

<p>So, to earn a diploma, one has to collect data and apply rigour statistical tests to prove or disprove a scientific hypothesis.<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>
I started learning some plotting tools and statistical techniques.
I remember me looking at code examples in <a href="https://www.kaggle.com/">Kaggle</a> where there used to be diverse datasets and a lot of community written notebooks to learn from.</p>

<blockquote>
  <p>My <a href="https://dspace.uclv.edu.cu/handle/123456789/1935">bachelor’s thesis</a> was about applying time series classification models to the problem of accurately forecasting rainfalls. That was the first time I performed some serious EDA in R and SPSS to find patterns in real-world meteorological data.</p>
</blockquote>

<p>A couple of years later, I started my Master’s in Computer Science and went into sport analytics.
I knew that if manage to collect enough data, formulate the right hypothesis, and use a completely new technique for the problem at hand, I could manage to get a paper published.
So I did exactly that (btw, <a href="https://sciendo.com/it/article/10.1515/ijcss-2016-0007">this</a> is the paper if you’re wondering).
I analyzed all the historical data publicly available of the MLB and applied what, at the time, were cutting edge ML techniques to predict the outcome of games.
I published a few papers on sport analytics later on using a similar strategy. 
I’ve no doubts that having high-quality EDA and convincing tables and figures was key to get those papers accepted.<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">2</a></sup></p>

<aside class="quote">
    <em>“EDA is essential for reproducible science, allowing other researchers to understand, replicate, and validate the findings.”</em> 
</aside>

<p>Later during my PhD, I switched to <a href="../blog/empirical-software-engineering-research-is-harder-than-you-think.html">empirical software engineering research</a>.
Soon, I realized that software is data as well, so EDA was critical to help me understand different phenomenons occurring in large software ecosystems.
I started working on “<a href="../blog/why-debloating-third-party-software.html">software debloating</a>,” and collected a massive amount of data representing the relationships between Java dependencies.
For example, I used EDA to find out which dependencies are the most bloated, which are the most used, and which are the most popular in the Java Maven ecosystem.</p>

<p>In conclusion, during all my academic career, the scale of the experiments and complexity of the data increased, but the importance of EDA remained the same.</p>

<h2 id="a-superpower-beyond-academia">A Superpower Beyond Academia</h2>

<p>After my PhD, I started working as a software developer. 
Being at a large organization helped me to understand better the motto “data driven decisions.”
Again, EDA is one of the most important skills to understand data relationships and hence take decisions.
It turns out that my academic skills are great to stand out, to better support my arguments with data, or simply to describe problems under the basis of facts.
This not only makes me more effective at work, also makes my ideas more convincing, so it resonates with my colleagues and managers.
EDA is both a tool and an asset.</p>

<aside class="quote">
    <em>“Ideas backed by data are more convincing, which makes them more likely to catch the attention of decision-makers.”</em> 
</aside>

<p>The market has also seeing the potential. 
Most large companies today rely on built-in tools for conducting EDA, such as <a href="https://www.elastic.co/">Elasticsearch</a>, <a href="https://www.tableau.com/">Tableau</a>, and <a href="https://www.microsoft.com/en-us/power-platform/products/power-bi">PowerBI</a>.
Yet, these platforms, despite their capabilities, often fall short when it comes to the depth of analysis required for complex problems.
There are instances where delving deeper into the data through custom coding is imperative, e.g., to leverage cutting-edge data analysis methodologies and statistical tests, or to reproduce the results of a research paper.
This is where R and Python come into play, offering a plethora of libraries and tools that enable the execution of complex EDA tasks.
On top of that, you can combine the data analysis with front-end development to create awesome interactive dashboards using some JavaScript library such as the famous <a href="https://d3js.org/">D3.js</a>, <a href="https://recharts.org/en-US/">Recharts</a>, or <a href="https://plotly.com/">Plotly</a>.</p>

<p>While AI continues to revolutionize how we interpret and utilize data, I believe that AI easily stumbles when it comes to EDA. 
The primary challenge for AI lies in understanding the context, which is required to effectively approach a business decision. 
For example, AI may not be able to discern the difference between a correlation and causation, or to spot nonsensical data points (such as a negative age or unusual outlier).
EDA is great for humans, as it’s inherently exploratory and subjective, involving creativity, domain expertise, and critical thinking.
This human touch allows for the identification of meaningful insights that AI, in its current state, struggles to replicate.</p>

<h1 id="the-craft-of-a-good-eda">The Craft of a Good EDA</h1>

<p>In this section, I share some insights on how I perform my EDA tasks.
I use R and RStudio, but you can relate the structure, narratives, aesthetics, and tools to any other programming language or environment you prefer.</p>

<h2 id="structure">Structure</h2>

<p>Before diving into the raw data, it is crucial to establish a clear structure for the  project.
I host my projects on GitHub, and structure them in R as follows:</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>example_eda_project_in_R/
│
├── data/
│   └── raw_data.csv
│
├── docs/
│   ├── project_overview.pdf
│   └── README.md
│
├── notebooks/
│   ├── figures/
│   ├── RQ1.Rmd
│   ├── RQ2.Rmd
│   └── RQ3.Rmd 
│
├── R/
│   └── data_processing.R 
│
├── .gitignore
└── README.md
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">data</code> directory contains the raw data, while the <code class="language-plaintext highlighter-rouge">docs</code> directory houses the project overview and a <code class="language-plaintext highlighter-rouge">README</code> file.
In the <code class="language-plaintext highlighter-rouge">notebooks</code> directory, I create separate R Markdown files for each research question and add a subdirectory named <code class="language-plaintext highlighter-rouge">figures</code> to store the plots.
I prefer notebooks to scripts, as they allow for a more interactive and engaging analysis.
However, I also include a <code class="language-plaintext highlighter-rouge">R</code> directory for data processing scripts that are common to multiple notebooks, which I import using the following code:</p>

<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">source</span><span class="p">(</span><span class="s2">"R/data_processing.R"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>Importing functions is a nice way to have (at least) some notion of reusable code in the project, beyond the notebooks.
I highly recommend mixing the use of notebooks and scripts, as it allows to preserve good software engineering practices and avoid the pervasive “<a href="https://medium.com/skyline-ai/jupyter-notebook-is-the-cancer-of-ml-engineering-70b98685ee71">notebook hell</a>” that many data scientist fall into.
This superstructure helps me keep my projects organized and easily accessible.</p>

<h2 id="hypotheses">Hypotheses</h2>

<p>I always start my EDA projects by defining clear hypotheses.
Finding the relevant hypothesis to test is crucial, as it drives the entire analysis.
It is important for the hypothesis to provide a clear value proposition.
For example, in a marketing project for a new product, the hypothesis could be something like: “The new product will increase sales by 20% in the first quarter.”
It could be also to dispel a myth, such as “Sales were not affected by negative reviews, as any publicity is good publicity,” or to prove or disprove a certain claim, such as “Investing <code class="language-plaintext highlighter-rouge">X</code> amount of money in marketing platform <code class="language-plaintext highlighter-rouge">Y</code> will increase sales by <code class="language-plaintext highlighter-rouge">Z</code>.”</p>

<p>A good hypothesis is:</p>

<ul>
  <li><strong>Concise:</strong> A single question or statement.</li>
  <li><strong>Refutable:</strong> It can be proven right or wrong.</li>
  <li><strong>Original:</strong> No one has investigated it before.</li>
  <li><strong>Testable:</strong> There is enough data to prove or disprove it.</li>
  <li><strong>Relevant:</strong> It is important for the business or the research.</li>
</ul>

<h2 id="narratives">Narratives</h2>

<p>We all love stories. 
This is something we acquired since we were kids and listened to our moms beside our beds.
Stories are powerful tools to convey information and make it memorable.
In EDA, the narrative is the story you want to tell with your data.
It is the message you want to convey to your audience.</p>

<p>Let me illustrate the power of EDA and narratives altogether, using an example that shows how to lie with statistics:</p>

<blockquote>
  <p>“I was sitting in the library, going through recent journal articles when, to everyone’s surprise, Donald Knuth walked in. I can now say that the average number of citations of all researchers in the library, including myself, is now in the thousands. This is a perfect example of how to lie with statistics. Be careful when using averages; they can be mean to you. Outliers can wreck your conclusions.”</p>
</blockquote>

<p>Packaging statistics into a narrative is an art in itself.
It requires a deep understanding of the data, the context, and the audience.
In academia, as well as in industry, the ability to craft compelling narratives from data is a highly sought-after skill.</p>

<h2 id="aesthetics">Aesthetics</h2>

<p>It is said that a picture is worth a thousand words.
This is especially true in EDA.
The aesthetics of your plots can make your work memorable or completely break your message.
In a study on visual information in scientific literature, it was found that higher-impact articles had more diagrams per page and a higher proportion of diagrams, but a lower proportion of photos.<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>
This underscores the importance of visualizations in conveying complex information.</p>

<p>Color, size, and shape are all important elements of a good plot. For example, I like this palette (shown below): <code class="language-plaintext highlighter-rouge">#00cd6c</code>, <code class="language-plaintext highlighter-rouge">#ffc61e</code>, <code class="language-plaintext highlighter-rouge">#a0b1ba</code>, <code class="language-plaintext highlighter-rouge">#ff1f5d</code>, <code class="language-plaintext highlighter-rouge">#009ade</code>, <code class="language-plaintext highlighter-rouge">#af58ba</code>, <code class="language-plaintext highlighter-rouge">#f28522</code>, and <code class="language-plaintext highlighter-rouge">#a6761d</code>. Do you also like it?</p>

<div style="margin-bottom: 15px">
<div style="background-color: #00cd6c; width: 100%; height: 10px; margin-top: 5px;"></div>
<div style="background-color: #ffc61e; width: 100%; height: 10px; margin-top: 5px;"></div>
<div style="background-color: #a0b1ba; width: 100%; height: 10px; margin-top: 5px;"></div>
<div style="background-color: #ff1f5d; width: 100%; height: 10px; margin-top: 5px;"></div>
<div style="background-color: #009ade; width: 100%; height: 10px; margin-top: 5px;"></div>
<div style="background-color: #af58ba; width: 100%; height: 10px; margin-top: 5px;"></div>
<div style="background-color: #f28522; width: 100%; height: 10px; margin-top: 5px;"></div>
<div style="background-color: #a6761d; width: 100%; height: 10px; margin-top: 5px;"></div>
</div>

<p>Here are some additional tips to make a plot better:</p>

<ul>
  <li>Use vector graphics for publication-quality plots (SVG or PDF).</li>
  <li>Remove all unnecessary elements (including gridlines and legends if possible).</li>
  <li>Use a caption to synthesize the message, not to describe the chart itself (I try to summarize the key takeaway in one sentence).</li>
  <li>Check for message bias using a third party (I ask another person what does she understand from the figure, if it takes more than 1 min to figure out, then there’s something wrong in my plot).</li>
  <li>Do the last retouches in a vector graphics editor (e.g., I use <a href="https://inkscape.org/">Inkscape</a>).</li>
</ul>

<h2 id="tools">Tools</h2>

<p>I use R and RStudio for my EDA projects.
In particular, I leverage the powerful <a href="https://www.tidyverse.org/">tidyverse</a> ecosystem, and <a href="https://ggplot2.tidyverse.org/">ggplot2</a> in particular for creating beautiful visualizations.
<code class="language-plaintext highlighter-rouge">ggplot2</code> is a powerful tool that leverages the <a href="https://link.springer.com/book/10.1007/0-387-28695-0">Grammar of Graphics</a> to create complex plots with ease.
This method is very popular because it allows for the creation of plots layer by layer, making it easy to customize every aspect of the plot.
I start by loading the data, then I summarize statistics, explore the data, create some plots, change aesthetics, scales, and labels.</p>

<p>Here is <a href="https://github.com/ASSERT-KTH/jdbl-experiments/blob/master/notebooks/analysis_size.Rmd#L66C1-L81C64">a real code example</a> of an EDA that I did for <a href="https://doi.org/10.1145/3546948">this paper</a> (Note that this code and the resulting figure is never shown or discussed anywhere in the published paper):</p>

<figure class="highlight"><pre><code class="language-r" data-lang="r"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre></td><td class="code"><pre><span class="c1"># ----------------------------------------------------------------------------</span><span class="w">
</span><span class="c1"># barplot of Jar sizes</span><span class="w">
</span><span class="c1"># ----------------------------------------------------------------------------</span><span class="w">
</span><span class="n">jar_size_barplot</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">df</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">ggplot</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Application</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Size</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Tool</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_bar</span><span class="p">(</span><span class="n">stat</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"identity"</span><span class="p">,</span><span class="w"> </span><span class="n">position</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">position_dodge</span><span class="p">())</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_text</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">label</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">paste</span><span class="p">(</span><span class="n">Size</span><span class="p">,</span><span class="w"> </span><span class="s2">"%"</span><span class="p">)),</span><span class="w">
            </span><span class="n">position</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">position_dodge</span><span class="p">(</span><span class="n">width</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.9</span><span class="p">),</span><span class="w">
            </span><span class="n">vjust</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">-0.3</span><span class="p">,</span><span class="w">
            </span><span class="n">family</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"LM Roman 10"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">scale_y_continuous</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="n">paste0</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="s2">"%"</span><span class="p">),</span><span class="w"> </span><span class="n">limits</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">100</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">labs</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">NULL</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">NULL</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">scale_fill_viridis</span><span class="p">(</span><span class="n">discrete</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="n">option</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"D"</span><span class="p">,</span><span class="w"> </span><span class="n">direction</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">-1</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme</span><span class="p">(</span><span class="n">legend.title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">element_blank</span><span class="p">())</span><span class="w">

</span><span class="n">ggsave</span><span class="p">(</span><span class="n">filename</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Figures/jar_size_barplot.pdf"</span><span class="p">,</span><span class="w"> </span><span class="n">plot</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">jar_size_barplot</span><span class="p">,</span><span class="w">
       </span><span class="n">height</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="n">width</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">7</span><span class="p">,</span><span class="w">  </span><span class="n">units</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"in"</span><span class="p">),</span><span class="w"> </span><span class="n">device</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"pdf"</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>The code above produces <a href="https://github.com/ASSERT-KTH/jdbl-experiments/blob/master/notebooks/Figures/analysis_size/jar_size_barplot.pdf">this figure</a>. 
As you can see, plotting is about taking into account small details and doing micro optimizations.
For example, the <code class="language-plaintext highlighter-rouge">geom_text</code> function is used to add the percentage values on top of the bars, and the <code class="language-plaintext highlighter-rouge">scale_y_continuous</code> function is used to change the labels to percentages.
These details make a plot better, but overall choosing the right chart type, the right colors, and the right message is a good starting point.</p>

<h1 id="examples">Examples</h1>

<p>I have used EDA in various projects, from analyzing sports’ statistics to academic software engineering research, and also in my current work. 
My code is far from beautiful, but <del>I want to believe</del> it is still better than the horrendous Excel sheets that still exist out there.
I would like to emphasize that the EDA code is never published or discussed anywhere in the research papers, but it’s solely available in the repositories which (surprisingly?) are almost never public.<sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">4</a></sup></p>

<p>Here are some examples of repositories where I have used EDA in my research papers:</p>

<ul>
  <li><a href="https://github.com/ASSERT-KTH/deptrim-experiments/tree/main/notebooks/R"><i class="fab fa-github"></i></a> <a href="https://github.com/ASSERT-KTH/deptrim-experiments/tree/main/notebooks/R">deptrim-experiments</a></li>
  <li><a href="https://github.com/ASSERT-KTH/jdbl-experiments/tree/master/notebooks"><i class="fab fa-github"></i></a> <a href="https://github.com/ASSERT-KTH/jdbl-experiments/tree/master/notebooks">jdbl-experiments</a></li>
  <li><a href="https://github.com/ASSERT-KTH/depclean-experiments/tree/master/notebooks"><i class="fab fa-github"></i></a> <a href="https://github.com/ASSERT-KTH/depclean-experiments/tree/master/notebooks">depclean-experiments</a></li>
  <li><a href="https://github.com/ASSERT-KTH/decompilercmp/tree/master/notebooks"><i class="fab fa-github"></i></a> <a href="https://github.com/ASSERT-KTH/decompilercmp/tree/master/notebooks">decompilercmp</a></li>
  <li><a href="https://github.com/chains-project/ethereum-ssc/tree/main/r_notebooks"><i class="fab fa-github"></i></a> <a href="https://github.com/chains-project/ethereum-ssc/tree/main/r_notebooks">ethereum-ssc</a></li>
  <li><a href="https://github.com/cesarsotovalero/msr-2019s"><i class="fab fa-github"></i></a> <a href="https://github.com/cesarsotovalero/msr-2019">msr-2019</a></li>
  <li><a href="https://github.com/cesarsotovalero/ijcss-comp-balance-laliga"><i class="fab fa-github"></i></a> <a href="https://github.com/cesarsotovalero/ijcss-comp-balance-laliga">ijcss-comp-balance-laliga</a></li>
</ul>

<h1 id="summary">Summary</h1>

<p>I attribute a large proportion of my professional successes to the fact that I understood the importance of explaining and manipulating data very early on in my career.
For me, EDA has proven to be a valuable asset in various fields and applications, from academia to industry.
From computer science research to software development, EDA has been a constant companion over the last 11 years.
My journey with EDA ranges from using it to analyze sports statistics to employing it as a critical tool in my research endeavors, and eventually, integrating it into my professional work.</p>

<p>Of course, some people are so skilled that it’s difficult for them to identify the most important technical skill in their career.
However, I believe recognizing which skills were crucial at different stages is beneficial for personal growth.
For specialists, it helps to focus on perfecting the right capabilities rather than trying to become a jack-of-all-trades.
For generalists, it helps to diversify around a particular skill set.
For me, the benefits of nurturing EDA over time have been varied: better job opportunities and professional growth.
Have you ever thought about it?</p>

<h1 id="footnotes">Footnotes</h1>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:2">
      <p>I find the table in <a href="https://help.xlstat.com/6443-which-statistical-test-should-you-use">this page</a> handy to determine which statistical test to use. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>I recommend reading the latest papers published in the <a href="https://www.jstatsoft.org/index">Journal of Statistical Software</a> and the <a href="https://www.jmlr.org/">Journal of Machine Learning Research</a>. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:1">
      <p>See “<a href="https://ieeexplore.ieee.org/abstract/document/7888968/?casa_token=-X0UVsB4pJoAAAAA:DgGkzsGt_6c-SovMKCFQvalKU5_I4bpqmjfVqQQTbmHQSeQPw3XzooyrcHM0hIFUL1IYnC4dyg">Viziometrics: Analyzing Visual Information in the Scientific Literature</a>,” in <em>IEEE Transactions on Big Data</em>, 2017 <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>I’ve peer-reviewed <a href="../service.html">quite a few</a> research papers, almost none of them include the code used to generate the figures in the paper. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>César Soto Valero</name><email>cesarsotovalero@gmail.com</email></author><category term="career" /><summary type="html"><![CDATA[Have you ever wonder which is most relevant technical skill in your career? It's not necessarily a programming language or the latest framework. For me, it is Exploratory Data Analysis (EDA). Why? Because being able to manipulate and find relevant information from various data sources has opened me doors to new opportunities in both academia and industry. This post is about my journey, the lessons learned, and the resources I have used to enhance my EDA skills over time.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://www.cesarsotovalero.net/img/posts/2024/2024-03-30/paint-cover.jpg" /><media:content medium="image" url="https://www.cesarsotovalero.net/img/posts/2024/2024-03-30/paint-cover.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>